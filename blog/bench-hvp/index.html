<!DOCTYPE html> <html> <script>let thunk=()=>{let t=t=>t.trim(),e=t=>t.innerText,n=t=>{let e=t.split(" "),n=e.slice(0,-1).join(" ");return[e.at(-1),n]},o=Array.from(document.getElementsByClassName("author")).map(e).map(t).map(n),a=o[0][0],i=(Array.from(document.getElementsByClassName("affiliation")).filter(t=>"P"===t.nodeName).map(e).map(t),"May 7, 2024"),r="How to compute Hessian-vector products?",s="The product between the Hessian of a function and a vector, the Hessian-vector product (HVP), is a fundamental quantity to study the variation of a function. It is ubiquitous in traditional optimization and machine learning. However, the computation of HVPs is often considered prohibitive in the context of deep learning, driving practitioners to use proxy quantities to evaluate the loss geometry. Standard automatic differentiation theory predicts that the computational complexity of an HVP is of the same order of magnitude as the complexity of computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, JAX and PyTorch, allow for efficient computation of these HVPs in standard deep learning cost functions.";{let t=o.map(t=>`${t[0]}, ${t[1]}`).join(" and "),e=`\n@inproceedings{${(a+"2024"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${t}},\n  title = {${r}},\n  abstract = {${s}},\n  booktitle = {ICLR Blogposts 2024},\n  year = {2024},\n  date = {${i}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=e}{let t=o.map(t=>t[0]),e=`\n${t=t.length>2?t[0]+", et al.":2==t.length?t[0]+" & "+t[1]:t[0]}, "${r}", ICLR Blogposts, 2024.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=e}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>How to compute Hessian-vector products? | ICLR Blogposts 2024</title> <meta name="author" content="ICLR Blog"/> <meta name="description" content="The product between the Hessian of a function and a vector, the Hessian-vector product (HVP), is a fundamental quantity to study the variation of a function. It is ubiquitous in traditional optimization and machine learning. However, the computation of HVPs is often considered prohibitive in the context of deep learning, driving practitioners to use proxy quantities to evaluate the loss geometry. Standard automatic differentiation theory predicts that the computational complexity of an HVP is of the same order of magnitude as the complexity of computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, JAX and PyTorch, allow for efficient computation of these HVPs in standard deep learning cost functions."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/2024/assets/img/iclr_favicon.ico"/> <link rel="stylesheet" href="/2024/assets/css/main.css"> <link rel="canonical" href="https://iclr-blogposts.github.io/2024/blog/bench-hvp/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/2024/assets/js/theme.js"></script> <script src="/2024/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2024/assets/js/distillpub/template.v2.js"></script> <script src="/2024/assets/js/distillpub/transforms.v2.js"></script> <script src="/2024/assets/js/distillpub/overrides.js"></script> <style type="text/css">.framed{border:1px var(--global-text-color) dashed!important;padding:20px}.marge{margin-left:20px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "How to compute Hessian-vector products?",
      "description": "The product between the Hessian of a function and a vector, the Hessian-vector product (HVP), is a fundamental quantity to study the variation of a function. It is ubiquitous in traditional optimization and machine learning. However, the computation of HVPs is often considered prohibitive in the context of deep learning, driving practitioners to use proxy quantities to evaluate the loss geometry. Standard automatic differentiation theory predicts that the computational complexity of an HVP is of the same order of magnitude as the complexity of computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, JAX and PyTorch, allow for efficient computation of these HVPs in standard deep learning cost functions.",
      "published": "May 7, 2024",
      "authors": [
        {
          "author": "Mathieu Dagréou",
          "authorURL": "https://matdag.github.io",
          "affiliations": [
            {
              "name": "Inria",
              "url": ""
            }
          ]
        },
        {
          "author": "Pierre Ablin",
          "authorURL": "https://pierreablin.com/",
          "affiliations": [
            {
              "name": "Apple",
              "url": ""
            }
          ]
        },
        {
          "author": "Samuel Vaiter",
          "authorURL": "https://samuelvaiter.com/",
          "affiliations": [
            {
              "name": "CNRS",
              "url": ""
            }
          ]
        },
        {
          "author": "Thomas Moreau",
          "authorURL": "https://tommoral.github.io/",
          "affiliations": [
            {
              "name": "Inria",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2024/">ICLR Blogposts 2024</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2024/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">other iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/"><strong>2024</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" target="_blank" rel="noopener noreferrer">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>How to compute Hessian-vector products?</h1> <p>The product between the Hessian of a function and a vector, the Hessian-vector product (HVP), is a fundamental quantity to study the variation of a function. It is ubiquitous in traditional optimization and machine learning. However, the computation of HVPs is often considered prohibitive in the context of deep learning, driving practitioners to use proxy quantities to evaluate the loss geometry. Standard automatic differentiation theory predicts that the computational complexity of an HVP is of the same order of magnitude as the complexity of computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, JAX and PyTorch, allow for efficient computation of these HVPs in standard deep learning cost functions.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#what-are-hvps-and-where-are-they-useful">What are HVPs and where are they useful?</a></div> <div><a href="#"></a></div> <ul> <li><a href="#inverse-hessian-vector-products-ihvps-in-optimization">Inverse Hessian-vector products (iHVPs) in optimization</a></li> <li><a href="#hvps-for-the-study-of-the-loss-landscape">HVPs for the study of the loss landscape</a></li> </ul> <div><a href="#a-quick-detour-by-automatic-differentiation">A quick detour by automatic differentiation</a></div> <div><a href="#"></a></div> <ul> <li><a href="#computational-graph">Computational graph</a></li> <li><a href="#forward-mode">Forward mode</a></li> <li><a href="#reverse-mode">Reverse mode</a></li> </ul> <div><a href="#naive-computation-of-hvps">Naive computation of HVPs</a></div> <div><a href="#hvps-with-automatic-differentiation">HVPs with automatic differentiation</a></div> <ul> <li><a href="#forward-over-reverse">Forward-over-reverse</a></li> <li><a href="#reverse-over-reverse">Reverse-over-reverse</a></li> <li><a href="#reverse-over-forward">Reverse-over-forward</a></li> </ul> <div><a href="#benchmark-with-deep-learning-architectures">Benchmark with deep learning architectures</a></div> <ul> <li><a href="#time-complexity">Time complexity</a></li> <li><a href="#memory-complexity">Memory complexity</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <p>Hessian-vector products (HVPs) play a central role in the study and the use of the geometric property of the loss function of deep neural networks<d-cite key="Foret2021SAM"></d-cite>, as well as in many recent bilevel optimizers<d-cite key="Arbel2022amigo"></d-cite>. However, computing such quantity is often considered prohibitive by practitioners, discouraging them from using algorithms that rely on HVPs.</p> <p>With this blog post, we aim to convince the practitioners that with modern automatic differentiation (AD) frameworks such as <code class="language-plaintext highlighter-rouge">JAX</code> or <code class="language-plaintext highlighter-rouge">PyTorch</code>, HVPs can be efficiently evaluated. Indeed, standard AD theory predicts that the computational cost of an HVP is of the same order as the cost of computing a gradient. After a brief introduction on why HVPs are useful for optimization and ML applications and on the basis of AD, we explain in detail the AD-based methods to compute an HVP and the reason for their efficiency. In particular, we show that one can compute HVPs without explicit Hessian computation. We then compare the different methods to compute HVPs for several deep neural network architectures in terms of time and memory for both <code class="language-plaintext highlighter-rouge">JAX</code> and <code class="language-plaintext highlighter-rouge">PyTorch</code>. Our results illustrate the complexity predicted by the theory, showing that computing an HVP is not much more expensive than computing a gradient. This opens an avenue to develop efficient second-order informed methods for neural networks.</p> <h2 id="what-are-hvps-and-where-are-they-useful">What are HVPs and where are they useful?</h2> <p>Let us first introduce the notion of Hessian and HVP. We will consider in this post a twice differentiable function \(f:\mathbb{R}^d\to\mathbb{R}\) that goes from a vector \(x\) in space \(\mathbb{R}^d\) to a real number in \(\mathbb{R}\). This typically corresponds to a function that maps the value of the parameters \(\theta\) of a neural network to the loss \(f(\theta)\). For such a function, standard AD can be used to efficiently compute the gradient of the loss \(\nabla f(\theta) = \left[ \frac{\partial f}{\partial \theta_i}(\theta)\right]_{1\le i \le d} \in \mathbb{R}^d\), using the backpropagation. The Hessian matrix of \(f\) at \(\theta\) is the matrix of its second-order partial derivatives</p> \[\nabla^2 f(\theta) = \left[\frac{\partial^2f}{\partial \theta_i\partial \theta_j}(\theta)\right]_{1\leq i,j\leq d}\in\mathbb{R}^{d\times d}\enspace.\] <p>This matrix corresponds to the derivative of the gradient and captures how the gradient will change when moving \(x\). To evaluate the variation of the gradient when moving \(\theta\) in the direction \(v\in\mathbb{R}^d\), one can compute the quantity \(\nabla^2 f(\theta) v\in\mathbb{R}^d\). This is the Hessian-vector product (HVP).</p> <p>Let us review some use cases of HVPs in optimization and machine learning.</p> <h3 id="inverse-hessian-vector-products-ihvps-in-optimization">Inverse Hessian-vector products (iHVPs) in optimization</h3> <p>When trying to find the minimum of the function \(f\), methods that account for the second-order information often rely on the product between the inverse Hessian and a vector to find a good update direction. For instance, Newton’s method relies on update rules of the form</p> \[\theta_{k+1} = \theta_k - \eta_k[\nabla^2f(\theta_k)]^{-1}\nabla f(\theta_k)\] <p>for some step-size \(\eta_k&gt;0\).</p> <p>When evaluating the term \([\nabla^2f(\theta_k)]^{-1}\nabla f(\theta_k)\), it would be very inefficient to first compute the full Hessian matrix \(\nabla^2f(\theta_k)\), then invert it and finally multiply this with the gradient \(\nabla f(\theta_k)\). Instead, one computes the inverse Hessian-Vector Product (iHPV) by solving the following linear system</p> <p>\begin{equation}\label{eq:linear_system} \nabla^2f(\theta)v = b\enspace. \end{equation}</p> <p>with \(b = \nabla f(\theta_k)\). This approach is much more efficient as it avoids computing and storing the full Hessian matrix, and only computes the inverse of the matrix in the direction \(v\).</p> <p>A second use case for the iHVP in optimization is with bilevel optimization. In bilevel optimization, one wants to solve the following problem</p> <p>\begin{equation}\label{eq:bilevel_pb} \min_{x\in\mathbb{R}^d} h(x) = F(x, y^* (x))\quad\text{with}\quad y^*(x) = \arg\min_{y\in\mathbb{R}^p} G(x, y)\enspace. \end{equation}</p> <p>The gradient of the function \(h\) can be computed using the implicit function theorem, giving the following expression</p> \[\nabla h(x) = \nabla_x F(x, y^* (x)) - \nabla_{xy}G(x, y^*(x))[\nabla_{yy}G(x, y^*(x))]^{-1}\nabla_y G(x, y^*(x))\enspace.\] <p>Here, the term \(\nabla^2_{yy} G(x, y)\) is the Hessian of the function \(G\) relatively to \(y\). Thus, this quantity also requires computing an iHVP.</p> <p>To compute the iHVP, there are many methods in the literature to solve \eqref{eq:linear_system}, like Neumann iterates<d-cite key="Ghadimi2018"></d-cite><d-cite key="Ji2021stocbio"></d-cite>, the Conjugate Gradient method<d-cite key="Hestenes1952CG"></d-cite><d-cite key="Nocedal2006"></d-cite> or gradient descent steps in the quadratic form \(v\mapsto \frac12\langle\nabla^2f(\theta)v, v\rangle - \langle b, v\rangle\)<d-cite key="Arbel2022amigo"></d-cite><d-cite key="Dagreou2022SABA"></d-cite>. These methods rely on HVPs, as illustrated by the highlighted terms in the Conjugate Gradient method. Thus, an efficient implementation of HVPs is crucial for the overall algorithm performance.</p> <p class="framed"> <b class="underline">Conjugate gradient to solve \eqref{eq:linear_system}</b><br> <b>Input</b> Initialization \(v_0\)<br> <b>Initialization</b> $$ r_0 = \textcolor{orange}{\nabla^2f(\theta) v_0} - b,\quad p_0 = -r_0,\quad t = 0 $$ <b>While</b> \(r_t \neq 0\) \begin{align*} \alpha_t &amp;=\frac{r_t^\top r_t}{p_t^\top \textcolor{orange}{\nabla^2f(\theta) p_t}} \\ v_{t+1} &amp;=v_t + \alpha_t p_t \\ r_{t+1} &amp;=r_t + \alpha_t\textcolor{orange}{\nabla^2f(\theta) p_t} \\ \beta_{t+1} &amp;=\frac{r_{t+1}^\top r_{t+1}}{r_t^\top r_t} \\ p_{t+1} &amp;=-r_{t+1} + \beta_{t+1} p_t\\ t &amp;=t + 1 \end{align*} </p> <h3 id="hvps-for-the-study-of-the-loss-landscape">HVPs for the study of the loss landscape</h3> <p>The study of the geometry of neural networks is an active field that aims at understanding the links between training dynamics, local geometry of the training loss and generalization<d-cite key="Keskar2017"></d-cite>. One way to study the local geometry of a neural network is to find the distribution of the eigenvalues of its Hessian matrix. Indeed, depending on the sign of the eigenvalues of the Hessian, one can for instance distinguish local minima, local maxima and saddle points. As an illustration, the following figure shows how the sign of the eigenvalues of the Hessian matrix of a function affects the shape of the function’s landscape around a stationary point.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/hess_eig-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/hess_eig-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/hess_eig-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-bench-hvp/hess_eig.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In several papers<d-cite key="Ghorbani2019"></d-cite><d-cite key="Dauphin2014"></d-cite><d-cite key="Foret2021SAM"></d-cite>, an approximation of the Hessian spectrum is computed thanks to the Lanczos algorithm<d-cite key="Lanczos1950"></d-cite>. This algorithm is a modification of the power method where each new iterate is taken in the orthogonal complement of the previous iterates. It outputs a factorization of the Hessian of the form $\nabla^2 f(\theta) = VTV^\top$ where \(V=(v_0,...,v_{k-1})\) is orthogonal and</p> \[T = \begin{pmatrix} \alpha_0&amp; \beta_1 &amp; 0 &amp; \cdots &amp; 0\\ \beta_1 &amp; \alpha_1 &amp; \beta_2 &amp; \ddots &amp; \vdots\\ 0 &amp; \beta_2 &amp; \alpha_2 &amp; \ddots &amp; 0\\ \vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \beta_{k-1}\\ 0 &amp; \cdots &amp; 0 &amp; \beta_{k-1} &amp; \alpha_{k-1} \end{pmatrix}\enspace.\] <p class="framed"> <b class="underline">Lanczos' algorithm</b><br> <b>Input</b> Initial vector \(v_0\).<br> <b>Initialization</b> $$ w'_0 = \textcolor{orange}{\nabla^2f(\theta)v_0},\quad \alpha_0 = w_0'^\top v_0,\quad w_0 = w_0' - \alpha_0 v_0 $$ <b>For</b> \(i = 1,\dots, k-1\):<br> \begin{align*} \beta_i &amp;= \|w_{i-1}\|\\ v_{i} &amp;= \frac{w_{i-1}}{\beta_{i}}\\ w_i' &amp;= \textcolor{orange}{\nabla^2f(\theta)v_i}\\ \alpha_i &amp;= w_i'^\top v_i\\ w_i &amp;= w_i' - \alpha_i v_i - \beta_iv_{i-1} \end{align*} </p> <p>We observe once again that the Hessian information is accessed through HVPs rather than the full Hessian matrix itself.</p> <h2 id="a-quick-detour-by-automatic-differentiation">A quick detour by automatic differentiation</h2> <p>Automatic differentiation (AD) is an important tool to compute exactly the derivatives of differentiable functions obtained as the composition of simple operations. There are two modes in AD; the forward mode that computes Jacobian-vector products (JVPs) and the reverse mode that computes vector-Jacobian products (VJPs). Since the gradient of a scalar function is a special case of the VJP, the reverse mode is the most frequently used in machine learning. It is typically used to compute the gradients of deep learning cost functions, where it is called <em>backpropagation</em><d-cite key="Rumelhart1986"></d-cite>.</p> <p>In what follows, we briefly present the notion of computational graph and the two AD modes. For a more detailed explanation, we refer the reader to the excellent survey by Baydin et al.<d-cite key="Baydin2018"></d-cite>.</p> <h3 id="computational-graph">Computational graph</h3> <p>A key ingredient of AD is a computational graph associated with the code that evaluates a function. It is a directed acyclic graph that represents the succession of elementary operations required the evaluate a function.<br> Simple computational graph of a function \(f:\mathbb{R}^d\to\mathbb{R}^p\) are typically</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/direct_graph-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/direct_graph-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/direct_graph-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-bench-hvp/direct_graph.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In this graph, the vertices \(z_i\in\mathbb{R}^{m_i}\) represent the intermediate states of the evaluation of \(f\). To get the vertex \(z_i\), we use the values of its parents in the graph \(z_{i-1}\), with simple transfer functions \(z_i(z_{i-1})\). The computational complexity of the function evaluation depends on the complexity of the considered graph, as one node might have more than one parent. The memory footprint of the evaluation of the function is also linked to the maximum number of parents that can have a vertex in the computational graph, as their value needs to be stored until all children nodes have been computed.</p> <p>Let us take an example with a multilayer linear perceptron (MLP) with 2 layers. The function \(f_x:\mathbb{R}^h\times \mathbb{R}^{h\times p}\to \mathbb{R}\) is defined for an input \(x\in\mathbb{R}^p\) by</p> <p>\begin{equation}\label{eq:mlp} f_x(U, W) = \frac12(UWx)^2\enspace. \end{equation}</p> <p>Here, the input \(\theta\) corresponds to the parameters of the network \((U, V)\) and the intermediate steps are \(z_1 = Wx\), \(z_2 = Uz_1\) and \(z_3 = \frac12 z_2^2\). A possible computational graph to get \(f_x(U, W)\) is the following</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/computational_graph-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/computational_graph-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/computational_graph-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-bench-hvp/computational_graph.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>and the associated Python code to compute \(f_x\) is</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">z3</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">z2</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">z3</span>
</code></pre></div></div> <p>Here, the feed-forward structure of the function makes the computational graph very simple, as each node has a single intermediate result parent.</p> <p>AD uses this computational graph to compute the function’s derivatives. Using the chain rule, the Jacobian \(\frac{\partial f}{\partial \theta}(\theta)\) of \(f\) is obtained as a product of the Jacobian of the intermediate states \(z_1, \dots, z_n\). \begin{equation}\label{eq:chain_rule} \underbrace{\frac{\partial f}{\partial \theta}(\theta)}_{p\times d} = \frac{\partial z_n}{\partial \theta} =\frac{\partial z_n}{\partial z_1}\frac{\partial z_1}{\partial \theta}=\cdots = \underbrace{\frac{\partial z_n}{\partial z_{n-1}}}_{p\times m_{n-1}}\underbrace{\frac{\partial z_{n-1}}{\partial z_{n-2}}}_{m_{n-1}\times m_{n-2}}\cdots\underbrace{\frac{\partial z_1}{\partial \theta}}_{m_1\times d}\enspace. \end{equation} Depending on the order of the multiplication, one can compute the derivative of \(f\) with respect to \(\theta\) in two ways: the forward mode and the reverse mode.</p> <h3 id="forward-mode">Forward mode</h3> <p>For a vector $v\in\mathbb{R}^d$, the Jacobian-vector product (JVP) corresponds to the directional derative of $f$ in the direction $v$. It can be computed by the forward mode AD</p> <p>\begin{equation}\label{eq:chain_rule_jvp} \frac{\partial f}{\partial \theta}(\theta)\times v = \frac{\partial z_n}{\partial z_{n-1}}\frac{\partial z_{n-1}}{\partial z_{n-2}}\cdots\frac{\partial z_1}{\partial \theta}v\enspace. \end{equation}</p> <p>It consists in doing the multiplications in \eqref{eq:chain_rule_jvp} from the right to the left. It is a forward pass in the computational graph where we propagate at the same time the states \(z_i\) and the partial derivatives \(\frac{\partial z_{i+1}}{\partial z_i}\). If \(f\) is real-valued, the \(i\)th coordinate of its gradient is exactly given by product of the Jacobian of \(f\) and the \(i\)th canonical basis vector \(e_i\) since \begin{equation} \frac{\partial f}{\partial \theta_i}(\theta) = \lim_{t\to 0}\frac{f(\theta+te_i)-f(\theta)}{t}\enspace. \end{equation} Thus, we can get its gradient by computing each of the \(d\) JVPs \(\left(\frac{\partial f}{\partial \theta_i}(\theta)\times e_i\right)_{1\leq i \leq d}\) with forward AD.</p> <p>To understand properly what is happening when using forward differentiation, let us go back to the linear MLP defined in \eqref{eq:mlp}. If we implement ourselves the forward differentiation to get the JVP, we obtain the following code</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">v_u</span><span class="p">,</span> <span class="n">v_w</span><span class="p">):</span>
    <span class="c1"># Forward diff of f
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">v_z1</span> <span class="o">=</span> <span class="n">v_w</span> <span class="o">@</span> <span class="n">x</span>  <span class="c1"># Directional derivative of W -&gt; W @ x in the direction v_w
</span>  
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">v_z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">v_z1</span> <span class="o">+</span> <span class="n">v_u</span> <span class="o">@</span> <span class="n">z1</span>  <span class="c1">#  Directional derivative of (U, z_1) -&gt; z2 in the direction (v_u, v_z1)
</span>  
    <span class="n">v_z3</span> <span class="o">=</span> <span class="n">v_z2</span> <span class="o">@</span> <span class="n">z2</span>  <span class="c1"># Directional derivative of z2 -&gt; .5*z2**2 in the direction v_z2 
</span>    <span class="k">return</span> <span class="n">v_z3</span>
</code></pre></div></div> <p>In comparison with the code of the evaluation of \(f_x\), there are two more operations corresponding to the computation of the dual variables <code class="language-plaintext highlighter-rouge">v_z1</code> and <code class="language-plaintext highlighter-rouge">v_z2</code>. In terms of memory, if we consider the computation of the JVP as coded in the previous snippet, the maximum number of parents of a vertex is four. This maximum is achieved by the vertex <code class="language-plaintext highlighter-rouge">v_z2</code> which has the vertices <code class="language-plaintext highlighter-rouge">U</code>, <code class="language-plaintext highlighter-rouge">v_z1</code>, <code class="language-plaintext highlighter-rouge">v_u</code> and <code class="language-plaintext highlighter-rouge">z1</code> as parents.</p> <p>In <code class="language-plaintext highlighter-rouge">JAX</code>, we get the JVP of a function \(f\) in the direction \(v\) with <code class="language-plaintext highlighter-rouge">jax.jvp(f, (params, ), (v, ))[1]</code>.</p> <h3 id="reverse-mode">Reverse mode</h3> <p>The reverse mode is also known as backpropagation in the context of deep learing. For $u\in\mathbb{R}^p$, it aims at computing VJPs</p> <p>\begin{equation}\label{eq:chain_rule_vjp} u^\top\frac{\partial f}{\partial \theta}(\theta) = u^\top\frac{\partial z_n}{\partial z_{n-1}}\frac{\partial z_{n-1}}{\partial z_{n-2}}\cdots\frac{\partial z_1}{\partial \theta}\enspace. \end{equation}</p> <p>In the reverse AD, the multiplications of \eqref{eq:chain_rule_jvp} are done from the left to the right. It requires doing one forward pass in the computational graph to compute the intermediate states \(z_i\) and then a backward pass to propagate the successive partial derivatives from the left to the right. Contrary to the forward mode, it has a more important memory footprint. Indeed, it requires storing the values of all the states. For instance, to compute the last term \(\frac{\partial z_3}{\partial z_2}\), one needs the value of \(z_2\) which was the first computed during the forward pass. If \(f\) is real-valued, \(u\) is a scalar and the VJP is the multiplication of the gradient of \(f\) by \(u\). Thus, one can get the gradient on \(f\) by using \(u=1\) and performing only one reverse differentiation. This makes this mode more efficient in computing gradients.</p> <p>Let us observe what happens if we code manually the backpropagation to get the gradient of the previous function \(f_x\) defined by \(f_x(U, W) = \frac12(UW x)^2\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="c1"># Forward pass
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">z3</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">z2</span><span class="o">**</span><span class="mi">2</span>

    <span class="c1"># Reverse pass
</span>    <span class="c1">## Transfer function: z3 = 0.5 * z2**2
</span>    <span class="n">dz2</span> <span class="o">=</span> <span class="n">z2</span>  <span class="c1"># derivative of z3 wrt z2
</span>  
    <span class="c1">## Transfer function: z2 = U @ z1
</span>    <span class="n">dU</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span>  <span class="c1"># derivative of z3 wrt U
</span>    <span class="n">dz1</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz2</span>  <span class="c1"># derivative of z3 wrt z1
</span>  
    <span class="c1">## Transfer function: z1 = W @ x
</span>    <span class="n">dW</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>   <span class="c1"># derivative of z3 wrt W
</span>    
    <span class="k">return</span> <span class="n">dU</span><span class="p">,</span> <span class="n">dW</span>
</code></pre></div></div> <p>This function returns the gradient of \(f_x\). At reading this code, we understand one needs to store all the intermediate values of the forward pass in the graph. Indeed, if we look at the case of <code class="language-plaintext highlighter-rouge">z1</code> which is the first node computed, it is used four steps later for the computation of <code class="language-plaintext highlighter-rouge">dU</code>.</p> <p>To get the gradient in JAX, one can use <code class="language-plaintext highlighter-rouge">jax.grad(f)(params)</code>.</p> <h2 id="naive-computation-of-hvps">Naive computation of HVPs</h2> <p>Since we are interested in computing \(\nabla^2 f(\theta)v\), the simplest way to do it is to compute the Hessian matrix and then multiply it by the vector \(v\). This can be achieved in <code class="language-plaintext highlighter-rouge">JAX</code> by calling <code class="language-plaintext highlighter-rouge">jax.hessian(f)(params) @ v</code>.</p> <p>This method is quite cumbersome making it impossible to use for deep neural networks. Indeed, the storage of the full Hessian matrix has \(\mathcal{O}(d^2)\) complexity where \(d\) is the dimension of the model’s parameters set.</p> <p>The good news is that we can compute HVP without computing the Hessian thanks to clever use of AD.</p> <h2 id="hvps-without-explicit-hessian-computation">HVPs without explicit Hessian computation</h2> <p>In 1994, Pearlmutter<d-cite key="Pearlmutter1994"></d-cite> proposed to leverage the following observation to compute HVP efficiently: the HVP is also the directional derivative of the gradient in the direction \(v\):</p> \[\nabla^2f(\theta) v = \lim_{\epsilon\to 0} \frac1\epsilon[\nabla f(\theta+\epsilon v)-\nabla f(\theta)] = \nabla [\langle \nabla f(.), v\rangle](\theta)\enspace.\] <p>Based on this identity, AD enables to compute HVPs in three ways, as described in the <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html" target="_blank" rel="noopener noreferrer">JAX documentation</a>.</p> <h3 id="forward-over-reverse">Forward-over-reverse</h3> <p>The forward-over-reverse mode consists in doing forward differentiation in a computational graph of the gradient of \(f\).</p> <p>Its implementation in <code class="language-plaintext highlighter-rouge">JAX</code> is only two lines of code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hvp_forward_over_reverse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jvp</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">))[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <p>In this case, <code class="language-plaintext highlighter-rouge">jax.grad(f)(params)</code> is computed by backward AD, whose complexity is two times the complexity of evaluating \(f\). Thus, the temporal complexity of <code class="language-plaintext highlighter-rouge">hvp_forward_over_reverse</code> is roughly four times the complexity of the evaluation of \(f\).</p> <p>To better see what happens, let us consider again our function \(f_x\) defined by \eqref{eq:mlp}. The Python code of the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> HVP is the following.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_over_reverse</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">v_U</span><span class="p">,</span> <span class="n">v_W</span><span class="p">):</span>
    <span class="c1"># Forward through the forward pass through f
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">v_z1</span> <span class="o">=</span> <span class="n">v_W</span> <span class="o">@</span> <span class="n">x</span>
  
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">v_z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">v_z1</span> <span class="o">+</span> <span class="n">v_U</span> <span class="o">@</span> <span class="n">z1</span>
    
    <span class="c1"># z3 = 0.5 * z2**2
</span>    <span class="c1"># Forward through the backward pass through f
</span>    <span class="n">z4</span> <span class="o">=</span> <span class="n">z2</span>  <span class="c1"># dz2
</span>    <span class="n">v_z4</span> <span class="o">=</span> <span class="n">v_z2</span>  <span class="c1"># v_dz2
</span>  
    <span class="n">z5</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z4</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span>  <span class="c1"># dU
</span>    <span class="n">v_z5</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">v_z4</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z4</span><span class="p">,</span> <span class="n">v_z1</span><span class="p">)</span>  <span class="c1"># v_dU
</span>  
    <span class="n">z6</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z4</span>  <span class="c1"># dz1
</span>    <span class="n">v_z6</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">v_z4</span> <span class="o">+</span> <span class="n">v_U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z4</span>  <span class="c1"># v_dz1
</span>  
    <span class="n">z7</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z6</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># dW
</span>    <span class="n">v_z7</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">v_z6</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># v_dW
</span>  
    <span class="k">return</span> <span class="n">v_z5</span><span class="p">,</span> <span class="n">v_z7</span>  <span class="c1"># v_dU, v_dW
</span></code></pre></div></div> <p>The take-home message of this part is that, after computing the gradient of \(f_x\), one can consider a computational graph of this gradient and perform forward differentiation through this new computational graph. Here, the variables <code class="language-plaintext highlighter-rouge">z1</code>,…, <code class="language-plaintext highlighter-rouge">z7</code> are the vertices of a computational graph of the gradient of \(f_x\). The nice thing is that this mode enables getting at the same time the gradient and the HVP. Indeed, in the previous snippet, <code class="language-plaintext highlighter-rouge">z5</code> and <code class="language-plaintext highlighter-rouge">z7</code> are the components of the gradient of \(f_x\) which could be also returned if needed. This feature can be useful in bilevel optimization for instance.</p> <h3 id="reverse-over-reverse">Reverse-over-reverse</h3> <p>Instead of doing forward differentiation of the gradient, one can multiply the gradient by \(v\) and thus get a scalar. We can then backpropagate into this scalar product. This is the reverse-over-reverse mode.</p> <p>It can be implemented by these lines of code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hvp_reverse_over_reverse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">vdot</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">y</span><span class="p">),</span> <span class="n">v</span><span class="p">))(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div> <p>Since the gradients are computed by backpropagation, the complexity of <code class="language-plaintext highlighter-rouge">hvp_reverse_over_reverse</code> is twice the complexity of <code class="language-plaintext highlighter-rouge">jax.grad(f)</code>, which is roughly four times the complexity of the evaluation of \(f\).</p> <p>Writting down the code of the reverse-over-reverse HVP for our function \(f_x\) defined by \eqref{eq:mlp} makes us understand the differences between this mode and the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> mode. Particularly, one can notice that there are more elementary operations in the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> mode than in the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> mode. Moreover, in terms of memory footprint, the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> requires storing the values of the vertices of the computational graph of the gradient of \(f_x\), while the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> only needs to store the values of the vertices of the computational graph of \(f_x\). Thus, the former is less efficient than the latter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reverse_over_reverse</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">v_u</span><span class="p">,</span> <span class="n">v_w</span><span class="p">):</span>
    <span class="c1"># Forward through &lt;grad(f), v&gt;
</span>    <span class="c1">## Forward through f
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">z3</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
  
    <span class="c1">## Reverse through f
</span>    <span class="n">z4</span> <span class="o">=</span> <span class="n">z2</span>  <span class="c1"># dz2
</span>    <span class="n">z4</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z3</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> <span class="c1"># dU
</span>    <span class="n">z5</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z3</span> <span class="c1"># dz1
</span>    <span class="n">z6</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z5</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># dW
</span>  
    <span class="c1"># Output: dot product &lt;grad(f), v&gt;
</span>    <span class="n">z7</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">z4</span> <span class="o">*</span> <span class="n">v_u</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">z6</span> <span class="o">*</span> <span class="n">v_w</span><span class="p">)</span>
  
    <span class="c1"># Backward through z7 = &lt;grad(f),v&gt;
</span>    <span class="c1">## z7 = jnp.sum(z4 * v_u) + jnp.sum(z6 * v_w)
</span>    <span class="n">dz6</span> <span class="o">=</span> <span class="n">v_w</span>
    <span class="n">dz4</span> <span class="o">=</span> <span class="n">v_u</span>
  
    <span class="c1">## z6 = jnp.outer(z5, x)
</span>    <span class="n">dz5</span> <span class="o">=</span> <span class="n">dz6</span> <span class="o">@</span> <span class="n">x</span>
  
    <span class="c1">## z5 = U.T @ z3
</span>    <span class="n">dz3</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">dz5</span>
    <span class="n">ddU</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z3</span><span class="p">,</span> <span class="n">dz5</span><span class="p">)</span>  <span class="c1"># Derivative of z7 wrt U
</span>  
    <span class="c1">## z4 = jnp.outer(z3, z1)
</span>    <span class="n">dz3</span> <span class="o">+=</span> <span class="n">dz4</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">dz1</span> <span class="o">=</span> <span class="n">dz4</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z3</span>
  
    <span class="c1">## z3 = z2
</span>    <span class="n">dz2</span> <span class="o">=</span> <span class="n">dz3</span>
  
    <span class="c1">## z2 = U @ z1
</span>    <span class="n">dz1</span> <span class="o">+=</span> <span class="n">dz2</span> <span class="o">*</span> <span class="n">U</span>
    <span class="c1"># As U appears multiple times in the graph, we sum its contributions
</span>    <span class="n">ddU</span> <span class="o">+=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> 
  
    <span class="c1">## z1 = W @ x
</span>    <span class="n">ddW</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># Derivative of z7 wrt W
</span>  
    <span class="k">return</span> <span class="n">ddU</span><span class="p">,</span> <span class="n">ddW</span>
</code></pre></div></div> <h3 id="reverse-over-forward">Reverse-over-forward</h3> <p>What about doing forward differentiation of \(f\) rather than reverse propagation? This is what is done in the reverse-over-forward mode. It consists in backpropagating in the computational graph of the JVP of \(f\) and \(v\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hvp_reverse_over_forward</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="n">jvp_fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">))[</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">jvp_fun</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div> <p>This method is more efficient than the previous one. Indeed, since we backpropagate only once, the memory burden is lower than for the <code class="language-plaintext highlighter-rouge">reverse_over_reverse</code> fashion. In comparison with <code class="language-plaintext highlighter-rouge">forward-over-reverse</code>, the complexity is the same. However, one can notice that the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> enables computing at the same time the gradient of \(f\) and the HVP, which is not the case for the <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> mode.</p> <p>The code of the <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> HVP for the MLP \(f_x\) defined by \eqref{eq:mlp} is the following.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reverse_over_forward</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">v_U</span><span class="p">,</span> <span class="n">v_W</span><span class="p">):</span>
    <span class="c1"># Forward diff of f to  &lt;grad(f), v&gt;
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">z6</span> <span class="o">=</span> <span class="n">v_W</span> <span class="o">@</span> <span class="n">x</span>  <span class="c1"># v_z1
</span>  
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">z5</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z6</span> <span class="o">+</span> <span class="n">v_U</span> <span class="o">@</span> <span class="n">z1</span>  <span class="c1"># v_z2
</span>  
    <span class="c1"># output &lt;grad(f), v&gt;
</span>    <span class="n">z4</span> <span class="o">=</span> <span class="n">z5</span> <span class="o">@</span> <span class="n">z2</span>  <span class="c1"># v_z3
</span>  
    <span class="c1"># Backward pass through &lt;grad(f), v&gt;
</span>    <span class="c1">## z4 = z5 @ z2
</span>    <span class="n">dz2</span> <span class="o">=</span> <span class="n">z5</span>
    <span class="n">dz5</span> <span class="o">=</span> <span class="n">z2</span>  <span class="c1"># dv_z2
</span>  
    <span class="c1">## z5 = U @ z6 + v_U @ z1
</span>    <span class="n">dz1</span> <span class="o">=</span> <span class="n">v_U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz5</span>
    <span class="n">dz6</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz5</span>  <span class="c1"># dv_z1
</span>    <span class="n">ddU</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz5</span><span class="p">,</span> <span class="n">z6</span><span class="p">)</span>  <span class="c1"># derivative of z4 wrt U
</span>  
    <span class="c1">## z2 = U @ z1
</span>    <span class="c1"># As U and dz1 appear multiple times, we sum their contributions
</span>    <span class="n">dz1</span> <span class="o">+=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz2</span>
    <span class="n">ddU</span> <span class="o">+=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span>
    
    <span class="c1">## z1 = W @ x
</span>    <span class="n">ddW</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ddU</span><span class="p">,</span> <span class="n">ddW</span>
</code></pre></div></div> <h2 id="benchmark-with-deep-learning-architectures">Benchmark with deep learning architectures</h2> <p>While these three methods compute the same outputs, the different ways of traversing the computational graph change their overall time and memory complexities. We now compare the computation of HVPs with these three methods for various deep-learning architectures. To cover a broad range of use cases, we consider a residual network (<a href="https://huggingface.co/docs/transformers/model_doc/resnet" target="_blank" rel="noopener noreferrer">ResNet34</a><d-cite key="He2015resnet"></d-cite>) and a transformer-based architecture (<a href="https://huggingface.co/docs/transformers/model_doc/vit" target="_blank" rel="noopener noreferrer">ViT-base</a><d-cite key="Dosovitskiy2021"></d-cite>) for image classification as well as a transformer for natural language processing (<a href="https://huggingface.co/docs/transformers/model_doc/bert#transformers.FlaxBertForTokenClassification" target="_blank" rel="noopener noreferrer">Bert-base</a>.<d-cite key="Devlin2019"></d-cite>). We use the <code class="language-plaintext highlighter-rouge">Flax</code> and <code class="language-plaintext highlighter-rouge">PyTorch</code> implementations of these architectures available in the <a href="https://huggingface.co/docs/transformers/" target="_blank" rel="noopener noreferrer">transformers package</a> provided by <a href="https://huggingface.co" target="_blank" rel="noopener noreferrer">Hugging Face 🤗</a>.</p> <p>All computations were run on an Nvidia A100 GPU with 40 GB of memory. We used the version 0.4.21. of <code class="language-plaintext highlighter-rouge">Jax</code> and the version 2.1.1. of <code class="language-plaintext highlighter-rouge">torch</code>.</p> <p>The code of the benchmark is available on <a href="https://github.com/MatDag/bench_hvp/" target="_blank" rel="noopener noreferrer">this repo</a>.</p> <h3 id="time-complexity">Time complexity</h3> <p>The first comparison we make is a comparison in terms of wall-clock time between the different ways to compute HVPs and also the computation of a gradient by backpropagation. For each architecture, we compute the gradient of the model with respect to the parameters by backpropagation. We also compute the HVPs in <code class="language-plaintext highlighter-rouge">forward-over-reverse</code>, <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> and <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> modes. For each computation, we measure the time taken. Specifically for the HVPs, we subtract the time taken by a gradient computation, to get only the time of the overhead required by the HVP computation. The inputs for each architecture are generated randomly. For the ResNet34 architecture, we generated a batch of images of size 224x224x3. To limit out-of-memory issues in the experiments, we generated for the ViT architecture images of size 96x96x3. For the BERT architecture, we generated a batch of sequences of length 32.</p> <p>We first use <code class="language-plaintext highlighter-rouge">JAX</code> with just-in-time compilation. Each computation is run 90 times. We plot on the left of the figure, the median computation time and also the 20% and 80% percentile in black. The computations are done with a batch size of 128. We observe that, in practice, the overhead over the gradient computation for the HVP computation is between one and twice the time of a gradient computation for the three architectures. Consequently, a whole HVP computation takes between twice and three times the time of a gradient calculation. This is consistent with the theory. One can notice that the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> is slightly slower than the others in all the cases. The <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> and <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> are, as for them, very close in terms of time.</p> <p>We also report on the right figure the computational time of each method with respect to the batch size for the ResNet34 architecture. We observe, as expected, that the computational time scales linearly with the batch size.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We run a similar experiment with the functional API available in <code class="language-plaintext highlighter-rouge">PyTorch</code> <a href="https://pytorch.org/docs/stable/func.html" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">torch.func</code></a> similar to the one <code class="language-plaintext highlighter-rouge">JAX</code> has. The results we get are more contrasted.</p> <p>In the case of ResNet34, the scaling between the different methods is similar to the one we get with <code class="language-plaintext highlighter-rouge">JAX</code>. Also, during our experiments, we figured out that batch normalization made the forward computation slow and induced out-of-memory issues. Thus, we removed the batch normalization layers from the ResNet34 architecture.</p> <p>For ViT and BERT, the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> is surprisingly longer than the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> method. Moreover, the scaling between the gradient and HVP computational time differs from the one we get with <code class="language-plaintext highlighter-rouge">JAX</code>. Indeed, for these architectures, the HVP computations take between four and five more time than the gradient computations. This is a discrepancy with what we would expect in theory. This might be because, at the time we are writing this blog post, the functional API of <code class="language-plaintext highlighter-rouge">PyTorch</code> is still in its early stages. Particularly, we could not use the compilation with <code class="language-plaintext highlighter-rouge">torch.compile</code> because it does not work with some operators of <code class="language-plaintext highlighter-rouge">torch.func</code> such as <code class="language-plaintext highlighter-rouge">torch.func.jvp</code>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="memory-complexity">Memory complexity</h3> <p>We also compare the memory footprint of each approach. The following figure provides the results we get with jax jitted code. On the left, we represent the result for each method and model with a batch size of 64. On the right, we show the evolution of the memory footprint of each method for the ResNet34 with the batch size. Surprisingly, we could observe that the memory footprint of the different methods to compute HVPs does not vary for a given model. This is counterintuitive since we expect that the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> method have a larger memory footprint due to the double backpropagation.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>However, we do the same experiment by <em>disabling the JIT compilation</em>. The result we get corroborates the theory. Indeed, one can observe in the following figure that the memory footprint of the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> method is larger than the one of the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> and <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> methods. This is because the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> involves two successive backward differentiations while the other two involve only one reverse differentiation. Moreover, it scales linearly with the batch size, which was not the case in the previous figure in the small batch size regime.</p> <p>In light of these two results, the clever memory allocation performed during just-in-time compilation reduces significantly the memory footprint of the HVP computations.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the following figure, we plot the results we get with the <code class="language-plaintext highlighter-rouge">PyTorch</code> implementation. One can observe that in all the cases the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> consumes more memory in comparison with the <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> mode. It is almost at the same level as <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> mode, which is quite unexpected.</p> <p>The right plot of the evolution of the memory footprint with the batch size for the ResNet34 architecture evolves linearly as expected.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we have explored the different ways to compute HVP from theoretical and practical perspectives. The three take-home messages to keep in mind are the following:</p> <ul> <li> <p>We can compute HVPs without computing Hessian matrices.</p> </li> <li> <p>In practice, computing an HVP takes between twice and four times the time taken by a gradient computation and requires two to three times more memory than computing a gradient.</p> </li> <li> <p>The AD framework and the use or not of the just-in-time compilation affects the practical performances of HVPs computations in time and memory.</p> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2024/assets/bibliography/2024-05-07-bench-hvp.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2024" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>