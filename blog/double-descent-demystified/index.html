<!DOCTYPE html> <html> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=a[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"May 7, 2024"),r="Double Descent Demystified",l="Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2024"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2024},\n  year = {2024},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=a.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2024.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Double Descent Demystified | ICLR Blogposts 2024</title> <meta name="author" content="ICLR Blog"/> <meta name="description" content="Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle"/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/2024/assets/img/iclr_favicon.ico"/> <link rel="stylesheet" href="/2024/assets/css/main.css"> <link rel="canonical" href="https://iclr-blogposts.github.io/2024/blog/double-descent-demystified/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/2024/assets/js/theme.js"></script> <script src="/2024/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2024/assets/js/distillpub/template.v2.js"></script> <script src="/2024/assets/js/distillpub/transforms.v2.js"></script> <script src="/2024/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Double Descent Demystified",
      "description": "Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle",
      "published": "May 7, 2024",
      "authors": [
        {
          "author": "Rylan Schaeffer",
          "authorURL": "https://scholar.google.com/citations?user=6tMEGz8AAAAJ&hl=en",
          "affiliations": [
            {
              "name": "Stanford University",
              "url": ""
            }
          ]
        },
        {
          "author": "Zachary Robertson",
          "authorURL": "https://scholar.google.com/citations?user=769PIisAAAAJ&hl=en&oi=ao",
          "affiliations": [
            {
              "name": "Stanford University",
              "url": ""
            }
          ]
        },
        {
          "author": "Akhilan Boopathy",
          "authorURL": "https://scholar.google.com/citations?user=21alU7EAAAAJ&hl=en",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        },
        {
          "author": "Mikail Khona",
          "authorURL": "https://scholar.google.com/citations?user=K5f0SYQAAAAJ&hl=en&oi=ao",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        },
        {
          "author": "Kateryna Pistunova",
          "authorURL": "https://scholar.google.com/citations?user=V7QY5j0AAAAJ&hl=en",
          "affiliations": [
            {
              "name": "Stanford University",
              "url": ""
            }
          ]
        },
        {
          "author": "Jason W. Rocks",
          "authorURL": "https://scholar.google.com/citations?user=rFHAzMUAAAAJ",
          "affiliations": [
            {
              "name": "Boston University",
              "url": ""
            }
          ]
        },
        {
          "author": "Ila R. Fiete",
          "authorURL": "https://scholar.google.com/citations?user=uE-CihIAAAAJ&hl=en&oi=ao",
          "affiliations": [
            {
              "name": "MIT",
              "url": ""
            }
          ]
        },
        {
          "author": "Andrey Gromov",
          "authorURL": "https://scholar.google.com/citations?user=D056qfMAAAAJ&hl=en&oi=ao",
          "affiliations": [
            {
              "name": "UMD & Meta AI FAIR",
              "url": ""
            }
          ]
        },
        {
          "author": "Sanmi Koyejo",
          "authorURL": "https://scholar.google.com/citations?user=EaaOeJwAAAAJ&hl=en&oi=ao",
          "affiliations": [
            {
              "name": "Stanford University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2024/">ICLR Blogposts 2024</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2024/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">other iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/"><strong>2024</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" target="_blank" rel="noopener noreferrer">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Double Descent Demystified</h1> <p>Identifying, Interpreting &amp; Ablating the Sources of a Deep Learning Puzzle</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#double-descent-in-ordinary-linear-regression">Double Descent in Ordinary Linear Regression</a></div> <ul> <li><a href="#empirical-evidence">Empirical Evidence</a></li> <li><a href="#notation-and-terminology">Notation and Terminology</a></li> <li><a href="#mathematical-analysis">Mathematical Analysis</a></li> <li><a href="#factor-1-low-variance-in-training-features">Factor 1 - Low Variance in Training Features</a></li> <li><a href="#factor-2-test-features-in-training-feature-subspace">Factor 2 - Test Features in Training Feature Subspace</a></li> <li><a href="#factor-3-errors-from-best-possible-model">Factor 3 - Errors from Best Possible Model</a></li> <li><a href="#divergence-at-the-interpolation-threshold">Divergence at the Interpolation Threshold</a></li> <li><a href="#generalization-in-overparameterized-linear-regression">Generalization in Overparameterized Linear Regression</a></li> </ul> <div><a href="#adversarial-data">Adversarial Data</a></div> <ul> <li><a href="#adversarial-test-examples">Adversarial Test Examples</a></li> <li><a href="#adversarial-training-data">Adversarial Training Data</a></li> </ul> <div><a href="#intuition-for-nonlinear-models">Intuition for Nonlinear Models</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Machine learning models, while incredibly powerful, can sometimes act unpredictably. One of the most intriguing behaviors is when the test loss suddenly diverges at the interpolation threshold, a phenomenon distinctly observed in <strong>double descent</strong> <d-cite key="vallet1989hebb"></d-cite><d-cite key="krogh1991simple"></d-cite><d-cite key="geman1992neural"></d-cite><d-cite key="krogh1992generalization"></d-cite><d-cite key="opper1995statistical"></d-cite><d-cite key="duin2000classifiers"></d-cite><d-cite key="spigler2018jamming"></d-cite><d-cite key="belkin2019reconciling"></d-cite><d-cite key="bartlett2020benign"></d-cite><d-cite key="belkin2020twomodels"></d-cite><d-cite key="nakkiran2021deep"></d-cite><d-cite key="poggio2019double"></d-cite><d-cite key="advani2020high"></d-cite><d-cite key="liang2020just"></d-cite><d-cite key="adlam2020understanding"></d-cite><d-cite key="rocks2022memorizing"></d-cite><d-cite key="rocks2021geometry"></d-cite><d-cite key="rocks2022bias"></d-cite><d-cite key="mei2022generalization"></d-cite><d-cite key="hastie2022surprises"></d-cite><d-cite key="bach2023highdimensional"></d-cite><d-cite key="curth2024u"></d-cite>.</p> <div id="fig_unablated_all"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/unablated-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/unablated-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/unablated-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/unablated.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/unablated-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/unablated-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/unablated-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/unablated.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/unablated-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/unablated-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/unablated-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/unablated.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/unablated-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/unablated-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/unablated-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/unablated.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. <b>Double descent in ordinary linear regression.</b> Three real datasets (California Housing, Diabetes, and WHO Life Expectancy) and one synthetic dataset (Student-Teacher) all exhibit double descent, with test loss spiking at the interpolation threshold. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>While significant theoretical work has been done to comprehend why double descent occurs, it can be difficult for a newcomer to gain a general understanding of why the test loss behaves in this manner, and under what conditions one should expect similar misbehavior. In this blog post, when we say double descent, we mean the divergence at the interpolation threshold, and not whether overparameterized models generalize (or fail to generalize).</p> <p>In this work, we intuitively and quantitatively explain why the test loss diverges at the interpolation threshold, with as much generality as possible and with as simple of mathematical machinery as possible, but also without sacrificing rigor. To accomplish this, we focus on the simplest supervised model - ordinary linear regression - using the most basic linear algebra primitive: the singular value decomposition. We identify three distinct interpretable factors which, when collectively present, trigger the divergence. Through practical experiments on real data sets, we confirm that both model’s test losses diverge at the interpolation threshold, and this divergence vanishes when even one of the three factors is removed. We complement our understanding by offering a geometric picture that reveals linear models perform representation learning when overparameterized, and conclude by shedding light on recent results in nonlinear models concerning superposition.</p> <h2 id="double-descent-in-ordinary-linear-regression">Double Descent in Ordinary Linear Regression</h2> <h3 id="empirical-evidence-of-double-descent-in-ordinary-linear-regression">Empirical Evidence of Double Descent in Ordinary Linear Regression</h3> <p>Before studying ordinary linear regression mathematically, does our claim that it exhibits double descent hold empirically? We show that it indeed does, using one synthetic and three real datasets: World Health Organization Life Expectancy <d-cite key="gochiashvili_2023_who"></d-cite>, California Housing <d-cite key="pace1997sparse"></d-cite>, Diabetes <d-cite key="efron2004least"></d-cite>; these three real datasets were selected on the basis of being easily accessible through sklearn <d-cite key="scikit-learn"></d-cite> or Kaggle. As shown in <a href="#fig_unablated_all">Fig 1</a>, all display a spike in test mean squared error at the interpolation threshold. Our simple Python code is <a href="">publicly available</a>.</p> <h3 id="notation-and-terminology">Notation and Terminology</h3> <p>Consider a regression dataset of $N$ training data with features $\vec{x}_n \in \mathbb{R}^D$ and targets $y_n \in \mathbb{R}$. We sometimes use matrix-vector notation to refer to the training data:</p> \[X \in \mathbb{R}^{N \times D} \quad , \quad Y \in \mathbb{R}^{N \times 1}.\] <p>In ordinary linear regression, we want to learn parameters $\hat{\vec{\beta}} \in \mathbb{R}^{D}$ such that:</p> \[\vec{x}_n \cdot \hat{\vec{\beta}} \approx y_n.\] <p>We will study three key parameters:</p> <ol> <li>The number of model parameters $P$</li> <li>The number of training data $N$</li> <li>The dimensionality of the data $D$</li> </ol> <p>We say that a model is <em>overparameterized</em> if $N &lt; P$ and <em>underparameterized</em> if $N &gt; P$. The <em>interpolation threshold</em> refers to $N=P$, because when $N\leq P$, the model can perfectly interpolate the training points. Recall that in ordinary linear regression, the number of parameters $P$ equals the dimension $D$ of the covariates. Consequently, rather than thinking about changing the number of parameters $P$, we’ll instead think about changing the number of data points $N$.</p> <h3 id="mathematical-analysis-of-ordinary-linear-regression">Mathematical Analysis of Ordinary Linear Regression</h3> <p>To understand under what conditions and why double descent occurs at the interpolation threshold in linear regression, we’ll study the two parameterization regimes. If the regression is <em>underparameterized</em>, we estimate the linear relationship between covariates $\vec{x}_n$ and target $y_n$ by solving the least-squares minimization problem:</p> \[\begin{align*} \hat{\vec{\beta}}_{under} \, &amp;:= \, \arg \min_{\vec{\beta}} \frac{1}{N} \sum_n ||\vec{x}_n \cdot \vec{\beta} - y_n||_2^2\\ \, &amp;:= \, \arg \min_{\vec{\beta}} ||X \vec{\beta} - Y ||_2^2. \end{align*}\] <p>The solution is the ordinary least squares estimator based on the second moment matrix $X^T X$:</p> \[\hat{\vec{\beta}}_{under} = (X^T X)^{-1} X^T Y.\] <p>If the model is overparameterized, the optimization problem is ill-posed since we have fewer constraints than parameters. Consequently, we choose a different (constrained) optimization problem that asks for the minimum norm parameters that still perfectly interpolate the training data:</p> \[\begin{align*} \hat{\vec{\beta}}_{over} \, &amp;:= \, \arg \min_{\vec{\beta}} ||\vec{\beta}||_2^2\\ \text{s.t.} \quad \quad \forall \, n \in &amp;\{1, ..., N\}, \quad \vec{x}_n \cdot \vec{\beta} = y_n. \end{align*}\] <p>We choose this optimization problem because it is the one gradient descent implicitly minimizes. The solution to this optimization problem uses the Gram matrix $X X^T \in \mathbb{R}^{N \times N}$:</p> \[\hat{\vec{\beta}}_{over} = X^T (X X^T)^{-1} Y.\] <p>One way to see why the Gram matrix appears is via constrained optimization: define the Lagrangian $\mathcal{L}(\vec{\beta}, \vec{\lambda}) \, := \, \frac{1}{2}||\vec{\beta}||_2^2 + \vec{\lambda}^T (Y - X \vec{\beta})$ with Lagrange multipliers $\vec{\lambda} \in \mathbb{R}^N$, then differentiate with respect to the parameters and Lagrange multipliers to obtain the overparameterized solution.</p> <p>After being fit, for test point $\vec{x}_{test}$, the model will make the following predictions:</p> \[\hat{y}_{test, under} = \vec{x}_{test} \cdot \hat{\vec{\beta}}_{under} = \vec{x}_{test} \cdot (X^T X)^{-1} X^T Y\] \[\hat{y}_{test, over} = \vec{x}_{test} \cdot \hat{\vec{\beta}}_{over} = \vec{x}_{test} \cdot X^T (X X^T)^{-1} Y.\] <p>Hidden in the above equations is an interaction between three quantities that can, when all grow extreme, create a divergence in the test loss!</p> <p>To reveal the three quantities, we’ll rewrite the regression targets by introducing a slightly more detailed notation. Unknown to us, there are some ideal linear parameters $\vec{\beta}^* \in \mathbb{R}^P = \mathbb{R}^D$ that truly minimize the test mean squared error. We can write any regression target as the inner product of the data $\vec{x}_n$ and the ideal parameters $\vec{\beta}^*$, plus an additional error term $e_n$ that is an “uncapturable” residual from the “viewpoint” of the model class</p> \[y_n = \vec{x}_n \cdot \vec{\beta}^* + e_n.\] <p>In matrix-vector form, we will equivalently write:</p> \[Y = X \vec{\beta}^* + E,\] <p>with $E \in \mathbb{R}^{N \times 1}$. To be clear, we are <em>not</em> imposing assumptions. Rather, we are introducing notation to express that there are (unknown) ideal linear parameters, and possibly non-zero errors $E$ that even the ideal model might be unable to capture; these errors $E$ could be random noise or could be fully deterministic patterns that this particular model class cannot capture. Using this new notation, we rewrite the model’s predictions to show how the test datum’s features $\vec{x}_{test}$, training data’s features $X$ and training data’s regression targets $Y$ interact.</p> <p>Let $y_{test}^* := \vec{x}_{test} \cdot \vec{\beta}^*$. In the underparameterized regime:</p> \[\begin{align*} \hat{y}_{test,under} &amp;= \vec{x}_{test} \cdot \hat{\vec{\beta}}_{under}\\ &amp;=\vec{x}_{test} \cdot (X^T X)^{-1} X^T Y\\ &amp;=\vec{x}_{test} \cdot (X^T X)^{-1} X^T (X \vec{\beta}^* + E)\\ &amp;=\vec{x}_{test} \cdot \vec{\beta}^* + \, \vec{x}_{test} \cdot (X^T X)^{-1} X^T E\\ \hat{y}_{test,under} - y_{test}^* &amp;= \vec{x}_{test} \cdot (X^T X)^{-1} X^T E. \end{align*}\] <p>This equation is important, but opaque. To extract the intuition, replace $X$ with its singular value decomposition $X = U S V^T$. Let $R \, := \, \text{rank}(X)$ and let $\sigma_1 &gt; \sigma_2 &gt; … &gt; \sigma_R &gt; 0$ be $X$’s (non-zero) singular values. Let $S^+$ denote the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" target="_blank" rel="noopener noreferrer">Moore-Penrose inverse</a>; in this context, this means that if a singular value $\sigma_r$ is non-zero, then in $S^+$, it becomes its reciprocal $1/\sigma_r$, but if the singular value is zero, then in $S^+$, it remains $0$. We can decompose the underparameterized prediction error along the orthogonal singular modes:</p> \[\begin{align*} \hat{y}_{test, under} - y_{test}^* &amp;= \vec{x}_{test} \cdot V S^{+} U^T E\\ &amp;= \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E). \end{align*}\] <p>This equation will be critical! The same term will appear in the overparameterized regime (plus one additional term):</p> \[\begin{align*} \hat{y}_{test,over} &amp;= \vec{x}_{test} \cdot \hat{\vec{\beta}}_{over}\\ &amp;= \vec{x}_{test} \cdot X^T (X X^T)^{-1} Y\\ &amp;= \vec{x}_{test} \cdot X^T (X X^T)^{-1} (X \beta^* + E)\\ \hat{y}_{test,over} - y_{test}^* &amp;= \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^* \\ &amp;\quad\quad + \quad \vec{x}_{test} \cdot X^T (X X^T)^{-1} E\\ &amp;= \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^* \\ &amp;\quad\quad + \quad \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E), \end{align*}\] <p>where the last step again replaced $X$ with its SVD $X = U S V^T$. Thus, the prediction errors in the overparameterized and underparameterized regimes will be:</p> \[\begin{align*} \hat{y}_{test,over} - y_{test}^* &amp;= \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E)\\ &amp;\quad \quad + \quad \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^*\\ \hat{y}_{test,under} - y_{test}^* &amp;= \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E). \end{align*}\] <p>The shared term in the two prediction errors causes the divergence:</p> \[\begin{equation} \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E). \label{eq:variance} \end{equation}\] <p>Eqn. \ref{eq:variance} is critical. It reveals that our test prediction error (and thus, our test squared error!) will depend on an interaction between 3 quantities:</p> <ol> <li> <p>How much the training features vary in each direction. More formally, the inverse (non-zero) singular values of the <em>training features</em> $X$:</p> \[\frac{1}{\sigma_r}\] </li> <li> <p>How much, and in which directions, the test features vary relative to the training features. More formally: how $\vec{x}_{test}$ projects onto $X$’s right singular vectors $V$:</p> \[\vec{x}_{test} \cdot \vec{v}_r\] </li> <li> <p>How well the best possible model in the model class can correlate the variance in the training features with the training regression targets. More formally: how the residuals $E$ of the best possible model in the model class (i.e. insurmountable “errors” from the “perspective” of the model class) project onto $X$’s left singular vectors $U$:</p> \[\vec{u}_r \cdot E\] </li> </ol> <p>We use the term “vary” when discussing $\vec{v}_r$ because $V$ can be related to the empirical (or sample) covariance matrix oftentimes studied in Principal Component Analysis. That is, if the SVD of $X$ is $U S V^T$, then $\frac{1}{N} X^T X = \frac{1}{N} V S^2 V^T$. If the training data are centered (a common preprocessing step), then this is the empirical covariance matrix and its eigenvectors $\vec{v}_1, …, \vec{v}_R$ identify the orthogonal directions of variance. We’ll return to this in <a href="#fig_geometric_smallest_nonzero_singular_value">Fig 6</a>.</p> <p><strong>Why does the test error diverge?</strong> When (1) and (3) are both present in the learning problem, the model’s parameters along this singular mode are likely incorrect. When (2) is added to the mix by a test datum $\vec{x}_{test}$ with a large projection along this mode, the model is forced to extrapolate significantly beyond what it saw in the training data, in a direction where the training data had an error-prone relationship between its predictions and the training targets, using parameters that are likely wrong. As a consequence, the test squared error explodes!</p> <h3 id="factor-1---low-variance-in-training-features">Factor 1 - Low Variance in Training Features</h3> <div id="fig_factor_1_small_singular_values"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_small_singular_values-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_small_singular_values-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_small_singular_values-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_small_singular_values.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_small_singular_values-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_small_singular_values-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_small_singular_values-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_small_singular_values.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_small_singular_values-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_small_singular_values-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_small_singular_values-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_small_singular_values.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_small_singular_values-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_small_singular_values-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_small_singular_values-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_small_singular_values.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. <b>Required Factor #1: How much training features vary in each direction.</b> The test loss diverges at the interpolation threshold only if training features $X$ contain small (non-zero) singular values. Ablation: By removing all singular values below a cutoff, the divergence at the interpolation threshold is diminished or disappears entirely. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>The test loss will not diverge if any of the three required factors are absent. What could cause that? One way is if small-but-nonzero singular values do not appear in the training data features. One way to accomplish this is by setting all singular values below a selected threshold to exactly 0. To test our understanding, we independently ablate all small singular values in the training features. Specifically, as we run the ordinary linear regression fitting process, and as we sweep the number of training data, we also sweep different singular value cutoffs and remove all singular values of the training features $X$ below the cutoff (<a href="#fig_factor_1_small_singular_values">Fig 2</a>).</p> <h3 id="factor-2---test-features-in-training-feature-subspace">Factor 2 - Test Features in Training Feature Subspace</h3> <div id="fig_test_feat_in_train_feat_subspace"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_feat_in_train_feat_subspace-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_feat_in_train_feat_subspace-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_feat_in_train_feat_subspace-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_feat_in_train_feat_subspace.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_feat_in_train_feat_subspace-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_feat_in_train_feat_subspace-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_feat_in_train_feat_subspace-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_feat_in_train_feat_subspace.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_feat_in_train_feat_subspace-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_feat_in_train_feat_subspace-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_feat_in_train_feat_subspace-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_feat_in_train_feat_subspace.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_feat_in_train_feat_subspace-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_feat_in_train_feat_subspace-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_feat_in_train_feat_subspace-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_feat_in_train_feat_subspace.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. <b>Required Factor #2: How much, and in which directions, test features vary relative to training features.</b> The test loss diverges only if the test features $\vec{x}_{test}$ have a large projection onto the training features $X$'s right singular vectors $V$. Ablation: By projecting the test features into the subspace of the leading singular modes, the divergence at the interpolation threshold is diminished or disappears entirely. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>Double descent should not occur if the test datum does not vary in different directions than the training features. Specifically, if the test datum lies entirely in the subspace of just a few of the leading singular directions, then the divergence is unlikely to occur. To test our understanding, we force the test data features to lie in the training features subspace: as we run the ordinary linear regression fitting process, and as we sweep the number of training data, we project the test features $\vec{x}_{test}$ onto the subspace spanned by the training features $X$ singular modes (<a href="#fig_test_feat_in_train_feat_subspace">Fig 3</a>).</p> <h3 id="factor-3---errors-from-best-possible-model">Factor 3 - Errors from Best Possible Model</h3> <div id="fig_no_residuals_in_ideal"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_residuals_in_ideal-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_residuals_in_ideal-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_residuals_in_ideal-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_residuals_in_ideal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_residuals_in_ideal-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_residuals_in_ideal-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_residuals_in_ideal-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_residuals_in_ideal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_residuals_in_ideal-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_residuals_in_ideal-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_residuals_in_ideal-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_residuals_in_ideal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_residuals_in_ideal-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_residuals_in_ideal-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_residuals_in_ideal-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_residuals_in_ideal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. <b>Required Factor #3: How well the best possible model in the model class can correlate variance in training features with training targets.</b> The test loss diverges only if the residuals $E$ from the best possible model in the model class on the training data have a large projection onto the training features $X$'s left singular vectors $U$. Ablation: By ensuring the true relationship between features and targets is within the model class i.e. linear, the divergence at the interpolation threshold disappears. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>Double descent should not occur if the best possible model in the model class makes no errors on the training data. For example, if we use a linear model class on data where the true relationship is a noiseless linear relationship, then at the interpolation threshold, we will have $D=P$ data, $P=D$ parameters, our line of best fit will exactly match the true relationship, and no divergence will occur. To test our understanding, we ensure no residual errors exist in the best possible model: we first use the entire dataset to fit a linear model, then replace all target values with the predictions made by the ideal linear model. We then rerun our typical fitting process using these new labels, sweeping the number of training data (<a href="#fig_no_residuals_in_ideal">Fig 4</a>).</p> <p>As a short aside, what could cause residual errors in the best possible model in the model class?</p> <ol> <li> <strong>Noise</strong>: If the data is noisy, then the best possible model in the model class will have residual errors.</li> <li> <strong>Model Misspecification</strong>: If the data is generated by a nonlinear model, but we use a linear model class (or vice versa), then the best possible model in the model class will have residual errors.</li> <li> <strong>Missing Features</strong>: Even if the data is noiseless and our model belongs to the correct model class, but we are missing covariates, then the best possible model in the model class will still have residual errors.</li> </ol> <h3 id="divergence-at-the-interpolation-threshold">Divergence at the Interpolation Threshold</h3> <div id="fig_least_informative_singular_value"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/least_informative_singular_value-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/least_informative_singular_value-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/least_informative_singular_value-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/least_informative_singular_value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/least_informative_singular_value-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/least_informative_singular_value-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/least_informative_singular_value-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/least_informative_singular_value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/least_informative_singular_value-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/least_informative_singular_value-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/least_informative_singular_value-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/least_informative_singular_value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/least_informative_singular_value-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/least_informative_singular_value-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/least_informative_singular_value-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/least_informative_singular_value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5. <b>The training features are most likely to obtain their smallest non-zero singular value when approaching the interpolation threshold.</b> </div> </div> <p>Why does this divergence happen near the interpolation threshold? The answer is that the first factor (small non-zero singular values in the training features $X$) is likely to occur at the interpolation threshold (<a href="#fig_least_informative_singular_value">Fig 5</a>), but why?</p> <p>Suppose we’re given a single training datum \(\vec{x}_1\). So long as this datum isn’t exactly zero, that datum varies in a single direction, meaning we gain information about the variance in that direction, but the variance in all orthogonal directions is exactly 0. With the second training datum \(\vec{x}_2\), so long as this datum isn’t exactly zero, that datum varies, but now, some fraction of \(\vec{x}_2\) might have a positive projection along \(\vec{x}_1\); if this happens (and it likely will, since the two vectors are unlikely to be exactly orthogonal), the shared direction gives us <em>more</em> information about the variance in this shared direction, but <em>less</em> information about the second orthogonal direction of variation. Ergo, the training data’s smallest non-zero singular value after 2 samples is probabilistically smaller than after 1 sample. As we approach the interpolation threshold, the probability that each additional datum has large variance in a new direction orthogonal to all previous directions grows unlikely (<a href="#fig_geometric_smallest_nonzero_singular_value">Fig 5</a>), but as we move beyond the interpolation threshold, the variance in each covariate dimension becomes increasingly clear.</p> <div id="fig_geometric_smallest_nonzero_singular_value"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=1-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=2-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=3-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=8-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=8-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=8-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=100-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=100-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=100-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=100.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6. <b>Geometric intuition for why the smallest non-zero singular value reaches its lowest value near the interpolation threshold.</b> If $1$ datum is observed, variance exists in only 1 direction. If $2$ data are observed, a second axis of variation appears, but because the two data are likely to share some component, the second axis is likely to have less variance than the first. At the interpolation threshold (here, $D=P=N=3$), because the three data are likely to share components along the first two axes, the third axis is likely to have even less variance. Beyond the interpolation threshold, additional data contribute additional variance to these three axes. </div> </div> <h3 id="generalization-in-overparameterized-linear-regression">Generalization in Overparameterized Linear Regression</h3> <p>You might be wondering why three of the datasets have low test squared error in the overparameterized regime (California Housing, Diabetes, Student-Teacher) but one (WHO Life Expectancy) does not. Recall that the overparameterized regime’s prediction error has another term \(\hat{y}_{test,over} - y_{test}^*\) not present in the underparameterized regime:</p> \[\begin{equation} \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^*. \label{eq:bias} \end{equation}\] <p>To understand why this bias exists, recall that our goal is to correlate fluctuations in the covariates $\vec{x}$ with fluctuations in the targets $y$. In the overparameterized regime, there are more parameters than data; consequently, for $N$ data points in $D=P$ dimensions, the model can “see” fluctuations in at most $N$ dimensions, but has no ``visibility” into the remaining $P-N$ dimensions. This causes information about the optimal linear relationship $\vec{\beta}^*$ to be lost, thereby increasing the overparameterized prediction error.</p> <div id="fig_overparameterized_generalization"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/overparameterized_generalization-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/overparameterized_generalization-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/overparameterized_generalization-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/overparameterized_generalization.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 7. <b>Geometry of Generalization in Overparameterized Ordinary Linear Regression.</b> The rowspace of the training features $X$ forms a subspace (here, $\mathbb{R}^1$) of the ambient space (here, $\mathbb{R}^2$). For test datum $\vec{x}_{test}$, the linear model forms an internal representation of the test datum $\hat{\vec{x}}_{test}$ by orthogonally projecting the test datum onto the rowspace via projection matrix $X^T (X X^T)^{-1} X$. The generalization error will then increase commensurate with the inner product between $\hat{\vec{x}}_{test} - \vec{x}_{test}$ and the best possible parameters for the function class $\vec{\beta}^*$. Three different possible $\vec{\beta}^*$ are shown with <span style="color:blue;">low (blue)</span>, <span style="color:green;">medium (green)</span> and <span style="color:red;">high (red)</span> generalization errors. </div> </div> <p>We previously saw that away from the interpolation threshold, the variance is unlikely to affect the discrepancy between the overparameterized model’s predictions and the ideal model’s predictions, meaning most of the discrepancy must therefore emerge from the bias (Eqn. \ref{eq:bias}). This bias term yields an intuitive geometric picture (<a href="#fig_overparameterized_generalization">Fig 7</a>) that also reveals a surprising fact: <em>overparameterized linear regression does representation learning!</em> Specifically, for test datum \(\vec{x}_{test}\), a linear model creates a representation of the test datum \(\hat{\vec{x}}_{test}\) by orthogonally projecting the test datum onto the row space of the training covariates \(X\) via the projection matrix \(X^T (X X^T)^{-1} X\):</p> \[\begin{equation*} \hat{\vec{x}}_{test} := X^T (X X^T)^{-1} X \; \vec{x}_{test}. \end{equation*}\] <p>Seen this way, the bias can be rewritten as the inner product between (1) the difference between its representation of the test datum and the test datum and (2) the ideal linear model’s fit parameters:</p> \[\begin{equation}\label{eq:overparam_gen_bias} (\hat{\vec{x}}_{test} - \vec{x}_{test}) \cdot \vec{\beta}^*. \end{equation}\] <div id="fig_test_bias_squared"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_bias_squared-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_bias_squared-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_bias_squared-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_bias_squared.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_bias_squared-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_bias_squared-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_bias_squared-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_bias_squared.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_bias_squared-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_bias_squared-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_bias_squared-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_bias_squared.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_bias_squared-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_bias_squared-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_bias_squared-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_bias_squared.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 8. <b>Test Error of Overparameterized Models.</b> Large inner product between the ideal model's parameters and the difference between the fit model's internal representations of the test data and the test data creates large test squared error for overparameterized models. </div> </div> <p>Intuitively, an overparameterized model will generalize well if the model’s representations capture the essential information necessary for the best model in the model class to perform well (<a href="#fig_test_bias_squared">Fig. 8</a>).</p> <h2 id="adversarial-test-data-and-adversarial-training-data">Adversarial Test Data and Adversarial Training Data</h2> <p>Our key equation (Eqn. \ref{eq:variance}) also reveals <em>why</em> adversarial test data and adversarial training data exist (at least in linear regression) and <em>how</em> mechanistically they function. For convenience, we repeat the equation:</p> \[\begin{equation*} \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E). \end{equation*}\] <p>Adversarial test examples are a well-known phenomenon in machine learning <d-cite key="szegedy2013intriguing"></d-cite> <d-cite key="goodfellow2014explaining"></d-cite> <d-cite key="kurakin2018adversarial"></d-cite> <d-cite key="athalye2018synthesizing"></d-cite> <d-cite key="xie2022word"></d-cite> that we can see in this equation. The adversarial test features correspond to \(\vec{x}_{test} \cdot \vec{v}_r\) being large, where one can drastically increase the test squared error by moving the test example in the direction of the right singular vector(s) with the smallest non-zero singular values (<a href="#fig_adversarial_train_data">Fig 9</a>).</p> <div id="fig_test_bias_squared"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_test_datum-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_test_datum-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_test_datum-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_test_datum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_test_datum-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_test_datum-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_test_datum-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_test_datum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_test_datum-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_test_datum-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_test_datum-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_test_datum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_test_datum-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_test_datum-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_test_datum-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_test_datum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 9. <b>Adversarial Test Examples in Linear Regression.</b> Adversarial examples arise by pushing $\vec{x}_{test}$ far along the trailing singular modes in the training features $X$. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>Less well-known are adversarial training data, akin to dataset poisoning <d-cite key="biggio2012poisoning"></d-cite> <d-cite key="steinhardt2017certified"></d-cite> <d-cite key="wallace2020concealed"></d-cite> <d-cite key="carlini2021contrastive"></d-cite> <d-cite key="carlini2021poisoning"></d-cite> <d-cite key="schuster2021you"></d-cite> or backdoor attacks <d-cite key="chen2017targeted"></d-cite> <d-cite key="gu2017badnets"></d-cite> <d-cite key="carlini2021contrastive"></d-cite>. Adversarial training examples correspond to \(\vec{u}_r \cdot E\) being large, where one can drastically increase the test squared error by moving the training errors $E$ in the direction of the left singular vector(s) with the smallest non-zero singular value. This gives a practical way to construct <em>adversarial training data</em>: training features and targets whose training loss is unchanged from unaltered training data, but causes the test loss to be 1-3 orders of magnitude larger (<a href="#fig_adversarial_train_data">Fig 10</a>).</p> <div id="fig_adversarial_train_data"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_train_data-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_train_data-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_train_data-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_train_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_train_data-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_train_data-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_train_data-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_train_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_train_data-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_train_data-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_train_data-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_train_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_train_data-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_train_data-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_train_data-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_train_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 10. <b>Adversarial Training Dataset in Linear Regression.</b> By manipulating the residual errors $E$ that the best possible model in the model class achieves on the training data, we construct training datasets that increase the test error of the learned model by 1-3 orders of magnitude without affecting its training error. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <h2 id="intuition-for-nonlinear-models">Intuition for Nonlinear Models</h2> <p>Although we mathematically studied ordinary linear regression, the intuition for why the test loss diverges extends to nonlinear models, such as polynomial regression and including certain classes of deep neural networks <d-cite key="jacot2018neural"></d-cite> <d-cite key="lee2017deep"></d-cite> <d-cite key="bordelon2020spectrum"></d-cite>. For a concrete example about how our intuition can shed light on the behavior of nonlinear models, Henighan et al. 2023 <d-cite key="henighan2023superposition"></d-cite> recently discovered interesting properties of shallow nonlinear autoencoders: depending on the number of training data, (1) autoencoders either store data points or features, and (2) the test loss increases sharply between these two regimes (<a href="#fig_henighan">Fig. 11</a>).</p> <div id="fig_henighan"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/henighan2023superposition-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/henighan2023superposition-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/henighan2023superposition-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/henighan2023superposition.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 11. <b>Superposition, Memorization and Double Descent in Nonlinear Shallow Autoencoders.</b> Figure from Henighan et al. 2023 <d-cite key="henighan2023superposition"></d-cite>. </div> </div> <p>Our work sheds light on the results in two ways:</p> <ol> <li> <p>Henighan et al. 2023 write, “It’s interesting to note that we’re observing double descent in the absence of label noise.” Our work clarifies that noise, in the sense of a random quantity, is <em>not</em> necessary to produce double descent. Rather, what is necessary is <em>residual errors from the perspective of the model class</em> ($E$, in our notation). Those errors could be entirely deterministic, such as a nonlinear model attempting to fit a noiseless linear relationship, or other model misspecifications.</p> </li> <li> <p>Henighan et al. 2023 write, “[Our work] suggests a naive mechanistic theory of overfitting and memorization: memorization and overfitting occur when models operate on ‘data point features’ instead of ‘generalizing features’.” Our work hopefully clarifies that this dichotomy is incorrect: when overparameterized, data point features are akin to the Gram matrix $X X^T$ and when underparameterized, generalizing features are akin to the second moment matrix $X^T X$. Our work hopefully clarifies that data point features can and very often do generalize, and that there is a deep connection between the two, i.e., their shared spectra.</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <p>In this work, we intuitively and quantitatively explained why the test loss misbehaves based on three interpretable factors, tested our understanding via ablations, connected our understanding to adversarial test examples and adversarial training datasets, and added conceptual clarity of recent discoveries in nonlinear models.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2024/assets/bibliography/2024-05-07-double-descent-demystified.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2024" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>