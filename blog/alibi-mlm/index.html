<!DOCTYPE html> <html> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},i=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),a=i[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"May 7, 2024"),r="Masked Language Model with ALiBi and CLAP head",l="As a new approach to positional encoding, Attention with Linear Biases (ALiBi) uses linear biases of the attention weights to encode positional information, with capability of context length extrapolation. In their paper however, Press et al. focus on the perplexity of autoregressive decoder-only language models, leaving the question of downstream tasks and its applicability to encoder-attention open. In this blogpost, we attempt to bridge the gap by testing masked language models (MLMs) with encoder-attention ALiBi and prediction head similar to the counterparts of the original ALiBi models. We find that while simplified prediction head may be beneficial, performance of MLMs with encoder-attention ALiBi starts to deteriorate with 2048 sequence length at larger scales. We put our results in the context of related recent experiments and tentatively identify the circumstances more challenging to positional encoding designs. Finally, we open-source our MLMs, with BERT-level performance and 2048 context length.";{let e=i.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(a+"2024"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2024},\n  year = {2024},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=i.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2024.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Masked Language Model with ALiBi and CLAP head | ICLR Blogposts 2024</title> <meta name="author" content="ICLR Blog"/> <meta name="description" content="As a new approach to positional encoding, Attention with Linear Biases (ALiBi) uses linear biases of the attention weights to encode positional information, with capability of context length extrapolation. In their paper however, Press et al. focus on the perplexity of autoregressive decoder-only language models, leaving the question of downstream tasks and its applicability to encoder-attention open. In this blogpost, we attempt to bridge the gap by testing masked language models (MLMs) with encoder-attention ALiBi and prediction head similar to the counterparts of the original ALiBi models. We find that while simplified prediction head may be beneficial, performance of MLMs with encoder-attention ALiBi starts to deteriorate with 2048 sequence length at larger scales. We put our results in the context of related recent experiments and tentatively identify the circumstances more challenging to positional encoding designs. Finally, we open-source our MLMs, with BERT-level performance and 2048 context length."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/2024/assets/img/iclr_favicon.ico"/> <link rel="stylesheet" href="/2024/assets/css/main.css"> <link rel="canonical" href="https://iclr-blogposts.github.io/2024/blog/alibi-mlm/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/2024/assets/js/theme.js"></script> <script src="/2024/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2024/assets/js/distillpub/template.v2.js"></script> <script src="/2024/assets/js/distillpub/transforms.v2.js"></script> <script src="/2024/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Masked Language Model with ALiBi and CLAP head",
      "description": "As a new approach to positional encoding, Attention with Linear Biases (ALiBi) uses linear biases of the attention weights to encode positional information, with capability of context length extrapolation. In their paper however, Press et al. focus on the perplexity of autoregressive decoder-only language models, leaving the question of downstream tasks and its applicability to encoder-attention open. In this blogpost, we attempt to bridge the gap by testing masked language models (MLMs) with encoder-attention ALiBi and prediction head similar to the counterparts of the original ALiBi models. We find that while simplified prediction head may be beneficial, performance of MLMs with encoder-attention ALiBi starts to deteriorate with 2048 sequence length at larger scales. We put our results in the context of related recent experiments and tentatively identify the circumstances more challenging to positional encoding designs. Finally, we open-source our MLMs, with BERT-level performance and 2048 context length.",
      "published": "May 7, 2024",
      "authors": [
        {
          "author": "Jason Chuan-Chih Chou",
          "authorURL": "https://scholar.google.com/citations?user=V7BXGawAAAAJ",
          "affiliations": [
            {
              "name": "Cohere For AI Community",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2024/">ICLR Blogposts 2024</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2024/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">other iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/"><strong>2024</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" target="_blank" rel="noopener noreferrer">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Masked Language Model with ALiBi and CLAP head</h1> <p>As a new approach to positional encoding, Attention with Linear Biases (ALiBi) uses linear biases of the attention weights to encode positional information, with capability of context length extrapolation. In their paper however, Press et al. focus on the perplexity of autoregressive decoder-only language models, leaving the question of downstream tasks and its applicability to encoder-attention open. In this blogpost, we attempt to bridge the gap by testing masked language models (MLMs) with encoder-attention ALiBi and prediction head similar to the counterparts of the original ALiBi models. We find that while simplified prediction head may be beneficial, performance of MLMs with encoder-attention ALiBi starts to deteriorate with 2048 sequence length at larger scales. We put our results in the context of related recent experiments and tentatively identify the circumstances more challenging to positional encoding designs. Finally, we open-source our MLMs, with BERT-level performance and 2048 context length.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#attention-with-linear-biases-alibi">Attention with Linear Biases (ALiBi)</a></div> <div><a href="#contrastive-language-pretraining-clap-head">Contrastive Language Pretraining (CLAP) Head</a></div> <div><a href="#experiments">Experiments</a></div> <ul> <li><a href="#wikitext-103">WikiText-103</a></li> <li><a href="#the-pile">The Pile</a></li> </ul> <div><a href="#conclusions">Conclusions</a></div> <div><a href="#model-checkpoints">Model Checkpoints</a></div> </nav> </d-contents> <p><em>Adapted and expanded from <a href="https://github.com/EIFY/fairseq" target="_blank" rel="noopener noreferrer">EIFY/fairseq</a>.</em></p> <p>Unmodified and unmasked, attention mechanism is permutation-invariant and positional encoding is therefore employed by transformer-based language models to break the symmetry and enable sequence modeling. In their ICLR 2022 paper, Press et al. <d-cite key="DBLP:conf/iclr/PressSL22"></d-cite> introduced Attention with Linear Biases (ALiBi) as a new approach to positional encoding, where the positional info of the tokens are encoded by applying an attention weight bias proportional to the distance between tokens:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/ALiBi-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/ALiBi-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/ALiBi-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-alibi-mlm/ALiBi.jpeg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>where \(m\) is a head-specific slope chosen to follow geometric sequence \(\frac{1}{2^{0.5}}, \frac{1}{2^1}, \frac{1}{2^{1.5}}, \dots, \frac{1}{2^\frac{n}{2}}\) for a model with \(n\) attention heads. This approach is shown to enable input length extrapolation in the sense that perplexity of the model remains stable as the inference context length exceeds training context length. The paper, however, focuses on autoregressive decoder-only models and relies on model perplexity as the metric, therefore leaves the question open whether ALiBi is applicable to MLMs like BERT <d-cite key="DBLP:conf/naacl/DevlinCLT19"></d-cite> and RoBERTa <d-cite key="liu2019roberta"></d-cite>. To help answer this question, we tested the two following changes to the RoBERTa baseline models, based on the first-party Fairseq toolkit <d-cite key="ott2019fairseq"></d-cite>:</p> <h2 id="attention-with-linear-biases-alibi">Attention with Linear Biases (ALiBi)</h2> <p>Since MLMs are based on encoders that attend to tokens both before and after the given position, considerations must be made regarding how to distinguish them. Press himself <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5" target="_blank" rel="noopener noreferrer">suggested the 3 following options for encoder-attention ALiBi</a>:</p> <ol> <li>Symmetric: Keep attention weight bias proportional to the distance between tokens and rely on the context to distinguish between tokens at +N and -N position.</li> <li>Nonsymmetric, one-sided: Make half of the heads only attend to the tokens before and half of the heads only attend to the tokens after. Weight bias is still proportional to the distance.</li> <li>Nonsymmetric with different slopes: Make the slopes \(m\) different forward and backward, with either learned or fixed values.</li> </ol> <p>With the observation that option 2 spends about half of the attention compute on no-op and option 3 can still result in bias value collision (e.g. \(m_{bwd} = 2 m_{fwd}\) and -1 vs. +2 positions), we implemented both option 1 and what we call “nonsymmetric with offset”: <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5#issuecomment-1213410982" target="_blank" rel="noopener noreferrer">Shift the linear biases ahead by <code class="language-plaintext highlighter-rouge">0.5 * slope</code></a>, i.e. the constant bias (right matrix of the figure above) becomes</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 0 -.5 -1.5 -2.5 -3.5
-1   0  -.5 -1.5 -2.5
-2  -1    0  -.5 -1.5
-3  -2   -1    0  -.5
-4  -3   -2   -1    0
</code></pre></div></div> <p>Unless otherwise noted, ALiBi for the following experiments means this nonsymmetric-with-offset encoder-attention ALiBi.</p> <h2 id="contrastive-language-pretraining-clap-head">Contrastive Language Pretraining (CLAP) Head</h2> <p>The prediction head is one part of the LMs that has received less attention that happens to differ between the ALiBi autoregressive decoder-only models and RoBERTa. Based on the configs and <a href="https://github.com/ofirpress/attention_with_linear_biases#saved-checkpoints" target="_blank" rel="noopener noreferrer">training logs</a>, the ALiBi models use the adaptive word embedding and softmax of Baevski &amp; Auli <d-cite key="DBLP:conf/iclr/BaevskiA19"></d-cite> with weight tying <d-cite key="press-wolf-2017-using"></d-cite>, whereas the RoBERTa prediction head has an additional fully-connected layer and nonlinearity on top of weight-tying. Inspired by CLIP <d-cite key="DBLP:conf/icml/RadfordKHRGASAM21"></d-cite>, we decided to test what we called Contrastive Language Pretraining (CLAP) head below, as the <a href="https://github.com/EIFY/fairseq/blob/8143446dfa88d9f8e246b366bd335f6c9b018db0/fairseq/models/roberta/model.py#L527-L543" target="_blank" rel="noopener noreferrer">simplest possible prediction head with weight tying</a> for the masked tokens plus the thermodynamic beta (inverse temperature):</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ClapHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Head for masked language modeling.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">initial_beta</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">initial_beta</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">masked_tokens</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c1"># Only project the masked tokens while training,
</span>        <span class="c1"># saves both memory and computation
</span>        <span class="k">if</span> <span class="n">masked_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">masked_tokens</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">F</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></code></pre></figure> <p>Compared to the <a href="https://github.com/facebookresearch/fairseq/blob/da8fb630880d529ab47e53381c30ddc8ad235216/fairseq/models/roberta/model.py#L470-L495" target="_blank" rel="noopener noreferrer">baseline RoBERTa prediction head</a></p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">RobertaLMHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Head for masked language modeling.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">activation_fn</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="nf">get_activation_fn</span><span class="p">(</span><span class="n">activation_fn</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">output_dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">masked_tokens</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Only project the masked tokens while training,
</span>        <span class="c1"># saves both memory and computation
</span>        <span class="k">if</span> <span class="n">masked_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">masked_tokens</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dense</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation_fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># project back to size of vocabulary with bias
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">x</span></code></pre></figure> <p>We removed the <code class="language-plaintext highlighter-rouge">embed_dim x embed_dim</code> fully-connected layer, activation function (GELU), layer norm, and the <code class="language-plaintext highlighter-rouge">output_dim</code> trainable bias. Just like CLIP, we added the trainable thermodynamic beta and L2-normalize the token embeddings before feeding them to the transformer and computing the inner products between them and the transformer output as the softmax logits, scaled by beta.</p> <h2 id="experiments">Experiments</h2> <h3 id="wikitext-103">WikiText-103</h3> <p>At first we tested the changes with the <a href="https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/" target="_blank" rel="noopener noreferrer">WikiText-103 dataset</a> <d-cite key="DBLP:conf/iclr/MerityX0S17"></d-cite> with a GeForce RTX 3080 16 GB Laptop GPU, using the validation set MLM perplexity as the metric. We tested the baseline (learned positional encoding + RoBERTa prediction head), learned-clap (learned positional encoding + CLAP head), ALiBi (ALiBi + RoBERTa prediction head), and zero-clap (ALiBi + CLAP head), in addition to baseline but with sinusoidal positional encoding instead of learned positional encoding:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/valid_ppl_cleaned-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/valid_ppl_cleaned-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/valid_ppl_cleaned-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-alibi-mlm/valid_ppl_cleaned.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>where solid lines are what’s considered “canonical” setup and dotted lines are experiments with the following variations in setup. These variations turned out to be irrelevant:</p> <ol> <li>Whether we use attention dropout or not</li> <li>Whether we use <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5" target="_blank" rel="noopener noreferrer">symmetric ALiBi (option 1)</a> or nonsymmetric-with-offset ALiBi above</li> <li> <del>Whether we use zero vector or a separate learnable embedding for the mask embedding</del><d-footnote>The intention was to test using zero vector instead of a separate learnable embedding for the mask embedding, which in combination with ALiBi results in no non-semantic information in the input embeddings. However, a bug prevented this variation from working correctly and the end effect was merely deleting the last two words (madeupword0001 and madeupword0002) from the dictionary instead, which we don't expect to be consequential.</d-footnote> </li> <li>Whether we L2-normalize the embeddings for the CLAP head or not</li> <li>Whether we scale the L2-normalized embeddings by <code class="language-plaintext highlighter-rouge">sqrt(embed_dim)</code> (<code class="language-plaintext highlighter-rouge">no_scale_embedding=False</code>) or not</li> </ol> <p>As we can see, the dotted lines are almost on top of the solid lines. Notably, sinusoidal positional encoding underperforms significantly compared to learned positional encoding.</p> <h3 id="the-pile">The Pile</h3> <p>As the next step, we scaled our experiments to train on the Pile <d-cite key="DBLP:journals/corr/abs-2101-00027"></d-cite> for one epoch. About half of the examples in the Pile has sequence length &gt; 1024, so we set sequence length to 2048. Even so, ~1/7 of the examples have sequence length &gt; 2048 and had to be discarded. In the end, one epoch consists of 133082 updates and <a href="https://github.com/EIFY/fairseq/blob/33fb2c306851f104cc567b7fe865b1e3fd1e6fe7/examples/roberta/config/pretraining/baseline_pile.yaml#L31-L36" target="_blank" rel="noopener noreferrer">we employ cosine learning rate schedule while “overestimating” the number of training steps by 10%</a>, as inspired by the Chinchilla paper <d-cite key="hoffmann2022training"></d-cite>. In addition to the validation MLM perplexity, we also fine-tuned the models on the <a href="https://gluebenchmark.com/" target="_blank" rel="noopener noreferrer">GLUE</a> benchmark <d-cite key="wang-etal-2018-glue"></d-cite>. As in the original RoBERTa paper, we tested both the <code class="language-plaintext highlighter-rouge">roberta.base</code> with 125M parameters and <code class="language-plaintext highlighter-rouge">roberta.large</code> with 355M parameters. These experiments were performed on 8 x A100 40GB SXM4 GPUs, where the <code class="language-plaintext highlighter-rouge">roberta.base</code> experiments took ~3 days and <code class="language-plaintext highlighter-rouge">roberta.large</code> experiments took ~9 days. In the table below, <code class="language-plaintext highlighter-rouge">PPL</code> is the final validation MLM perplexity, <code class="language-plaintext highlighter-rouge">STS-B</code> is the best validation loss, and all the others are the best validation accuracies over 10 epochs of finetuning.</p> <h4 id="robertabase"><code class="language-plaintext highlighter-rouge">roberta.base</code></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>             PPL↓ CoLA MNLI MRPC QNLI QQP  RTE  SST-2 STS-B↓
baseline     2.94 83.6 84.2 90   91.6 91.3 73.6 92.1  0.028
learned-clap 2.86 81.7 84.4 86.3 90.9 91.2 72.6 92.5  0.027
alibi        2.93 69.2 85.1 80.9 92   91.5 63.9 93.1  0.033
zero-clap    2.83 70.5 84.9 75.5 90.6 91.1 54.9 89.7  0.041
</code></pre></div></div> <p>*<em>Baseline but with sinusoidal positional encoding instead of learned positional encoding failed to converge.</em></p> <h4 id="robertalarge"><code class="language-plaintext highlighter-rouge">roberta.large</code></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>             PPL↓ CoLA MNLI MRPC QNLI QQP  RTE  SST-2 STS-B↓
baseline*    2.55 83.7 86.8 84.3 92.5 91.8 79.8 93.3  0.027
learned-clap 2.5  84.1 86.3 89.7 92.8 91.7 79.8 93.7  0.023
alibi        2.65 69.1 86.5 68.4 92.4 91.7 52.7 93.6  0.123
zero-clap    2.54 69.1 86.7 81.9 92.2 91.6 52.7 93.1  0.031
</code></pre></div></div> <p>*<em>Loss spiked somewhere between 24000-24500 updates and the model failed to recover. Loosely following the practice of <code class="language-plaintext highlighter-rouge">5.1 Training Instability</code> in the PaLM paper <d-cite key="chowdhery2022palm"></d-cite>, we solved the issue by restarting the training from the 20000 updates checkpoint with the PyTorch random seed changed from <code class="language-plaintext highlighter-rouge">1</code> to <code class="language-plaintext highlighter-rouge">2</code>.</em></p> <p>We found that ALiBi no longer helps lowering the validation MLM perplexity. Furthermore, ALiBi turned out to be harmful for several specific GLUE tasks (<code class="language-plaintext highlighter-rouge">CoLA</code>, <code class="language-plaintext highlighter-rouge">MRPC</code>, and <code class="language-plaintext highlighter-rouge">RTE</code>). CLAP head on its own, however, seems to be competitive and in fact outperforms the baseline with <code class="language-plaintext highlighter-rouge">roberta.large</code>.</p> <h2 id="conclusions">Conclusions</h2> <p>This seems to be another case where models with lower perplexity do not necessarily yield higher accuracies for downstream tasks and architectural changes beneficial for models at smaller scales do not imply the same for models at larger scales <d-cite key="tay2022scaling"></d-cite>. CLAP head, however, is simpler than the standard prediction head for MLMs, requires minimal changes, and may be worth trying especially at larger scales.</p> <p>In the broader context, MosaicBERT <d-cite key="portes2023mosaicbert"></d-cite> and LittleBird <d-cite key="lee-etal-2022-littlebird"></d-cite> are most similar to our experiments. In the MosaicBERT paper, Portes et al. also evaluate BERT-style MLMs with symmetric (option 1) encoder-attention ALiBi on the GLUE benchmark and find performance exceeding the BERT baseline within limited training budget. However, these MosaicBERT models were trained with much shorter (128) sequence length and so may have avoided the sequence length regime in which perplexity and performance of certain downstream tasks start to deteriorate <d-footnote>The same can be said about <d-cite key="haviv-etal-2022-transformer"></d-cite>, which also reports in Table 4 the MLM perplexity of RoBERTa large models trained on an excerpt of the Pile with various positional encodings including <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5#issuecomment-1207346198" target="_blank" rel="noopener noreferrer">symmetric (option 1)</a> encoder-attention ALiBi with 128 sequence length.</d-footnote>. The LittleBird architecture is designed for question answering and built with BiALiBi (Bidirectional ALiBi), a variation of option 3 (nonsymmetric with different slopes) where the model not only learned the forward and backward slopes \(m_{fwd}\) and \(m_{bwd}\), but also a special bias value for the attention weight of the global <code class="language-plaintext highlighter-rouge">[CLS]</code> token. Lee et al. evaluate LittleBird models on a collection of QA Benchmarks for both English and Korean and report favorable performance, but leave the question open whether they work well for other NLP tasks. Notably, we also found our ALiBi models capable of matching the baseline performance of the question answering task <code class="language-plaintext highlighter-rouge">QNLI</code>, so the reported performance is compatible with our experiments even without attributing to the other differences in architecture or pretraining task.</p> <p>Finally, what can we say about the original decoder-attention ALiBi and positional encodings in general? The original decoder-attention ALiBi has been shown to help not only perplexity, but also performance on evaluation suites consist of a diverse set of tasks like the EleutherAI Language Model Evaluation Harness <d-cite key="scao2022what"></d-cite>. This discrepancy may be explained by the causal mask, which has been proven to be sufficient for encoding positional information in theory <d-cite key="DBLP:journals/corr/abs-2305-19466"></d-cite><d-footnote>One caveat is that Proof C.1 of <d-cite key="DBLP:journals/corr/abs-2305-19466"></d-cite> for absolute positional encoding depends on distinguishing values of unit fractions 1/t, which eventually fails due to precision limit. For example, 1/1464 can't be distinguished from 1/1465 in float16, well within the context length of interest.</d-footnote>, if not quite matching the performance of models with additional positional encodings in practice <d-cite key="scao2022what"></d-cite><d-cite key="haviv-etal-2022-transformer"></d-cite>. Perhaps we can conclude that</p> <ol> <li>Decoder-attention positional encodings really should be considered causal mask + additional encodings and how they complement each other should be taken into account.</li> <li>Longer context length and certain downstream tasks are more challenging for positional encodings. One worthwhile direction may be to rank their difficulties systematically and iterate on the more challenging circumstances first for future positional encoding designs.</li> </ol> <h2 id="model-checkpoints">Model checkpoints</h2> <p>Final checkpoints for models trained on the Pile:</p> <h3 id="robertabase-1"><code class="language-plaintext highlighter-rouge">roberta.base</code></h3> <p><a href="https://drive.google.com/file/d/1r9VwJCU3AeuivNULRuY3Taq_3AEBg-v5/view?usp=share_link" target="_blank" rel="noopener noreferrer">baseline</a> <a href="https://drive.google.com/file/d/1KmO3FEaawz0tHW-s581NmrkL-OZklLYk/view?usp=share_link" target="_blank" rel="noopener noreferrer">learned-clap</a> <a href="https://drive.google.com/file/d/1s4Tcjnbawq1W6LBcknysj6NdpMfJdek6/view?usp=share_link" target="_blank" rel="noopener noreferrer">alibi</a> <a href="https://drive.google.com/file/d/1PwE_MASg4FinuKq6DX29A8c2lPP2B6nb/view?usp=share_link" target="_blank" rel="noopener noreferrer">zero-clap</a></p> <h3 id="robertalarge-1"><code class="language-plaintext highlighter-rouge">roberta.large</code></h3> <p><a href="https://drive.google.com/file/d/1XSStju8S9y1BCHpXqZ_fZcueH3A0yW2c/view?usp=share_link" target="_blank" rel="noopener noreferrer">baseline</a> <a href="https://drive.google.com/file/d/1UyFxC3XoQ5eAhhXaAUQznLbBLa0J_45U/view?usp=share_link" target="_blank" rel="noopener noreferrer">learned-clap</a> <a href="https://drive.google.com/file/d/1D22xJxJTI4gPAD4gHfKaN1ytjQTy2u_y/view?usp=share_link" target="_blank" rel="noopener noreferrer">alibi</a> <a href="https://drive.google.com/file/d/1ktiRIVqz46DbV261_WxA9RELR971_2iu/view?usp=share_link" target="_blank" rel="noopener noreferrer">zero-clap</a></p> <p>To load them, install <a href="https://github.com/EIFY/fairseq" target="_blank" rel="noopener noreferrer">EIFY/fairseq</a> following <a href="https://github.com/facebookresearch/fairseq/blob/b8ac3fa6cc95f9dc97085232d4faf125e5bcd2e7/README.md#requirements-and-installation" target="_blank" rel="noopener noreferrer">the original instructions</a> and download the GPT-2 fairseq dictionary:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt
</code></pre></div></div> <p>Then all of the checkpoints above except the <code class="language-plaintext highlighter-rouge">zero-clap</code> ones can load as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python
Python 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from fairseq.models.roberta import RobertaModel
&gt;&gt;&gt; roberta = RobertaModel.from_pretrained('/checkpoint-dir', 'learned-clap-large.pt', '/dict-dir')
(...)
&gt;&gt;&gt; roberta.fill_mask('The capital of China is &lt;mask&gt;.', topk=3)
[('The capital of China is Beijing.', 0.7009016871452332, ' Beijing'), ('The capital of China is Shanghai.', 0.23566904664039612, ' Shanghai'), ('The capital of China is Moscow.', 0.010170688852667809, ' Moscow')]
&gt;&gt;&gt;
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">zero-clap</code> ones were trained without the last two <code class="language-plaintext highlighter-rouge">madeupword</code>’s<d-footnote>This is due to the same bug that affected the WikiText-103 variation above and its only visible effect.</d-footnote>, so you need to delete them from <code class="language-plaintext highlighter-rouge">dict.txt</code> before loading, i.e.:</p> <pre>
(...)
50009 0
50256 0
madeupword0000 0
<strike>madeupword0001 0
madeupword0002 0</strike>
</pre> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python
Python 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from fairseq.models.roberta import RobertaModel
&gt;&gt;&gt; roberta = RobertaModel.from_pretrained('/checkpoint-dir', 'zero-clap-large.pt', '/dict-dir')
(...)
&gt;&gt;&gt; roberta.fill_mask('The capital of China is &lt;mask&gt;.', topk=3)
[('The capital of China is Beijing.', 0.7051425576210022, ' Beijing'), ('The capital of China is Shanghai.', 0.21408841013908386, ' Shanghai'), ('The capital of China is Taiwan.', 0.007823833264410496, ' Taiwan')]
&gt;&gt;&gt;
</code></pre></div></div> <p>The rest of the original <a href="https://github.com/facebookresearch/fairseq/blob/b8ac3fa6cc95f9dc97085232d4faf125e5bcd2e7/examples/roberta/README.md#example-usage" target="_blank" rel="noopener noreferrer">example usage</a> should also just work. While these checkpoints have only been tested with this fork, the <code class="language-plaintext highlighter-rouge">baseline</code> ones should also work with the <a href="https://github.com/facebookresearch/fairseq" target="_blank" rel="noopener noreferrer">original fairseq repo</a> with minimum changes to the state dict:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; path = '/checkpoint-dir/baseline-large.pt'
&gt;&gt;&gt; with open(path, 'rb') as f:
...   state = torch.load(f, map_location=torch.device("cpu"))
...
&gt;&gt;&gt;
&gt;&gt;&gt; del state['cfg']['task']['omit_mask']
(...)
&gt;&gt;&gt; torch.save(state, '/checkpoint-dir/compatible.pt')
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2024/assets/bibliography/2024-05-07-alibi-mlm.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2024" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>