<!DOCTYPE html> <html> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},i=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),a=i[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"May 7, 2024"),l="The N Implementation Details of RLHF with PPO",r="Reinforcement Learning from Human Feedback (RLHF) is pivotal in the modern application of language modeling, as exemplified by ChatGPT. This blog post delves into an in-depth exploration of RLHF, attempting to reproduce the results from OpenAI's inaugural RLHF paper, published in 2019. Our detailed examination provides valuable insights into the implementation details of RLHF, which often go unnoticed.";{let e=i.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(a+"2024"+l.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${l}},\n  abstract = {${r}},\n  booktitle = {ICLR Blogposts 2024},\n  year = {2024},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=i.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${l}", ICLR Blogposts, 2024.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The N Implementation Details of RLHF with PPO | ICLR Blogposts 2024</title> <meta name="author" content="ICLR Blog"/> <meta name="description" content="Reinforcement Learning from Human Feedback (RLHF) is pivotal in the modern application of language modeling, as exemplified by ChatGPT. This blog post delves into an in-depth exploration of RLHF, attempting to reproduce the results from OpenAI's inaugural RLHF paper, published in 2019. Our detailed examination provides valuable insights into the implementation details of RLHF, which often go unnoticed."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/2024/assets/img/iclr_favicon.ico"/> <link rel="stylesheet" href="/2024/assets/css/main.css"> <link rel="canonical" href="https://iclr-blogposts.github.io/2024/blog/the-n-implementation-details-of-rlhf-with-ppo/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/2024/assets/js/theme.js"></script> <script src="/2024/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2024/assets/js/distillpub/template.v2.js"></script> <script src="/2024/assets/js/distillpub/transforms.v2.js"></script> <script src="/2024/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "The N Implementation Details of RLHF with PPO",
      "description": "Reinforcement Learning from Human Feedback (RLHF) is pivotal in the modern application of language modeling, as exemplified by ChatGPT. This blog post delves into an in-depth exploration of RLHF, attempting to reproduce the results from OpenAI's inaugural RLHF paper, published in 2019. Our detailed examination provides valuable insights into the implementation details of RLHF, which often go unnoticed.",
      "published": "May 7, 2024",
      "authors": [
        {
          "author": "Shengyi Costa Huang",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Hugging Face",
              "url": ""
            }
          ]
        },
        {
          "author": "Tianlin Liu",
          "authorURL": "",
          "affiliations": [
            {
              "name": "University of Basel",
              "url": ""
            }
          ]
        },
        {
          "author": "Leandro von Werra",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Hugging Face",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2024/">ICLR Blogposts 2024</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2024/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/2024/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">other iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/"><strong>2024</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" target="_blank" rel="noopener noreferrer">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>The N Implementation Details of RLHF with PPO</h1> <p>Reinforcement Learning from Human Feedback (RLHF) is pivotal in the modern application of language modeling, as exemplified by ChatGPT. This blog post delves into an in-depth exploration of RLHF, attempting to reproduce the results from OpenAI's inaugural RLHF paper, published in 2019. Our detailed examination provides valuable insights into the implementation details of RLHF, which often go unnoticed.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#matching-learning-curves">Matching Learning Curves</a></div> <div><a href="#general-implementation-details">General Implementation Details</a></div> <div><a href="#reward-model-implementation-details">Reward Model Implementation Details</a></div> <div><a href="#policy-training-implementation-details">Policy Training Implementation Details</a></div> <div><a href="#pytorch-adam-optimizer-numerical-issues-w-r-t-rlhf">PyTorch Adam optimizer numerical issues w.r.t RLHF</a></div> <div><a href="#limitations">Limitations</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <p><strong>Reinforcement Learning from Human Feedback</strong> (RLHF) has been an impactful technique for training modern language models such as ChatGPT. In our quest to research more on RLHF, this blog post closely examines OpenAI’s inaugural RLHF paper <d-cite key="Ziegler2019fine"></d-cite> published in 2019 together with its open-source codebase at available at <a href="https://github.com/openai/lm-human-preferences" target="_blank" rel="noopener noreferrer"><em>openai/lm-human-preferences</em></a>. Despite being based on TensorFlow-1, the code base released by OpenAI is very well-evaluated and benchmarked, making it a good place to study RLHF implementation engineering details.</p> <p>We aim to:</p> <ol> <li>reproduce OpenAI’s results in stylistic tasks and match the learning curves of <a href="https://github.com/openai/lm-human-preferences" target="_blank" rel="noopener noreferrer"><em>openai/lm-human-preferences</em></a>, using the modern PyTorch <d-cite key="Paszke2019pytorch"></d-cite> and JAX <d-cite key="Jax2018github"></d-cite> frameworks in conjunction with HuggingFace Transformers <d-cite key="Wolf2020transformers"></d-cite> that are predominantly used by the open-source community nowadays;</li> <li>present a checklist of implementation details, similar to the spirit of <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/" target="_blank" rel="noopener noreferrer"><em>The 37 Implementation Details of Proximal Policy Optimization</em></a> <d-cite key="Shengyi2022the37implementation"></d-cite> and <a href="https://andyljones.com/posts/rl-debugging.html" target="_blank" rel="noopener noreferrer"><em>Debugging RL, Without the Agonizing Pain</em></a>;</li> <li>provide a simple-to-read and minimal reference implementation of RLHF;</li> </ol> <p>This work is just for educational / learning purposes. For advanced users requiring more features, such as running larger models with parameter-efficient fine-tuning, <a href="https://github.com/huggingface/trl" target="_blank" rel="noopener noreferrer"><em>huggingface/trl</em></a> would be a great choice.</p> <ul> <li>In <a href="#matching-learning-curves">Matching Learning Curves</a>, we show our main contribution: creating a codebase that can reproduce OpenAI’s results in the stylistic tasks and matching learning curves very closely with <a href="https://github.com/openai/lm-human-preferences" target="_blank" rel="noopener noreferrer"><em>openai/lm-human-preferences</em></a>.</li> <li>We then take a technical deep dive into the implementation details that are relevant to reproducing OpenAI’s work. In <a href="#general-implementation-details">General Implementation Details</a>, we talk about basic details, such as how rewards/values are generated and how responses are generated. In <a href="#reward-model-implementation-details">Reward Model Implementation Details</a>, we talk about details such as reward normalization. In <a href="#policy-training-implementation-details">Policy Training Implementation Details</a>, we discuss details such as rejection sampling and reward “whitening”. <ul> <li>In <a href="#pytorch-adam-optimizer-numerical-issues-wrt-rlhf"><strong>PyTorch Adam optimizer numerical issues w.r.t RLHF</strong></a>, we highlight a very interesting implementation difference in Adam <d-cite key="Kingma2015adam"></d-cite> between TensorFlow and PyTorch, which causes an aggressive update in the model training.</li> </ul> </li> <li>Next, we examine the effect of training different base models (e.g., gpt2-xl, falcon-1b,) given that the reward labels are produced with <code class="language-plaintext highlighter-rouge">gpt2-large</code>.</li> <li>Finally, we conclude our work with limitations and discussions.</li> </ul> <p>Here are the important links:</p> <ul> <li>💾 <a href="https://github.com/vwxyzjn/lm-human-preference-details" target="_blank" rel="noopener noreferrer">Our reproduction codebase</a> </li> <li>🤗 <a href="https://huggingface.co/spaces/lm-human-preference-details/rlhf-demo" target="_blank" rel="noopener noreferrer">Demo of RLHF model comparison</a> </li> <li>🐝 <a href="https://wandb.ai/openrlbenchmark/lm_human_preference_details" target="_blank" rel="noopener noreferrer">All w&amp;b training logs</a> </li> </ul> <h1 id="matching-learning-curves">Matching Learning Curves</h1> <p>Our main contribution is to reproduce OpenAI’s results in stylistic tasks, such as sentiment and descriptiveness. As shown in the figure below, our codebase (orange curves) can produce nearly identical learning curves as OpenAI’s codebase (blue curves).</p> <div class="l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/curve-matching-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/curve-matching-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/curve-matching-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/curve-matching.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="a-note-on-running-openailm-human-preferences">A note on running openai/lm-human-preferences</h2> <p>To make a direct comparison, we ran the original RLHF code at <a href="https://github.com/openai/lm-human-preferences" target="_blank" rel="noopener noreferrer"><em>openai/lm-human-preferences</em></a>, which will offer valuable metrics to help validate and diagnose our reproduction. We were able to set the original TensorFlow 1.x code up, but it requires a hyper-specific setup:</p> <ul> <li>OpenAI’s dataset was partially corrupted/lost (so we replaced them with similar HF datasets, which may or may not cause a performance difference) <ul> <li>Specifically, its book dataset was lost during OpenAI’s GCP - Azure migration (<a href="https://github.com/openai/lm-human-preferences/issues/17#issuecomment-1044051496" target="_blank" rel="noopener noreferrer">https://github.com/openai/lm-human-preferences/issues/17#issuecomment-1044051496</a>). We replaced the book dataset with Hugging Face’s <code class="language-plaintext highlighter-rouge">bookcorpus</code> dataset, which is, in principle, what OpenAI used.</li> </ul> </li> <li>It can’t run on 1 V100 because it doesn’t implement gradient accumulation. Instead, it uses a large batch size and splits the batch across 8 GPUs, and will OOM on just 1 GPU.</li> <li>It can’t run on 8x A100 because it uses TensorFlow 1.x, which is incompatible with Cuda 8+</li> <li>It can’t run on 8x V100 (16GB) because it will OOM</li> <li>It can only run on 8x V100 (32GB), which is only offered by AWS as the <code class="language-plaintext highlighter-rouge">p3dn.24xlarge</code> instance.</li> </ul> <h1 id="general-implementation-details">General Implementation Details</h1> <p>We now take a technical deep dive into the implementation details that are relevant to reproducing OpenAI’s work. In this section, we talk about basic details, such as how rewards/values are generated and how responses are generated. Here are these details in no particular order:</p> <ol> <li> <strong>The reward model and policy’s value head take input as the concatenation of <code class="language-plaintext highlighter-rouge">query</code> and <code class="language-plaintext highlighter-rouge">response</code></strong> <ol> <li>The reward model and policy’s value head do <em>not</em> only look at the response. Instead, it concatenates the <code class="language-plaintext highlighter-rouge">query</code> and <code class="language-plaintext highlighter-rouge">response</code> together as <code class="language-plaintext highlighter-rouge">query_response</code> (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/rewards.py#L105-L107" target="_blank" rel="noopener noreferrer">lm_human_preferences/rewards.py#L105-L107</a>).</li> <li>So, for example, if <code class="language-plaintext highlighter-rouge">query = "he was quiet for a minute, his eyes unreadable"</code>., and the <code class="language-plaintext highlighter-rouge">response = "He looked at his left hand, which held the arm that held his arm out in front of him."</code>, then the reward model and policy’s value do a forward pass on <code class="language-plaintext highlighter-rouge">query_response = "he was quiet for a minute, his eyes unreadable. He looked at his left hand, which held the arm that held his arm out in front of him."</code> and produced rewards and values of shape <code class="language-plaintext highlighter-rouge">(B, T, 1)</code>, where <code class="language-plaintext highlighter-rouge">B</code> is the batch size, <code class="language-plaintext highlighter-rouge">T</code> is the sequence length, and <code class="language-plaintext highlighter-rouge">1</code> is the reward head dimension of 1 (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/rewards.py#L105-L107" target="_blank" rel="noopener noreferrer">lm_human_preferences/rewards.py#L105-L107</a>, <a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/policy.py#L111" target="_blank" rel="noopener noreferrer">lm_human_preferences/policy.py#L111</a>).</li> <li>The <code class="language-plaintext highlighter-rouge">T</code> means that each token has a reward associated with it and its previous context. For example, the <code class="language-plaintext highlighter-rouge">eyes</code> token would have a reward corresponding to <code class="language-plaintext highlighter-rouge">he was quiet for a minute, his eyes</code>.</li> </ol> </li> <li> <strong>Pad with a special padding token and truncate inputs.</strong> <ol> <li>OpenAI sets a fixed input length for query <code class="language-plaintext highlighter-rouge">query_length</code>; it <strong>pads</strong> sequences that are too short with <code class="language-plaintext highlighter-rouge">pad_token</code> (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L66-L67" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/datasets.py#L66-L67</a>) and <strong>truncates</strong> sequences that are too long (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L57" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/datasets.py#L57</a>). See <a href="https://huggingface.co/docs/transformers/pad_truncation" target="_blank" rel="noopener noreferrer">here</a> for a general introduction to the concept). When padding the inputs, OpenAI uses a token beyond the vocabulary (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/encodings.py#L56" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/encodings.py#L56</a>). <ol> <li> <strong>Note on HF’s transformers — padding token.</strong> According to (<a href="https://github.com/huggingface/transformers/issues/2630#issuecomment-578159876" target="_blank" rel="noopener noreferrer">transformers#2630#issuecomment-578159876</a>), padding tokens were not used during the pre-training of GPT and GPT-2; therefore transformer’s gpt2 models have no official padding token associated with its tokenizer. A common practice is to set <code class="language-plaintext highlighter-rouge">tokenizer.pad_token = tokenizer.eos_token</code>, but in this work, we shall distinguish these two special tokens to match OpenAI’s original setting, so we will use <code class="language-plaintext highlighter-rouge">tokenizer.add_special_tokens({"pad_token": "[PAD]"})</code>.</li> </ol> <p>Note that having no padding token is a default setting for decoder models, since they train with “packing” during pretraining, which means that many sequences are concatenated and separated by the EOS token and chunks of this sequence that always have the max length are fed to the model during pretraining.</p> </li> <li>When putting everything together, here is an example</li> </ol> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="kn">import</span> <span class="n">transformers</span>
 <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="sh">"</span><span class="s">right</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">tokenizer</span><span class="p">.</span><span class="nf">add_special_tokens</span><span class="p">({</span><span class="sh">"</span><span class="s">pad_token</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">[PAD]</span><span class="sh">"</span><span class="p">})</span>
 <span class="n">query_length</span> <span class="o">=</span> <span class="mi">5</span>
 <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
     <span class="sh">"</span><span class="s">usually, he would</span><span class="sh">"</span><span class="p">,</span>
     <span class="sh">"</span><span class="s">she thought about it</span><span class="sh">"</span><span class="p">,</span>
 <span class="p">]</span>    
 <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
 <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
     <span class="n">tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)[:</span><span class="n">query_length</span><span class="p">])</span>
    
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">tokens</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
 <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span>
     <span class="p">{</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">:</span> <span class="n">tokens</span><span class="p">},</span>
     <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">max_length</span><span class="sh">"</span><span class="p">,</span>
     <span class="n">max_length</span><span class="o">=</span><span class="n">query_length</span><span class="p">,</span>
     <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
     <span class="n">return_attention_mask</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
 <span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">inputs</span><span class="sh">"</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    
 <span class="sh">"""</span><span class="s">prints are
 tokens [[23073, 11, 339, 561], [7091, 1807, 546, 340]]
 inputs {</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="s">: tensor([[23073,    11,   339,   561, 50257],
         [ 7091,  1807,   546,   340, 50257]]), </span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="s">: tensor([[1, 1, 1, 1, 0],
         [1, 1, 1, 1, 0]])}
 </span><span class="sh">"""</span>
</code></pre></div> </div> </li> <li> <strong>Adjust position indices correspondingly for padding tokens</strong> <ol> <li>When calculating the logits, OpenAI’s code works by masking out padding tokens properly. This is achieved by finding out the token indices corresponding to the padding tokens (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L296-L297" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/model.py#L296-L297</a>), followed by adjusting their position indices correspondingly (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L320" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/model.py#L320</a>).</li> <li> <p>For example, if the <code class="language-plaintext highlighter-rouge">query=[23073, 50259, 50259]</code> and <code class="language-plaintext highlighter-rouge">response=[11, 339, 561]</code>, where (<code class="language-plaintext highlighter-rouge">50259</code> is OpenAI’s padding token), it then creates position indices as <code class="language-plaintext highlighter-rouge">[[0 1 1 1 2 3]]</code> and logits as follows. Note how the logits corresponding to the padding tokens remain the same as before! This is the effect we should be aiming for in our reproduction.</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="n">all_logits</span> <span class="p">[[[</span> <span class="o">-</span><span class="mf">35.28693</span>   <span class="o">-</span><span class="mf">34.2875</span>    <span class="o">-</span><span class="mf">38.16074</span>  <span class="p">...</span>  <span class="o">-</span><span class="mf">41.595802</span>  <span class="o">-</span><span class="mf">41.082108</span>
     <span class="o">-</span><span class="mf">35.36577</span> <span class="p">]</span>
   <span class="p">[</span> <span class="o">-</span><span class="mf">35.28693</span>   <span class="o">-</span><span class="mf">34.2875</span>    <span class="o">-</span><span class="mf">38.16074</span>  <span class="p">...</span>  <span class="o">-</span><span class="mf">41.595802</span>  <span class="o">-</span><span class="mf">41.082108</span>
     <span class="o">-</span><span class="mf">35.36577</span> <span class="p">]</span>
   <span class="p">[</span> <span class="o">-</span><span class="mf">35.28693</span>   <span class="o">-</span><span class="mf">34.2875</span>    <span class="o">-</span><span class="mf">38.16074</span>  <span class="p">...</span>  <span class="o">-</span><span class="mf">41.595802</span>  <span class="o">-</span><span class="mf">41.082108</span>
     <span class="o">-</span><span class="mf">35.36577</span> <span class="p">]</span>
   <span class="p">[</span><span class="o">-</span><span class="mf">111.303955</span> <span class="o">-</span><span class="mf">110.94471</span>  <span class="o">-</span><span class="mf">112.90624</span>  <span class="p">...</span> <span class="o">-</span><span class="mf">113.13064</span>  <span class="o">-</span><span class="mf">113.7788</span>
    <span class="o">-</span><span class="mf">109.17345</span> <span class="p">]</span>
   <span class="p">[</span><span class="o">-</span><span class="mf">111.51512</span>  <span class="o">-</span><span class="mf">109.61077</span>  <span class="o">-</span><span class="mf">114.90231</span>  <span class="p">...</span> <span class="o">-</span><span class="mf">118.43514</span>  <span class="o">-</span><span class="mf">111.56671</span>
    <span class="o">-</span><span class="mf">112.12478</span> <span class="p">]</span>
   <span class="p">[</span><span class="o">-</span><span class="mf">122.69775</span>  <span class="o">-</span><span class="mf">121.84468</span>  <span class="o">-</span><span class="mf">128.27417</span>  <span class="p">...</span> <span class="o">-</span><span class="mf">132.28055</span>  <span class="o">-</span><span class="mf">130.39604</span>
    <span class="o">-</span><span class="mf">125.707756</span><span class="p">]]]</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">50257</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>Note on HF’s transformers — <code class="language-plaintext highlighter-rouge">position_ids</code> and <code class="language-plaintext highlighter-rouge">padding_side</code>.</strong> We can replicate the exact logits using Hugging Face’s transformer with 1) left padding and 2) pass in the appropriate <code class="language-plaintext highlighter-rouge">position_ids</code>:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="kn">import</span> <span class="n">torch</span>
 <span class="kn">import</span> <span class="n">transformers</span>
 <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="sh">"</span><span class="s">right</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">tokenizer</span><span class="p">.</span><span class="nf">add_special_tokens</span><span class="p">({</span><span class="sh">"</span><span class="s">pad_token</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">[PAD]</span><span class="sh">"</span><span class="p">})</span>
 <span class="n">pad_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span>
 <span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span>
     <span class="p">[</span><span class="n">pad_id</span><span class="p">,</span> <span class="n">pad_id</span><span class="p">,</span> <span class="mi">23073</span><span class="p">],</span>
 <span class="p">])</span>
 <span class="n">response</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span>
     <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">339</span><span class="p">,</span> <span class="mi">561</span><span class="p">],</span>
 <span class="p">])</span>
 <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span>
        
 <span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
 <span class="n">response</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">response</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
 <span class="n">context_length</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
 <span class="n">query_response</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">query</span><span class="p">,</span> <span class="n">response</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
 <span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span><span class="p">)</span>
 <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">query_responses</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
     <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">query_responses</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span>
     <span class="n">position_ids</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">long</span><span class="p">()</span>  <span class="c1"># exclusive cumsum
</span>     <span class="n">input_ids</span> <span class="o">=</span> <span class="n">query_responses</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
     <span class="n">input_ids</span><span class="p">[</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
     <span class="k">return</span> <span class="nf">policy</span><span class="p">(</span>
         <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
         <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
         <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
         <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
         <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
     <span class="p">)</span>
 <span class="n">output</span> <span class="o">=</span> <span class="nf">forward</span><span class="p">(</span><span class="n">pretrained_model</span><span class="p">,</span> <span class="n">query_response</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
 <span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">logits</span>
 <span class="n">logits</span> <span class="o">/=</span> <span class="n">temperature</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        
 <span class="sh">"""</span><span class="s">
 tensor([[[ -26.9395,  -26.4709,  -30.0456,  ...,  -33.2208,  -33.2884,
            -27.4360],
          [ -27.1677,  -26.7330,  -30.2386,  ...,  -33.6813,  -33.6931,
            -27.5928],
          [ -35.2869,  -34.2875,  -38.1608,  ...,  -41.5958,  -41.0821,
            -35.3658],
          [-111.3040, -110.9447, -112.9062,  ..., -113.1306, -113.7788,
           -109.1734],
          [-111.5152, -109.6108, -114.9024,  ..., -118.4352, -111.5668,
           -112.1248],
          [-122.6978, -121.8447, -128.2742,  ..., -132.2805, -130.3961,
           -125.7078]]], grad_fn=&lt;DivBackward0&gt;)
 </span><span class="sh">"""</span>
</code></pre></div> </div> </li> <li> <strong>Note on HF’s transformers — <code class="language-plaintext highlighter-rouge">position_ids</code> during <code class="language-plaintext highlighter-rouge">generate</code>:</strong> during generate we should not pass in <code class="language-plaintext highlighter-rouge">position_ids</code> because the <code class="language-plaintext highlighter-rouge">position_ids</code> are already adjusted in <code class="language-plaintext highlighter-rouge">transformers</code> (see <a href="https://github.com/huggingface/transformers/pull/7552" target="_blank" rel="noopener noreferrer">huggingface/transformers#/7552</a>).</li> </ol> <p>Usually, we almost never pass <code class="language-plaintext highlighter-rouge">position_ids</code> in transformers. All the masking and shifting logic are already implemented e.g. in the <code class="language-plaintext highlighter-rouge">generate</code> function (need permanent code link).</p> </li> <li> <strong>Response generation samples a fixed-length response without padding.</strong> <ol> <li>During response generation, OpenAI uses <code class="language-plaintext highlighter-rouge">top_k=0, top_p=1.0</code> and just do categorical samples across the vocabulary (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/sample.py#L43" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/sample.py#L43</a>) and the code would keep sampling until a fixed-length response is generated (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/policy.py#L103" target="_blank" rel="noopener noreferrer">lm_human_preferences/policy.py#L103</a>). Notably, even if it encounters EOS (end-of-sequence) tokens, it will keep sampling.</li> <li> <p><strong>Note on HF’s transformers — sampling could stop at <code class="language-plaintext highlighter-rouge">eos_token</code>:</strong> in <code class="language-plaintext highlighter-rouge">transformers</code>, the generation could stop at <code class="language-plaintext highlighter-rouge">eos_token</code> (<a href="https://github.com/huggingface/transformers/blob/67b85f24def79962ce075353c2627f78e0e53e9f/src/transformers/generation/utils.py#L2248-L2256" target="_blank" rel="noopener noreferrer">src/transformers/generation/utils.py#L2248-L2256</a>), which is not the same as OpenAI’s setting. To align the setting, we need to do set <code class="language-plaintext highlighter-rouge">pretrained_model.generation_config.eos_token_id = None, pretrained_model.generation_config.pad_token_id = None</code>. Note that <code class="language-plaintext highlighter-rouge">transformers.GenerationConfig(eos_token_id=None, pad_token_id=None, ...)</code> does not work because <code class="language-plaintext highlighter-rouge">pretrained_model.generation_config</code> would override and set a <code class="language-plaintext highlighter-rouge">eos_token</code>.</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="kn">import</span> <span class="n">torch</span>
 <span class="kn">import</span> <span class="n">transformers</span>
 <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="sh">"</span><span class="s">right</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">tokenizer</span><span class="p">.</span><span class="nf">add_special_tokens</span><span class="p">({</span><span class="sh">"</span><span class="s">pad_token</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">[PAD]</span><span class="sh">"</span><span class="p">})</span>
 <span class="n">pad_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span>
 <span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span>
     <span class="p">[</span><span class="n">pad_id</span><span class="p">,</span> <span class="n">pad_id</span><span class="p">,</span> <span class="mi">23073</span><span class="p">],</span>
 <span class="p">])</span>
 <span class="n">response</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span>
     <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">339</span><span class="p">,</span> <span class="mi">561</span><span class="p">],</span>
 <span class="p">])</span>
 <span class="n">response_length</span> <span class="o">=</span> <span class="mi">4</span>
 <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.7</span>
 <span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt2</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">pretrained_model</span><span class="p">.</span><span class="n">generation_config</span><span class="p">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># disable `pad_token_id` and `eos_token_id` because we just want to
</span> <span class="n">pretrained_model</span><span class="p">.</span><span class="n">generation_config</span><span class="p">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># generate tokens without truncation / padding
</span> <span class="n">generation_config</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="nc">GenerationConfig</span><span class="p">(</span>
     <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">response_length</span><span class="p">,</span>
     <span class="n">min_new_tokens</span><span class="o">=</span><span class="n">response_length</span><span class="p">,</span>
     <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
     <span class="n">top_k</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
     <span class="n">top_p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
     <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
 <span class="p">)</span>
 <span class="n">context_length</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
 <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">query</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span>
 <span class="n">input_ids</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
 <span class="n">input_ids</span><span class="p">[</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># set padding tokens to 0
</span> <span class="n">output</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
     <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
     <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
     <span class="c1"># position_ids=attention_mask.cumsum(1) - attention_mask.long(), # generation collapsed if this was turned on.
</span>     <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
     <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
 <span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">sequences</span><span class="p">)</span>
        
 <span class="sh">"""</span><span class="s">
 tensor([[    0,     0, 23073, 16851,    11,   475,   991]])
 </span><span class="sh">"""</span>
</code></pre></div> </div> </li> <li>Note that in a more recent codebase https://github.com/openai/summarize-from-feedback, OpenAI does stop sampling when encountering EOS token (<a href="https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/utils/experiment_helpers.py#L19" target="_blank" rel="noopener noreferrer">summarize_from_feedback/utils/experiment_helpers.py#L19</a>). However in this work we aim to do a 1:1 replication, so we align the setting that could keep sampling even eos_token is encountered</li> </ol> </li> <li> <strong>Learning rate annealing for reward model and policy training.</strong> <ol> <li>As Ziegler et al. (2019) <d-cite key="Ziegler2019fine"></d-cite> suggested, the reward model is trained for a single epoch to avoid overfitting the limited amount of human annotation data (e.g., the <code class="language-plaintext highlighter-rouge">descriptiveness</code> task only had about 5000 labels). During this single epoch, the learning rate is annealed to zero (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L249" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_reward.py#L249</a>).</li> <li>Similar to reward model training, the policy’s learning rate is annealed to zero (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L172-L173" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L172-L173</a>).</li> </ol> </li> <li> <strong>Use different seeds for different processes</strong> <ol> <li>When spawning 8 GPU processes to do data parallelism, OpenAI sets a different random seed per process (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/utils/core.py#L108-L111" target="_blank" rel="noopener noreferrer">lm_human_preferences/utils/core.py#L108-L111</a>). Implementation-wise, this is done via <code class="language-plaintext highlighter-rouge">local_seed = args.seed + process_rank * 100003</code>. The seed is going to make the model produce different responses and get different scores, for example. <ol> <li>Note: We believe the dataset shuffling has a bug — the dataset is shuffled using the same seed for some reason (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/lm_tasks.py#L94-L97" target="_blank" rel="noopener noreferrer">lm_human_preferences/lm_tasks.py#L94-L97</a>).</li> </ol> </li> </ol> </li> </ol> <h1 id="reward-model-implementation-details">Reward Model Implementation Details</h1> <p>In this section, we discuss reward-model-specific implementation details. We talk about details such as reward normalization and layer initialization. Here are these details in no particular order:</p> <ol> <li> <strong>The reward model only outputs the value at the last token.</strong> <ol> <li>Notice that the rewards obtained after the forward pass on the concatenation of <code class="language-plaintext highlighter-rouge">query</code> and <code class="language-plaintext highlighter-rouge">response</code> will have the shape <code class="language-plaintext highlighter-rouge">(B, T, 1)</code>, where <code class="language-plaintext highlighter-rouge">B</code> is the batch size, <code class="language-plaintext highlighter-rouge">T</code> is the sequence length (which is always the same; it is <code class="language-plaintext highlighter-rouge">query_length + response_length = 64 + 24 = 88</code> in OpenAI’s setting for stylistic tasks, see <a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/launch.py#L9-L11" target="_blank" rel="noopener noreferrer">launch.py#L9-L11</a>), and <code class="language-plaintext highlighter-rouge">1</code> is the reward head dimension of 1. For RLHF purposes, the original codebase extracts the reward of the last token (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/rewards.py#L132" target="_blank" rel="noopener noreferrer">lm_human_preferences/rewards.py#L132</a>), so that the rewards will only have shape <code class="language-plaintext highlighter-rouge">(B, 1)</code>.</li> <li>Note that in a more recent codebase <a href="https://github.com/openai/summarize-from-feedback" target="_blank" rel="noopener noreferrer"><em>openai/summarize-from-feedback</em></a>, OpenAI stops sampling when encountering EOS token (<a href="https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/utils/experiment_helpers.py#L19" target="_blank" rel="noopener noreferrer">summarize_from_feedback/utils/experiment_helpers.py#L19</a>). When extracting rewards, it is going to identify the <code class="language-plaintext highlighter-rouge">last_response_index</code>, the index before the EOS token (<a href="https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/reward_model.py#L11-L13" target="_blank" rel="noopener noreferrer">#L11-L13</a>), and extract the reward at that index (<a href="https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/reward_model.py#L59" target="_blank" rel="noopener noreferrer">summarize_from_feedback/reward_model.py#L59</a>). However in this work we just stick with the original setting.</li> </ol> </li> <li> <strong>Reward head layer initialization</strong> <ol> <li>The weight of the reward head is initialized according to \( \mathcal{N}\left(0,1 /\left(\sqrt{d_{\text {model }}+1}\right)\right) \) (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L368" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/model.py#L368,</a> <a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L251-L252" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/model.py#L251-L252</a>). This aligns with the settings in Stiennon et al., 2020 <d-cite key="stiennon2020learning"></d-cite> (<a href="https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/query_response_model.py#L106-L107" target="_blank" rel="noopener noreferrer">summarize_from_feedback/query_response_model.py#L106-L107</a>) (P.S., Stiennon et al., 2020 <d-cite key="stiennon2020learning"></d-cite> had a typo on page 17 saying the distribution is \( \mathcal{N}\left(0,1 /\left(d_{\text {model }}+1\right)\right) \) without the square root)</li> <li>The bias of the reward head is set to 0 (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L254" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/model.py#L254</a>).</li> </ol> </li> <li> <strong>Reward model normalization before and after</strong> <ol> <li>In the paper, Ziegler el al. (2019) mentioned that “to keep the scale of the reward model consistent across training, we normalize it so that it has mean 0 and variance 1 for<br> \( x \sim \mathcal{D}, y \sim \rho(·|x) \).” To perform the normalization process, the code first creates a <code class="language-plaintext highlighter-rouge">reward_gain</code> and <code class="language-plaintext highlighter-rouge">reward_bias</code>, such that the reward can be calculated by <code class="language-plaintext highlighter-rouge">reward = reward * reward_gain + reward_bias</code> (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/rewards.py#L50-L51" target="_blank" rel="noopener noreferrer">lm_human_preferences/rewards.py#L50-L51</a>).</li> <li>When performing the normalization process, the code first sets <code class="language-plaintext highlighter-rouge">reward_gain=1, reward_bias=0</code> (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L211" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_reward.py#L211</a>), followed by collecting sampled queries from the target dataset (e.g., <code class="language-plaintext highlighter-rouge">bookcorpus, tldr, cnndm</code>), completed responses, and evaluated rewards. It then gets the <strong>empirical mean and std</strong> of the evaluated reward (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L162-L167" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_reward.py#L162-L167</a>) and tries to compute what the <code class="language-plaintext highlighter-rouge">reward_gain</code> and <code class="language-plaintext highlighter-rouge">reward_bias</code> should be.</li> <li>Let us use \( \mu_{\mathcal{D}} \) to denote the empirical mean, \( \sigma_{\mathcal{D}} \) the empirical std, \(g\) the <code class="language-plaintext highlighter-rouge">reward_gain</code>, \(b\) <code class="language-plaintext highlighter-rouge">reward_bias</code>, \( \mu_{\mathcal{T}} = 0\) <strong>target mean</strong> and \( \sigma_{\mathcal{T}}=1\) <strong>target std</strong>. Then we have the following formula.</li> </ol> \[\begin{aligned}g*\mathcal{N}(\mu_{\mathcal{D}}, \sigma_{\mathcal{D}}) + b &amp;= \mathcal{N}(g*\mu_{\mathcal{D}}, g*\sigma_{\mathcal{D}}) + b\\&amp;= \mathcal{N}(g*\mu_{\mathcal{D}} + b, g*\sigma_{\mathcal{D}}) \\&amp;= \mathcal{N}(\mu_{\mathcal{T}}, \sigma_{\mathcal{T}}) \\g &amp;= \frac{\sigma_{\mathcal{T}}}{\sigma_{\mathcal{D}}} \\b &amp;= \mu_{\mathcal{T}} - g*\mu_{\mathcal{D}}\end{aligned}\] <ol> <li> <p>The normalization process is then applied <strong>before</strong> and <strong>after</strong> reward model training (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L232-L234" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_reward.py#L232-L234</a>, <a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L252-L254" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_reward.py#L252-L254</a>).</p> </li> <li> <p>Note that responses \( y \sim \rho(·|x) \) we generated for the normalization purpose are from the pre-trained language model \(\rho \). The model \(\rho \) is fixed as a reference and is not updated in reward learning (<a href="https://github.com/openai/lm-human-preferences/blob/master/lm_human_preferences/train_reward.py#L286C1-L286C31" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_reward.py#L286C1-L286C31</a>).</p> </li> </ol> </li> </ol> <h1 id="policy-training-implementation-details">Policy Training Implementation Details</h1> <p>In this section, we will delve into details, such as layer initialization, data post-processing, and dropout settings. We will also explore techniques, such as of rejection sampling and reward “whitening”, and adaptive KL. Here are these details in no particular order:</p> <ol> <li> <strong>Scale the logits by sampling temperature.</strong> <ol> <li>When calculating the log probability of responses, the model first outputs the logits of the tokens in the responses, followed by dividing the logits with the sampling temperature (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/policy.py#L121" target="_blank" rel="noopener noreferrer">lm_human_preferences/policy.py#L121</a>). I.e., <code class="language-plaintext highlighter-rouge">logits /= self.temperature</code> </li> <li>In an informal test, we found that without this scaling, the KL would rise faster than expected, and performance would deteriorate.</li> </ol> </li> <li> <strong>Value head layer initialization</strong> <ol> <li>The weight of the value head is initialized according to \(\mathcal{N}\left(0,0\right)\) (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L368" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/model.py#L368,</a> <a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L251-L252" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/model.py#L251-L252</a>). This is</li> <li>The bias of the reward head is set to 0 (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L254" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/model.py#L254</a>).</li> </ol> </li> <li> <strong>Select query texts that start and end with a period</strong> <ol> <li>This is done as part of the data preprocessing; <ol> <li>Tries to select text only after <code class="language-plaintext highlighter-rouge">start_text="."</code> (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L51" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/datasets.py#L51</a>)</li> <li>Tries select text just before <code class="language-plaintext highlighter-rouge">end_text="."</code> (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L61" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/datasets.py#L61</a>)</li> <li>Then pad the text (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L66-L67" target="_blank" rel="noopener noreferrer">lm_human_preferences/language/datasets.py#L66-L67</a>)</li> </ol> </li> <li>When running <code class="language-plaintext highlighter-rouge">openai/lm-human-preferences</code>, OpenAI’s datasets were partially corrupted/lost (<a href="https://github.com/openai/lm-human-preferences/issues/17#issuecomment-1044051496" target="_blank" rel="noopener noreferrer">openai/lm-human-preferences/issues/17#issuecomment-104405149</a>), so we had to replace them with similar HF datasets, which may or may not cause a performance difference)</li> <li>For the book dataset, we used <a href="https://huggingface.co/datasets/bookcorpus" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/bookcorpus</a>, which we find not necessary to extract sentences that start and end with periods because the dataset ) is already pre-processed this way (e.g., <code class="language-plaintext highlighter-rouge">"usually , he would be tearing around the living room , playing with his toys ."</code>) To this end, we set <code class="language-plaintext highlighter-rouge">start_text=None, end_text=None</code> for the <code class="language-plaintext highlighter-rouge">sentiment</code> and <code class="language-plaintext highlighter-rouge">descriptiveness</code> tasks.</li> </ol> </li> <li> <strong>Disable dropout</strong> <ol> <li>Ziegler et al. (2019) <d-cite key="Ziegler2019fine"></d-cite> suggested, “We do not use dropout for policy training.” This is also done in the code (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/policy.py#L48" target="_blank" rel="noopener noreferrer">lm_human_preferences/policy.py#L48</a>).</li> </ol> </li> <li> <strong>Rejection sampling</strong> <ol> <li>Ziegler et al. (2019) <d-cite key="Ziegler2019fine"></d-cite> suggested, “We use rejection sampling to ensure there is a period between tokens 16 and 24 and then truncate at that period (This is a crude approximation for ‘end of sentence.’ We chose it because it is easy to integrate into the RL loop, and even a crude approximation is sufficient for the intended purpose of making the human evaluation task somewhat easier). During the RL finetuning, we penalize continuations that don’t have such a period by giving them a fixed reward of −1.”</li> <li>Specifically, this is achieved with the following steps: <ol> <li> <strong>Token truncation</strong>: We want to truncate at the first occurrence of <code class="language-plaintext highlighter-rouge">truncate_token</code> that appears at or after position <code class="language-plaintext highlighter-rouge">truncate_after</code> in the responses (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L378" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L378</a>)</li> <li> <strong>Run reward model on truncated response:</strong> After the response has been truncated by the token truncation process, the code then runs the reward model on the <strong>truncated response</strong>.</li> <li> <strong>Rejection sampling</strong>: if there is not a period between tokens 16 and 24, then replace the score of the response with a fixed low value (such as -1)(<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L384" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L384</a>, <a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L384-L402" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L384-L402</a>)</li> <li>To give some examples in <code class="language-plaintext highlighter-rouge">descriptiveness</code>:</li> </ol> <figure> </figure> </li> </ol> </li> </ol> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/descriptiveness-samples-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/descriptiveness-samples-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/descriptiveness-samples-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/descriptiveness-samples.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <p>&lt;/figure&gt;</p> <ol> <li> <strong>Discount factor = 1</strong> <ol> <li>The discount parameter \(\gamma\) is set to 1 (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L56" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L56</a>), which means that future rewards are given the same weight as immediate rewards.</li> </ol> </li> <li> <strong>Terminology of the training loop: batches and minibatches in PPO</strong> <ol> <li> <p>OpenAI uses the following training loop (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L184-L192" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L184-L192</a>). Note: we additionally added the <code class="language-plaintext highlighter-rouge">micro_batch_size</code> to help deal with the case in gradient accumulation. At each epoch, it shuffles the batch indices.</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>
 <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
 <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
 <span class="n">nminibatches</span> <span class="o">=</span> <span class="mi">2</span>
 <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">2</span>
 <span class="n">mini_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="n">nminibatches</span>
 <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">mini_batch_size</span> <span class="o">//</span> <span class="n">gradient_accumulation_steps</span>
 <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">data:</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">batch_size:</span><span class="sh">"</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">mini_batch_size:</span><span class="sh">"</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="p">)</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">micro_batch_size:</span><span class="sh">"</span><span class="p">,</span> <span class="n">micro_batch_size</span><span class="p">)</span>
 <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
     <span class="n">batch_inds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">permutation</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">epoch:</span><span class="sh">"</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="sh">"</span><span class="s">batch_inds:</span><span class="sh">"</span><span class="p">,</span> <span class="n">batch_inds</span><span class="p">)</span>
     <span class="k">for</span> <span class="n">mini_batch_start</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="p">):</span>
         <span class="n">mini_batch_end</span> <span class="o">=</span> <span class="n">mini_batch_start</span> <span class="o">+</span> <span class="n">mini_batch_size</span>
         <span class="n">mini_batch_inds</span> <span class="o">=</span> <span class="n">batch_inds</span><span class="p">[</span><span class="n">mini_batch_start</span><span class="p">:</span><span class="n">mini_batch_end</span><span class="p">]</span>
                
         <span class="c1"># `optimizer.zero_grad()` set optimizer to zero for gradient accumulation
</span>         <span class="k">for</span> <span class="n">micro_batch_start</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="p">,</span> <span class="n">micro_batch_size</span><span class="p">):</span>
             <span class="n">micro_batch_end</span> <span class="o">=</span> <span class="n">micro_batch_start</span> <span class="o">+</span> <span class="n">micro_batch_size</span> 
             <span class="n">micro_batch_inds</span> <span class="o">=</span> <span class="n">mini_batch_inds</span><span class="p">[</span><span class="n">micro_batch_start</span><span class="p">:</span><span class="n">micro_batch_end</span><span class="p">]</span>
             <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">____⏩ a forward pass on</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">micro_batch_inds</span><span class="p">])</span>
         <span class="c1"># `optimizer.step()`
</span>         <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">⏪ a backward pass on</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">mini_batch_inds</span><span class="p">])</span>
        
 <span class="c1"># data: [0. 1. 2. 3. 4. 5. 6. 7.]
</span> <span class="c1"># batch_size: 8
</span> <span class="c1"># mini_batch_size: 4
</span> <span class="c1"># micro_batch_size: 2
</span> <span class="c1"># epoch: 0 batch_inds: [6 4 0 7 3 5 1 2]
</span> <span class="c1"># ____⏩ a forward pass on [6. 4.]
</span> <span class="c1"># ____⏩ a forward pass on [0. 7.]
</span> <span class="c1"># ⏪ a backward pass on [6. 4. 0. 7.]
</span> <span class="c1"># ____⏩ a forward pass on [3. 5.]
</span> <span class="c1"># ____⏩ a forward pass on [1. 2.]
</span> <span class="c1"># ⏪ a backward pass on [3. 5. 1. 2.]
</span> <span class="c1"># epoch: 1 batch_inds: [6 7 3 2 0 4 5 1]
</span> <span class="c1"># ____⏩ a forward pass on [6. 7.]
</span> <span class="c1"># ____⏩ a forward pass on [3. 2.]
</span> <span class="c1"># ⏪ a backward pass on [6. 7. 3. 2.]
</span> <span class="c1"># ____⏩ a forward pass on [0. 4.]
</span> <span class="c1"># ____⏩ a forward pass on [5. 1.]
</span> <span class="c1"># ⏪ a backward pass on [0. 4. 5. 1.]
</span> <span class="c1"># epoch: 2 batch_inds: [1 4 5 6 0 7 3 2]
</span> <span class="c1"># ____⏩ a forward pass on [1. 4.]
</span> <span class="c1"># ____⏩ a forward pass on [5. 6.]
</span> <span class="c1"># ⏪ a backward pass on [1. 4. 5. 6.]
</span> <span class="c1"># ____⏩ a forward pass on [0. 7.]
</span> <span class="c1"># ____⏩ a forward pass on [3. 2.]
</span> <span class="c1"># ⏪ a backward pass on [0. 7. 3. 2.]
</span> <span class="c1"># epoch: 3 batch_inds: [7 2 4 1 3 0 6 5]
</span> <span class="c1"># ____⏩ a forward pass on [7. 2.]
</span> <span class="c1"># ____⏩ a forward pass on [4. 1.]
</span> <span class="c1"># ⏪ a backward pass on [7. 2. 4. 1.]
</span> <span class="c1"># ____⏩ a forward pass on [3. 0.]
</span> <span class="c1"># ____⏩ a forward pass on [6. 5.]
</span> <span class="c1"># ⏪ a backward pass on [3. 0. 6. 5.]
</span></code></pre></div> </div> </li> </ol> </li> <li> <strong>Per-token KL penalty</strong> <ul> <li>The code adds a per-token KL penalty (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L150-L153" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L150-L153</a>) to the rewards, in order to discourage the policy to be very different from the original policy.</li> <li>Using the <code class="language-plaintext highlighter-rouge">"usually, he would"</code> as an example, it gets tokenized to <code class="language-plaintext highlighter-rouge">[23073, 11, 339, 561]</code>. Say we use <code class="language-plaintext highlighter-rouge">[23073]</code> as the query and <code class="language-plaintext highlighter-rouge">[11, 339, 561]</code> as the response. Then under the default <code class="language-plaintext highlighter-rouge">gpt2</code> parameters, the response tokens will have log probabilities of the reference policy <code class="language-plaintext highlighter-rouge">logprobs=[-3.3213, -4.9980, -3.8690]</code> . <ul> <li>During the first PPO update epoch and minibatch update, so the active policy will have the same log probabilities <code class="language-plaintext highlighter-rouge">new_logprobs=[-3.3213, -4.9980, -3.8690]</code>. , so the per-token KL penalty would be <code class="language-plaintext highlighter-rouge">kl = new_logprobs - logprobs = [0., 0., 0.,]</code> </li> <li>However, after the first gradient backward pass, we could have <code class="language-plaintext highlighter-rouge">new_logprob=[3.3213, -4.9980, -3.8690]</code> , so the per-token KL penalty becomes <code class="language-plaintext highlighter-rouge">kl = new_logprobs - logprobs = [-0.3315, -0.0426, 0.6351]</code> </li> <li>Then the <code class="language-plaintext highlighter-rouge">non_score_reward = beta * kl</code> , where <code class="language-plaintext highlighter-rouge">beta</code> is the KL penalty coefficient \(\beta\), and it’s added to the <code class="language-plaintext highlighter-rouge">score</code> obtained from the reward model to create the <code class="language-plaintext highlighter-rouge">rewards</code> used for training. The <code class="language-plaintext highlighter-rouge">score</code> is only given at the end of episode; it could look like <code class="language-plaintext highlighter-rouge">[0.4,]</code> , and we have <code class="language-plaintext highlighter-rouge">rewards = [beta * -0.3315, beta * -0.0426, beta * 0.6351 + 0.4]</code>.</li> </ul> </li> </ul> </li> <li> <strong>Per-minibatch reward and advantage whitening, with optional mean shifting</strong> <ol> <li>OpenAI implements a <code class="language-plaintext highlighter-rouge">whiten</code> function that looks like below, basically normalizing the <code class="language-plaintext highlighter-rouge">values</code> by subtracting its mean followed by dividing by its standard deviation. Optionally, <code class="language-plaintext highlighter-rouge">whiten</code> can shift back the mean of the whitened <code class="language-plaintext highlighter-rouge">values</code> with <code class="language-plaintext highlighter-rouge">shift_mean=True</code>.</li> </ol> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">whiten</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">shift_mean</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
     <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
     <span class="n">whitened</span> <span class="o">=</span> <span class="p">(</span><span class="n">values</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rsqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
     <span class="k">if</span> <span class="ow">not</span> <span class="n">shift_mean</span><span class="p">:</span>
         <span class="n">whitened</span> <span class="o">+=</span> <span class="n">mean</span>
     <span class="k">return</span> <span class="n">whitened</span>
</code></pre></div> </div> <ol> <li>In each minibatch, OpenAI then whitens the reward <code class="language-plaintext highlighter-rouge">whiten(rewards, shift_mean=False)</code> without shifting the mean (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L325" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L325</a>) and whitens the advantages <code class="language-plaintext highlighter-rouge">whiten(advantages)</code> with the shifted mean (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L338" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L338</a>).</li> <li> <strong>Optimization note:</strong> if the number of minibatches is one (which is the case in this reproduction) we only need to whiten rewards, calculate and whiten advantages once since their values won’t change.</li> <li> <p><strong>TensorFlow vs PyTorch note:</strong> Different behavior of <code class="language-plaintext highlighter-rouge">tf.moments</code> vs <code class="language-plaintext highlighter-rouge">torch.var</code>: The behavior of whitening is different in torch vs tf because the variance calculation is different:</p> <div class="language-jsx highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="k">import</span> <span class="nx">numpy</span> <span class="nx">as</span> <span class="nx">np</span>
 <span class="k">import</span> <span class="nx">tensorflow</span> <span class="nx">as</span> <span class="nx">tf</span>
 <span class="k">import</span> <span class="nx">torch</span>
        
 <span class="nx">def</span> <span class="nf">whiten_tf</span><span class="p">(</span><span class="nx">values</span><span class="p">,</span> <span class="nx">shift_mean</span><span class="o">=</span><span class="nx">True</span><span class="p">):</span>
     <span class="nx">mean</span><span class="p">,</span> <span class="kd">var</span> <span class="o">=</span> <span class="nx">tf</span><span class="p">.</span><span class="nx">nn</span><span class="p">.</span><span class="nf">moments</span><span class="p">(</span><span class="nx">values</span><span class="p">,</span> <span class="nx">axes</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nx">values</span><span class="p">.</span><span class="nx">shape</span><span class="p">.</span><span class="nx">rank</span><span class="p">)))</span>
     <span class="nx">mean</span> <span class="o">=</span> <span class="nx">tf</span><span class="p">.</span><span class="nc">Print</span><span class="p">(</span><span class="nx">mean</span><span class="p">,</span> <span class="p">[</span><span class="nx">mean</span><span class="p">],</span> <span class="dl">'</span><span class="s1">mean</span><span class="dl">'</span><span class="p">,</span> <span class="nx">summarize</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
     <span class="kd">var</span> <span class="o">=</span> <span class="nx">tf</span><span class="p">.</span><span class="nc">Print</span><span class="p">(</span><span class="kd">var</span><span class="p">,</span> <span class="p">[</span><span class="kd">var</span><span class="p">],</span> <span class="dl">'</span><span class="s1">var</span><span class="dl">'</span><span class="p">,</span> <span class="nx">summarize</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
     <span class="nx">whitened</span> <span class="o">=</span> <span class="p">(</span><span class="nx">values</span> <span class="o">-</span> <span class="nx">mean</span><span class="p">)</span> <span class="o">*</span> <span class="nx">tf</span><span class="p">.</span><span class="nf">rsqrt</span><span class="p">(</span><span class="kd">var</span> <span class="o">+</span> <span class="mi">1</span><span class="nx">e</span><span class="o">-</span><span class="mi">8</span><span class="p">)</span>
     <span class="k">if</span> <span class="nx">not</span> <span class="nx">shift_mean</span><span class="p">:</span>
         <span class="nx">whitened</span> <span class="o">+=</span> <span class="nx">mean</span>
     <span class="k">return</span> <span class="nx">whitened</span>
        
 <span class="nx">def</span> <span class="nf">whiten_pt</span><span class="p">(</span><span class="nx">values</span><span class="p">,</span> <span class="nx">shift_mean</span><span class="o">=</span><span class="nx">True</span><span class="p">,</span> <span class="nx">unbiased</span><span class="o">=</span><span class="nx">True</span><span class="p">):</span>
     <span class="nx">mean</span><span class="p">,</span> <span class="kd">var</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nx">values</span><span class="p">),</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="nx">values</span><span class="p">,</span> <span class="nx">unbiased</span><span class="o">=</span><span class="nx">unbiased</span><span class="p">)</span>
     <span class="nf">print</span><span class="p">(</span><span class="dl">"</span><span class="s2">mean</span><span class="dl">"</span><span class="p">,</span> <span class="nx">mean</span><span class="p">)</span>
     <span class="nf">print</span><span class="p">(</span><span class="dl">"</span><span class="s2">var</span><span class="dl">"</span><span class="p">,</span> <span class="kd">var</span><span class="p">)</span>
     <span class="nx">whitened</span> <span class="o">=</span> <span class="p">(</span><span class="nx">values</span> <span class="o">-</span> <span class="nx">mean</span><span class="p">)</span> <span class="o">*</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">rsqrt</span><span class="p">(</span><span class="kd">var</span> <span class="o">+</span> <span class="mi">1</span><span class="nx">e</span><span class="o">-</span><span class="mi">8</span><span class="p">)</span>
     <span class="k">if</span> <span class="nx">not</span> <span class="nx">shift_mean</span><span class="p">:</span>
         <span class="nx">whitened</span> <span class="o">+=</span> <span class="nx">mean</span>
     <span class="k">return</span> <span class="nx">whitened</span>
        
 <span class="nx">rewards</span> <span class="o">=</span> <span class="nx">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
     <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span>
 <span class="p">])</span>
        
 <span class="kd">with</span> <span class="nx">tf</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span> <span class="nx">as</span> <span class="nx">sess</span><span class="p">:</span>
     <span class="nf">print</span><span class="p">(</span><span class="nx">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="nf">whiten_tf</span><span class="p">(</span><span class="nx">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">(</span><span class="nx">rewards</span><span class="p">,</span> <span class="nx">dtype</span><span class="o">=</span><span class="nx">tf</span><span class="p">.</span><span class="nx">float32</span><span class="p">),</span> <span class="nx">shift_mean</span><span class="o">=</span><span class="nx">False</span><span class="p">)))</span>
     <span class="nf">print</span><span class="p">(</span><span class="nf">whiten_pt</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nx">rewards</span><span class="p">),</span> <span class="nx">shift_mean</span><span class="o">=</span><span class="nx">False</span><span class="p">,</span> <span class="nx">unbiased</span><span class="o">=</span><span class="nx">True</span><span class="p">))</span>
     <span class="nf">print</span><span class="p">(</span><span class="nf">whiten_pt</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nx">rewards</span><span class="p">),</span> <span class="nx">shift_mean</span><span class="o">=</span><span class="nx">False</span><span class="p">,</span> <span class="nx">unbiased</span><span class="o">=</span><span class="nx">False</span><span class="p">))</span>
</code></pre></div> </div> <div class="language-jsx highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="nx">mean</span><span class="p">[</span><span class="mf">1.5999999</span><span class="p">]</span>
 <span class="kd">var</span><span class="p">[</span><span class="mf">0.0666666627</span><span class="p">]</span>
 <span class="p">[[</span><span class="mf">0.05080712</span> <span class="mf">0.4381051</span>  <span class="mf">0.8254035</span> <span class="p">]</span>
  <span class="p">[</span><span class="mf">1.2127019</span>  <span class="mf">1.6000004</span>  <span class="mf">1.9872988</span> <span class="p">]</span>
  <span class="p">[</span><span class="mf">2.3745968</span>  <span class="mf">2.7618952</span>  <span class="mf">3.1491938</span> <span class="p">]]</span>
 <span class="nx">mean</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">1.6000</span><span class="p">,</span> <span class="nx">dtype</span><span class="o">=</span><span class="nx">torch</span><span class="p">.</span><span class="nx">float64</span><span class="p">)</span>
 <span class="kd">var</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">0.0750</span><span class="p">,</span> <span class="nx">dtype</span><span class="o">=</span><span class="nx">torch</span><span class="p">.</span><span class="nx">float64</span><span class="p">)</span>
 <span class="nf">tensor</span><span class="p">([[</span><span class="mf">0.1394</span><span class="p">,</span> <span class="mf">0.5046</span><span class="p">,</span> <span class="mf">0.8697</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">1.2349</span><span class="p">,</span> <span class="mf">1.6000</span><span class="p">,</span> <span class="mf">1.9651</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">2.3303</span><span class="p">,</span> <span class="mf">2.6954</span><span class="p">,</span> <span class="mf">3.0606</span><span class="p">]],</span> <span class="nx">dtype</span><span class="o">=</span><span class="nx">torch</span><span class="p">.</span><span class="nx">float64</span><span class="p">)</span>
 <span class="nx">mean</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">1.6000</span><span class="p">,</span> <span class="nx">dtype</span><span class="o">=</span><span class="nx">torch</span><span class="p">.</span><span class="nx">float64</span><span class="p">)</span>
 <span class="kd">var</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">0.0667</span><span class="p">,</span> <span class="nx">dtype</span><span class="o">=</span><span class="nx">torch</span><span class="p">.</span><span class="nx">float64</span><span class="p">)</span>
 <span class="nf">tensor</span><span class="p">([[</span><span class="mf">0.0508</span><span class="p">,</span> <span class="mf">0.4381</span><span class="p">,</span> <span class="mf">0.8254</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">1.2127</span><span class="p">,</span> <span class="mf">1.6000</span><span class="p">,</span> <span class="mf">1.9873</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">2.3746</span><span class="p">,</span> <span class="mf">2.7619</span><span class="p">,</span> <span class="mf">3.1492</span><span class="p">]],</span> <span class="nx">dtype</span><span class="o">=</span><span class="nx">torch</span><span class="p">.</span><span class="nx">float64</span><span class="p">)</span>
        
</code></pre></div> </div> </li> </ol> </li> <li> <strong>Clipped value function</strong> <ol> <li>As done in the original PPO (<a href="https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/ppo2/model.py#L68-L75" target="_blank" rel="noopener noreferrer">baselines/ppo2/model.py#L68-L75</a>), the value function is clipped (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L343-L348" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L343-L348</a>) in a similar fashion as the policy objective.</li> </ol> </li> <li> <strong>Adaptive KL</strong> <ul> <li> <p>The KL divergence penalty coefficient \(\beta\) is modified adaptively based on the KL divergence between the current policy and the previous policy. If the KL divergence is outside a predefined target range, the penalty coefficient is adjusted to bring it closer to the target range (<a href="https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L115-L124" target="_blank" rel="noopener noreferrer">lm_human_preferences/train_policy.py#L115-L124</a>). It’s implemented as follows:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="k">class</span> <span class="nc">AdaptiveKLController</span><span class="p">:</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">init_kl_coef</span><span class="p">,</span> <span class="n">hparams</span><span class="p">):</span>
          <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">init_kl_coef</span>
          <span class="n">self</span><span class="p">.</span><span class="n">hparams</span> <span class="o">=</span> <span class="n">hparams</span>
        
      <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">):</span>
          <span class="n">target</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">.</span><span class="n">target</span>
          <span class="n">proportional_error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">current</span> <span class="o">/</span> <span class="n">target</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
          <span class="n">mult</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">proportional_error</span> <span class="o">*</span> <span class="n">n_steps</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">.</span><span class="n">horizon</span>
          <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">*=</span> <span class="n">mult</span>
</code></pre></div> </div> </li> <li> <p>For the <code class="language-plaintext highlighter-rouge">sentiment</code> and <code class="language-plaintext highlighter-rouge">descriptiveness</code> tasks examined in this work, we have <code class="language-plaintext highlighter-rouge">init_kl_coef=0.15, hparams.target=6, hparams.horizon=10000</code>.</p> </li> </ul> </li> </ol> <h2 id="pytorch-adam-optimizer-numerical-issues-wrt-rlhf"><strong>PyTorch Adam optimizer numerical issues w.r.t RLHF</strong></h2> <ul> <li>This implementation detail is so interesting that it deserves a full section.</li> <li>PyTorch Adam optimizer (<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html" target="_blank" rel="noopener noreferrer">torch.optim.Adam.html</a>) has a different implementation compared to TensorFlow’s Adam optimizer (TF1 Adam at <a href="https://github.com/tensorflow/tensorflow/blob/v1.15.2/tensorflow/python/training/adam.py" target="_blank" rel="noopener noreferrer">tensorflow/v1.15.2/adam.py</a>, TF2 Adam at <a href="https://github.com/keras-team/keras/blob/v2.13.1/keras/optimizers/adam.py#L26-L220" target="_blank" rel="noopener noreferrer">keras/adam.py#L26-L220</a>). In particular, <strong>PyTorch follows Algorithm 1</strong> of the Kingma and Ba’s Adam <d-cite key="Kingma2015adam"></d-cite>, but <strong>TensorFlow uses the formulation just before Section 2.1</strong> of the paper and its <code class="language-plaintext highlighter-rouge">epsilon</code> referred to here is <code class="language-plaintext highlighter-rouge">epsilon hat</code> in the paper. In a pseudocode comparison, we have the following</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### pytorch adam implementation:
</span><span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">step</span>
<span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">step</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="n">bias_correction1</span>
<span class="n">bias_correction2_sqrt</span> <span class="o">=</span> <span class="nf">_dispatch_sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)</span>
<span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">bias_correction2_sqrt</span><span class="p">).</span><span class="nf">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<span class="n">param</span><span class="p">.</span><span class="nf">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>

<span class="c1">### tensorflow adam implementation:
</span><span class="n">lr_t</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="nf">_dispatch_sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">step</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">step</span><span class="p">)</span>
<span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">().</span><span class="nf">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
<span class="n">param</span><span class="p">.</span><span class="nf">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">lr_t</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Let’s compare the update equations of pytorch-style and tensorflow-style adam. Following the notation of the adam paper <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener noreferrer">(Kingma and Ba, 2014)</a>, we have the gradient update rules for pytorch adam (Algorithm 1 of Kingma and Ba’s paper) and tensorflow-style adam (the formulation just before Section 2.1 of Kingma and Ba’s paper) as below:</li> </ul> \[\begin{aligned}\text{pytorch adam :}\quad \theta_t &amp; =\theta_{t-1}-\alpha \cdot \hat{m}_t /\left(\sqrt{\hat{v}_t}+\varepsilon\right) \\&amp; =\theta_{t-1}- \alpha \underbrace{\left[m_t /\left(1-\beta_1^t\right)\right]}_{=\hat{m}_t} /\left[\sqrt{\underbrace{v_t /\left(1-\beta_2^t\right)}_{=\hat{v}_t} }+\varepsilon\right]\\&amp; =\theta_{t-1}- \alpha\left[m_t /\left(1-\beta_1^t\right)\right]\frac{\sqrt{1-\beta_2^t}}{\sqrt{v_t}+\color{green}{\varepsilon \sqrt{1-\beta_2^t}}}\end{aligned}\] \[\begin{aligned}\text{tensorflow adam:}\quad \theta_t &amp; =\theta_{t-1}-\alpha_t m_t /\left(\sqrt{v_t}+\hat{\varepsilon}\right) \\&amp; =\theta_{t-1}-\underbrace{\left[\alpha \sqrt{1-\beta_2^t} /\left(1-\beta_1^t\right)\right]}_{=\alpha_t} m_t /\left(\sqrt{v_t}+\hat{\varepsilon}\right) \\&amp; =\theta_{t-1}- \alpha\left[m_t /\left(1-\beta_1^t\right)\right] \frac{\sqrt{1-\beta_2^t}}{\sqrt{v_t}+\color{green}{\hat{\varepsilon}}} \end{aligned}\] <ul> <li>The equations above highlight that the distinction between pytorch and tensorflow implementation is their <strong>normalization terms</strong>, \(\color{green}{\varepsilon \sqrt{1-\beta_2^t}}\) and \(\color{green}{\hat{\varepsilon}}\). The two versions are equivalent if we set \(\hat{\varepsilon} =\varepsilon \sqrt{1-\beta_2^t}\) . However, in the pytorch and tensorflow APIs, we can only set \(\varepsilon\) (pytorch) and \(\hat{\varepsilon}\) (tensorflow) via the <code class="language-plaintext highlighter-rouge">eps</code> argument, causing differences in their update equations. What if we set \(\varepsilon\) and \(\hat{\varepsilon}\) to the same value, say, 1e-5? Then for tensorflow adam, the normalization term \(\hat{\varepsilon} = \text{1e-5}\) is just a constant. But for pytorch adam, the normalization term \({\varepsilon \sqrt{1-\beta_2^t}}\) changes over time. Importantly, initially much smaller than 1e-5 when the timestep \(t\) is small, the term \({\varepsilon \sqrt{1-\beta_2^t}}\) gradually approaches to 1e-5 as timesteps increase. The plot below compares these two normalization terms over timesteps:</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/norma_const_comparison-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/norma_const_comparison-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/norma_const_comparison-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/norma_const_comparison.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>The above figure shows that, if we set the same <code class="language-plaintext highlighter-rouge">eps</code> in pytorch adam and tensorflow adam, then pytorch-adam uses a much smaller normalization term than tensorflow-adam in the early phase of training. In other words, pytorch adam goes for <strong>more aggressive gradient updates early in the training</strong>. Our experiments support this finding, as we will demonstrate below.</li> <li> <p>How does this impact reproducibility and performance? To align settings, we record the original query, response, and rewards from <a href="https://github.com/openai/lm-human-preferences" target="_blank" rel="noopener noreferrer">https://github.com/openai/lm-human-preferences</a> and save them. We also record the metrics of the first two epochs of training with TF1’s <code class="language-plaintext highlighter-rouge">AdamOptimizer</code> optimizer as the ground truth. Below are some key metrics:</p> <table> <thead> <tr> <th> </th> <th>OpenAI’s TF1 Adam</th> <th>PyTorch’s Adam</th> <th>Our custom Tensorflow-style Adam</th> </tr> </thead> <tbody> <tr> <td>policy/approxkl</td> <td>0.00037167023</td> <td>0.0023672834504395723</td> <td>0.000374998344341293</td> </tr> <tr> <td>policy/clipfrac</td> <td>0.0045572915</td> <td>0.02018229104578495</td> <td>0.0052083334885537624</td> </tr> <tr> <td>ratio_mean</td> <td>1.0051285</td> <td>1.0105520486831665</td> <td>1.0044583082199097</td> </tr> <tr> <td>ratio_var</td> <td>0.0007716546</td> <td>0.005374275613576174</td> <td>0.0007942612282931805</td> </tr> <tr> <td>ratio_max</td> <td>1.227216</td> <td>1.8121057748794556</td> <td>1.250215768814087</td> </tr> <tr> <td>ratio_min</td> <td>0.7400441</td> <td>0.4011387825012207</td> <td>0.7299948930740356</td> </tr> <tr> <td>logprob_diff_mean</td> <td>0.0047487603</td> <td>0.008101251907646656</td> <td>0.004073789343237877</td> </tr> <tr> <td>logprob_diff_var</td> <td>0.0007207897</td> <td>0.004668936599045992</td> <td>0.0007334011606872082</td> </tr> <tr> <td>logprob_diff_max</td> <td>0.20474821</td> <td>0.594489574432373</td> <td>0.22331619262695312</td> </tr> <tr> <td>logprob_diff_min</td> <td>-0.30104542</td> <td>-0.9134478569030762</td> <td>-0.31471776962280273</td> </tr> </tbody> </table> </li> <li> <strong>PyTorch’s <code class="language-plaintext highlighter-rouge">Adam</code> produces a more aggressive update</strong> for some reason. Here are some evidence: <ul> <li> <strong>PyTorch’s <code class="language-plaintext highlighter-rouge">Adam</code>’s <code class="language-plaintext highlighter-rouge">logprob_diff_var</code></strong> <strong>is 6x higher</strong>. Here <code class="language-plaintext highlighter-rouge">logprobs_diff = new_logprobs - logprobs</code> is the difference between the log probability of tokens between the initial and current policy after two epochs of training. Having a larger <code class="language-plaintext highlighter-rouge">logprob_diff_var</code> means the scale of the log probability changes is larger than that in OpenAI’s TF1 Adam.</li> <li> <strong>PyTorch’s <code class="language-plaintext highlighter-rouge">Adam</code> presents a more extreme ratio max and min.</strong> Here <code class="language-plaintext highlighter-rouge">ratio = torch.exp(logprobs_diff)</code>. Having a <code class="language-plaintext highlighter-rouge">ratio_max=1.8121057748794556</code> means that for some token, the probability of sampling that token is 1.8x more likely under the current policy, as opposed to only 1.2x with OpenAI’s TF1 Adam.</li> <li> <strong>Larger <code class="language-plaintext highlighter-rouge">policy/approxkl</code> <code class="language-plaintext highlighter-rouge">policy/clipfrac</code>.</strong> Because of the aggressive update, the ratio gets clipped <strong>4.4x more often, and the approximate KL divergence is 6x larger.</strong> </li> <li>The aggressive update is likely gonna cause further issues. E.g., <code class="language-plaintext highlighter-rouge">logprob_diff_mean</code> is 1.7x larger in PyTorch’s <code class="language-plaintext highlighter-rouge">Adam</code>, which would correspond to 1.7x larger KL penalty in the next reward calculation; this could get compounded. In fact, this might be related to the famous KL divergence issue — KL penalty is much larger than it should be and the model could pay more attention and optimizes for it more instead, therefore causing negative KL divergence.</li> </ul> </li> <li> <strong>Larger models get affected more.</strong> We conducted experiments comparing PyTorch’s <code class="language-plaintext highlighter-rouge">Adam</code> (codename <code class="language-plaintext highlighter-rouge">pt_adam</code>) and our custom TensorFlow-style (codename <code class="language-plaintext highlighter-rouge">tf_adam</code>) with <code class="language-plaintext highlighter-rouge">gpt2</code> and <code class="language-plaintext highlighter-rouge">gpt2-xl</code>. We found that the performance are roughly similar under <code class="language-plaintext highlighter-rouge">gpt2</code>; however with <code class="language-plaintext highlighter-rouge">gpt2-xl</code>, we observed a more aggressive updates, meaning that larger models get affected by this issue more. <ul> <li>When the initial policy updates are more aggressive in <code class="language-plaintext highlighter-rouge">gpt2-xl</code>, the training dynamics get affected. For example, we see a much larger <code class="language-plaintext highlighter-rouge">objective/kl</code> and <code class="language-plaintext highlighter-rouge">objective/scores</code> spikes with <code class="language-plaintext highlighter-rouge">pt_adam</code>, especially with <code class="language-plaintext highlighter-rouge">sentiment</code> — <em>the biggest KL was as large as 17.5</em> in one of the random seeds, suggesting an undesirable over-optimization.</li> <li>Furthermore, because of the larger KL, many other training metrics are affected as well. For example, we see a much larger <code class="language-plaintext highlighter-rouge">clipfrac</code> (the fraction of time the <code class="language-plaintext highlighter-rouge">ratio</code> gets clipped by PPO’s objective clip coefficient 0.2) and <code class="language-plaintext highlighter-rouge">approxkl</code>.</li> </ul> </li> </ul> <div class="l-page"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/adam_gpt2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/adam_gpt2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/adam_gpt2-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/adam_gpt2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/adam_gpt2_xl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/adam_gpt2_xl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/adam_gpt2_xl-1400.webp"></source> <img src="/2024/assets/img/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo/adam_gpt2_xl.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h1 id="limitations">Limitations</h1> <p>Noticed this work does not try to reproduce the summarization work in CNN DM or TL;DR. This was because we found the training to be time-consuming and brittle.</p> <p>The particular training run we had showed poor GPU utilization (around 30%), so it takes almost 4 days to perform a training run, which is highly expensive (only AWS sells p3dn.24xlarge, and it costs $31.212 per hour)</p> <p>Additionally, training was brittle. While the reward goes up, we find it difficult to reproduce the “smart copier” behavior reported by Ziegler et al. (2019) <d-cite key="Ziegler2019fine"></d-cite>. Below are some sample outputs — clearly, the agent overfits somehow.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>QUERY: The modern take on Death &amp; Taxes still plays it. I know as a Tron player I find Mindcensor 
pretty frustrating. ⏎ However, the answer to your question is probably that the decks that 
are currently taking up the lion's share of the metagame don't really care about its effect.
It has a chance to act as an instant speed Stone Rain + Gut Shot in response to a fetch 
crack, but after that it's a 2/1 flyer that dies to all relevant removal. ⏎ It's especially 
powerful against Tron since so much of the deck's consistency is in its tutor effects -- 
Sylvan Scrying, Expedition Map, and Eye of Ugin. This combined with Pyroclasm and Oblivion 
Stone being the only creature removal the deck has available to it before Karn make 
Mindcensor fairly potent. ⏎ I'm having trouble thinking of any other popular decks (ignoring
the interaction with fetches) that Mindcensor really sticks it to, however.

PPO GENERATED RESPONSE: Mindcensor is a very good card, but it's not really worth playing.&lt;|endoftext|&gt;Image 
caption The family was forced from their home after they were not allowed to leave the 
house⏎⏎A family of four has been jailed for five years after admitting breaching home rule 
by refusing to leave their home.⏎

reward: -0.48
kl: +11.37
total reward: -0.82

QUERY: To make a long story short, my SO (27/m) and I live in my hometown. We've been together for 
4 years. His family lives across the country. A lot of the time we spend with my parents but
besides that, we aren't very social. His family not being here doesn't make me feel like I 
can't be with mine. I just don't like to. ⏎ I find my extended family (so not my parents) 
extremely irritating. Especially the ones around my age. They're dramatic and rude. Every 
family event is some Jerry Springer episode I don't want to participate in. I don't have any
issues with anyone in particular. I just find them annoying. I'm never uncivil or openly 
telling them how I feel. If there's get togethers, I tend to avoid them. I don't ask anyone 
to hang out or come over. ⏎ My SO and I have experienced multiple miscarriages due to 
unknown reasons. This has made me very sensitive and a bit of a loaner when it comes to the 
girls in my family my age. I'm not jealous that my family has children. I just don't seem to
be able to relate to them. When I explained our issues, they reacted very unsympathetic and 
just weren't very supportive. I guess that was the time that I started to pull away and not 
try to keep up any relationships. ⏎ My SO and I are pregnant, half way! This is something 
we're so joyed about. However, when we told everyone, they just didn't really give a 
reaction. No congrats or that they were happy for us. Since then, I completely pulled away. 
I talked to my parents about this and I just don't feel the need to contact anyone. The 
reason I'm posting is because I discussed deleting some relatives off social media with my 
mom and she was telling me how it's really rude. I don't think it is. We don't talk. There's
no relationship. Honestly, I think they're just friends with me for gossip purposes. A few 
times I tried reaching out to some of my cousins with no reply. ⏎ I guess what I'm asking is
if I should be trying harder to establish something with my family or if it's perfectly OK 
just to delete them and not talk to them again? Our primary contact is Facebook even though 
we live in the same town.

PPO GENERATED RESPONSE:  We're dating now. We've met, we're dating. We're dating now. We're dating now. We're dating
now. We're dating now. We're dating now.⏎⏎We've met⏎⏎We're dating now.⏎⏎We're dating now.⏎

reward: +2.09
kl: +19.45
total reward: +1.50

QUERY: This tower totally decided to screw me over on the final stage. So a bit of background I 
don't have the strongest squad but pretty much I have: ⏎ Guardian Loch (Sacred Jewel) ⏎ 
Anima Tiara (Flesh Armor) ⏎ Lord Cayena (sacred Jewel) ⏎ Oracle Twins (sacred Jewel) ⏎ 
Guardian Logan (Burny soul stone) ⏎ and was using a friends maxed michelle. ⏎ So throughout 
the earlier stages I've seen this guy about 5 times but have never caught him. So I had a 
little trouble with stage 9 but felt like with the additional 50% atk boost and bb from 
michelle I could do it. Also on stage 9 the reason I died was because I didn't take any 
stimulants and this guy managed to paralyze 3 members of my squad. So I replaced mega cures 
for stimulants. ⏎ Round 3 I meet him. He gets a lucky crit on my caynea and takes her down. 
I have to use my one revive, take him down and  capture  him. I'm pretty excited. ⏎ Round 7 
I see the warning danger sign. I face him again and manage to take him down without any of 
my units dying no capture. At this point I'm starting to get low on healing items. Also I've
only heard of meeting him twice on a run. But then I checked this thread and saw that he 
always appears on the last stage. I thought that maybe it glitched and he wouldn't appear on
the final round. ⏎ Nope. ⏎ This guy shows up on the final round showing up 3 times in one 
run. Worst part was I manged to get him down to a sliver of health so that as long as one 
unit hit him I would've defeated him with the next turn. This was also the turn he decided 
to use soul desecration twice. I definitely felt like my soul got desecrated watching all my
units die. So at this point it was give up the captured unit and try all over again (knowing
my luck never capture him again) or gem for the last hit. I was really looking forward to 
getting the gem for completion (f2p gotta scrap up everything right?) but I let it go.

PPO GENERATED RESPONSE:  ⏎⏎When you capture a unit try to kill it.⏎⏎Kill them.⏎⏎Kill them.⏎⏎Kill them.⏎⏎Kill 
them.⏎⏎Kill them.⏎⏎Kill them.⏎⏎Kill them.⏎⏎Kill them.⏎⏎Kill them.⏎

reward: +0.79
kl: +10.82
total reward: +0.46
</code></pre></div></div> <h1 id="conclusion">Conclusion</h1> <p>In this work, we took a deep dive into OpenAI’s original RLHF codebase and compiled a list of its implementation details. We also created a minimal base which reproduces the same learning curves as OpenAI’s original RLHF codebase, when the dataset and hyperparameters are controlled. Furthermore, we identify surprising implementation details such as the adam optimizer’s setting which causes aggressive updates in early RLHF training.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2024/assets/bibliography/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2024" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>