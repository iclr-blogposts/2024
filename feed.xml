<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://iclr-blogposts.github.io/2024/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2024/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-09T00:19:46+02:00</updated><id>https://iclr-blogposts.github.io/2024/feed.xml</id><title type="html">ICLR Blogposts 2024</title><subtitle>Home to the 2024 ICLR Blogposts track </subtitle><entry><title type="html">Masked Language Model with ALiBi and CLAP head</title><link href="https://iclr-blogposts.github.io/2024/blog/alibi-mlm/" rel="alternate" type="text/html" title="Masked Language Model with ALiBi and CLAP head"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/alibi-mlm</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/alibi-mlm/"><![CDATA[<p><em>Adapted and expanded from <a href="https://github.com/EIFY/fairseq">EIFY/fairseq</a>.</em></p> <p>Unmodified and unmasked, attention mechanism is permutation-invariant and positional encoding is therefore employed by transformer-based language models to break the symmetry and enable sequence modeling. In their ICLR 2022 paper, Press et al. <d-cite key="DBLP:conf/iclr/PressSL22"></d-cite> introduced Attention with Linear Biases (ALiBi) as a new approach to positional encoding, where the positional info of the tokens are encoded by applying an attention weight bias proportional to the distance between tokens:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/ALiBi-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/ALiBi-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/ALiBi-1400.webp"/> <img src="/2024/assets/img/2024-05-07-alibi-mlm/ALiBi.jpeg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>where \(m\) is a head-specific slope chosen to follow geometric sequence \(\frac{1}{2^{0.5}}, \frac{1}{2^1}, \frac{1}{2^{1.5}}, \dots, \frac{1}{2^\frac{n}{2}}\) for a model with \(n\) attention heads. This approach is shown to enable input length extrapolation in the sense that perplexity of the model remains stable as the inference context length exceeds training context length. The paper, however, focuses on autoregressive decoder-only models and relies on model perplexity as the metric, therefore leaves the question open whether ALiBi is applicable to MLMs like BERT <d-cite key="DBLP:conf/naacl/DevlinCLT19"></d-cite> and RoBERTa <d-cite key="liu2019roberta"></d-cite>. To help answer this question, we tested the two following changes to the RoBERTa baseline models, based on the first-party Fairseq toolkit <d-cite key="ott2019fairseq"></d-cite>:</p> <h2 id="attention-with-linear-biases-alibi">Attention with Linear Biases (ALiBi)</h2> <p>Since MLMs are based on encoders that attend to tokens both before and after the given position, considerations must be made regarding how to distinguish them. Press himself <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5">suggested the 3 following options for encoder-attention ALiBi</a>:</p> <ol> <li>Symmetric: Keep attention weight bias proportional to the distance between tokens and rely on the context to distinguish between tokens at +N and -N position.</li> <li>Nonsymmetric, one-sided: Make half of the heads only attend to the tokens before and half of the heads only attend to the tokens after. Weight bias is still proportional to the distance.</li> <li>Nonsymmetric with different slopes: Make the slopes \(m\) different forward and backward, with either learned or fixed values.</li> </ol> <p>With the observation that option 2 spends about half of the attention compute on no-op and option 3 can still result in bias value collision (e.g. \(m_{bwd} = 2 m_{fwd}\) and -1 vs. +2 positions), we implemented both option 1 and what we call “nonsymmetric with offset”: <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5#issuecomment-1213410982">Shift the linear biases ahead by <code class="language-plaintext highlighter-rouge">0.5 * slope</code></a>, i.e. the constant bias (right matrix of the figure above) becomes</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 0 -.5 -1.5 -2.5 -3.5
-1   0  -.5 -1.5 -2.5
-2  -1    0  -.5 -1.5
-3  -2   -1    0  -.5
-4  -3   -2   -1    0
</code></pre></div></div> <p>Unless otherwise noted, ALiBi for the following experiments means this nonsymmetric-with-offset encoder-attention ALiBi.</p> <h2 id="contrastive-language-pretraining-clap-head">Contrastive Language Pretraining (CLAP) Head</h2> <p>The prediction head is one part of the LMs that has received less attention that happens to differ between the ALiBi autoregressive decoder-only models and RoBERTa. Based on the configs and <a href="https://github.com/ofirpress/attention_with_linear_biases#saved-checkpoints">training logs</a>, the ALiBi models use the adaptive word embedding and softmax of Baevski &amp; Auli <d-cite key="DBLP:conf/iclr/BaevskiA19"></d-cite> with weight tying <d-cite key="press-wolf-2017-using"></d-cite>, whereas the RoBERTa prediction head has an additional fully-connected layer and nonlinearity on top of weight-tying. Inspired by CLIP <d-cite key="DBLP:conf/icml/RadfordKHRGASAM21"></d-cite>, we decided to test what we called Contrastive Language Pretraining (CLAP) head below, as the <a href="https://github.com/EIFY/fairseq/blob/8143446dfa88d9f8e246b366bd335f6c9b018db0/fairseq/models/roberta/model.py#L527-L543">simplest possible prediction head with weight tying</a> for the masked tokens plus the thermodynamic beta (inverse temperature):</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ClapHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Head for masked language modeling.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">initial_beta</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">initial_beta</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">masked_tokens</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c1"># Only project the masked tokens while training,
</span>        <span class="c1"># saves both memory and computation
</span>        <span class="k">if</span> <span class="n">masked_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">masked_tokens</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">F</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></code></pre></figure> <p>Compared to the <a href="https://github.com/facebookresearch/fairseq/blob/da8fb630880d529ab47e53381c30ddc8ad235216/fairseq/models/roberta/model.py#L470-L495">baseline RoBERTa prediction head</a></p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">RobertaLMHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Head for masked language modeling.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">activation_fn</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="nf">get_activation_fn</span><span class="p">(</span><span class="n">activation_fn</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">output_dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">masked_tokens</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Only project the masked tokens while training,
</span>        <span class="c1"># saves both memory and computation
</span>        <span class="k">if</span> <span class="n">masked_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">masked_tokens</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dense</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation_fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># project back to size of vocabulary with bias
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">x</span></code></pre></figure> <p>We removed the <code class="language-plaintext highlighter-rouge">embed_dim x embed_dim</code> fully-connected layer, activation function (GELU), layer norm, and the <code class="language-plaintext highlighter-rouge">output_dim</code> trainable bias. Just like CLIP, we added the trainable thermodynamic beta and L2-normalize the token embeddings before feeding them to the transformer and computing the inner products between them and the transformer output as the softmax logits, scaled by beta.</p> <h2 id="experiments">Experiments</h2> <h3 id="wikitext-103">WikiText-103</h3> <p>At first we tested the changes with the <a href="https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/">WikiText-103 dataset</a> <d-cite key="DBLP:conf/iclr/MerityX0S17"></d-cite> with a GeForce RTX 3080 16 GB Laptop GPU, using the validation set MLM perplexity as the metric. We tested the baseline (learned positional encoding + RoBERTa prediction head), learned-clap (learned positional encoding + CLAP head), ALiBi (ALiBi + RoBERTa prediction head), and zero-clap (ALiBi + CLAP head), in addition to baseline but with sinusoidal positional encoding instead of learned positional encoding:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/valid_ppl_cleaned-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/valid_ppl_cleaned-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-alibi-mlm/valid_ppl_cleaned-1400.webp"/> <img src="/2024/assets/img/2024-05-07-alibi-mlm/valid_ppl_cleaned.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>where solid lines are what’s considered “canonical” setup and dotted lines are experiments with the following variations in setup. These variations turned out to be irrelevant:</p> <ol> <li>Whether we use attention dropout or not</li> <li>Whether we use <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5">symmetric ALiBi (option 1)</a> or nonsymmetric-with-offset ALiBi above</li> <li><del>Whether we use zero vector or a separate learnable embedding for the mask embedding</del><d-footnote>The intention was to test using zero vector instead of a separate learnable embedding for the mask embedding, which in combination with ALiBi results in no non-semantic information in the input embeddings. However, a bug prevented this variation from working correctly and the end effect was merely deleting the last two words (madeupword0001 and madeupword0002) from the dictionary instead, which we don't expect to be consequential.</d-footnote></li> <li>Whether we L2-normalize the embeddings for the CLAP head or not</li> <li>Whether we scale the L2-normalized embeddings by <code class="language-plaintext highlighter-rouge">sqrt(embed_dim)</code> (<code class="language-plaintext highlighter-rouge">no_scale_embedding=False</code>) or not</li> </ol> <p>As we can see, the dotted lines are almost on top of the solid lines. Notably, sinusoidal positional encoding underperforms significantly compared to learned positional encoding.</p> <h3 id="the-pile">The Pile</h3> <p>As the next step, we scaled our experiments to train on the Pile <d-cite key="DBLP:journals/corr/abs-2101-00027"></d-cite> for one epoch. About half of the examples in the Pile has sequence length &gt; 1024, so we set sequence length to 2048. Even so, ~1/7 of the examples have sequence length &gt; 2048 and had to be discarded. In the end, one epoch consists of 133082 updates and <a href="https://github.com/EIFY/fairseq/blob/33fb2c306851f104cc567b7fe865b1e3fd1e6fe7/examples/roberta/config/pretraining/baseline_pile.yaml#L31-L36">we employ cosine learning rate schedule while “overestimating” the number of training steps by 10%</a>, as inspired by the Chinchilla paper <d-cite key="hoffmann2022training"></d-cite>. In addition to the validation MLM perplexity, we also fine-tuned the models on the <a href="https://gluebenchmark.com/">GLUE</a> benchmark <d-cite key="wang-etal-2018-glue"></d-cite>. As in the original RoBERTa paper, we tested both the <code class="language-plaintext highlighter-rouge">roberta.base</code> with 125M parameters and <code class="language-plaintext highlighter-rouge">roberta.large</code> with 355M parameters. These experiments were performed on 8 x A100 40GB SXM4 GPUs, where the <code class="language-plaintext highlighter-rouge">roberta.base</code> experiments took ~3 days and <code class="language-plaintext highlighter-rouge">roberta.large</code> experiments took ~9 days. In the table below, <code class="language-plaintext highlighter-rouge">PPL</code> is the final validation MLM perplexity, <code class="language-plaintext highlighter-rouge">STS-B</code> is the best validation loss, and all the others are the best validation accuracies over 10 epochs of finetuning.</p> <h4 id="robertabase"><code class="language-plaintext highlighter-rouge">roberta.base</code></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>             PPL↓ CoLA MNLI MRPC QNLI QQP  RTE  SST-2 STS-B↓
baseline     2.94 83.6 84.2 90   91.6 91.3 73.6 92.1  0.028
learned-clap 2.86 81.7 84.4 86.3 90.9 91.2 72.6 92.5  0.027
alibi        2.93 69.2 85.1 80.9 92   91.5 63.9 93.1  0.033
zero-clap    2.83 70.5 84.9 75.5 90.6 91.1 54.9 89.7  0.041
</code></pre></div></div> <p>*<em>Baseline but with sinusoidal positional encoding instead of learned positional encoding failed to converge.</em></p> <h4 id="robertalarge"><code class="language-plaintext highlighter-rouge">roberta.large</code></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>             PPL↓ CoLA MNLI MRPC QNLI QQP  RTE  SST-2 STS-B↓
baseline*    2.55 83.7 86.8 84.3 92.5 91.8 79.8 93.3  0.027
learned-clap 2.5  84.1 86.3 89.7 92.8 91.7 79.8 93.7  0.023
alibi        2.65 69.1 86.5 68.4 92.4 91.7 52.7 93.6  0.123
zero-clap    2.54 69.1 86.7 81.9 92.2 91.6 52.7 93.1  0.031
</code></pre></div></div> <p>*<em>Loss spiked somewhere between 24000-24500 updates and the model failed to recover. Loosely following the practice of <code class="language-plaintext highlighter-rouge">5.1 Training Instability</code> in the PaLM paper <d-cite key="chowdhery2022palm"></d-cite>, we solved the issue by restarting the training from the 20000 updates checkpoint with the PyTorch random seed changed from <code class="language-plaintext highlighter-rouge">1</code> to <code class="language-plaintext highlighter-rouge">2</code>.</em></p> <p>We found that ALiBi no longer helps lowering the validation MLM perplexity. Furthermore, ALiBi turned out to be harmful for several specific GLUE tasks (<code class="language-plaintext highlighter-rouge">CoLA</code>, <code class="language-plaintext highlighter-rouge">MRPC</code>, and <code class="language-plaintext highlighter-rouge">RTE</code>). CLAP head on its own, however, seems to be competitive and in fact outperforms the baseline with <code class="language-plaintext highlighter-rouge">roberta.large</code>.</p> <h2 id="conclusions">Conclusions</h2> <p>This seems to be another case where models with lower perplexity do not necessarily yield higher accuracies for downstream tasks and architectural changes beneficial for models at smaller scales do not imply the same for models at larger scales <d-cite key="tay2022scaling"></d-cite>. CLAP head, however, is simpler than the standard prediction head for MLMs, requires minimal changes, and may be worth trying especially at larger scales.</p> <p>In the broader context, MosaicBERT <d-cite key="portes2023mosaicbert"></d-cite> and LittleBird <d-cite key="lee-etal-2022-littlebird"></d-cite> are most similar to our experiments. In the MosaicBERT paper, Portes et al. also evaluate BERT-style MLMs with symmetric (option 1) encoder-attention ALiBi on the GLUE benchmark and find performance exceeding the BERT baseline within limited training budget. However, these MosaicBERT models were trained with much shorter (128) sequence length and so may have avoided the sequence length regime in which perplexity and performance of certain downstream tasks start to deteriorate <d-footnote>The same can be said about <d-cite key="haviv-etal-2022-transformer"></d-cite>, which also reports in Table 4 the MLM perplexity of RoBERTa large models trained on an excerpt of the Pile with various positional encodings including <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5#issuecomment-1207346198">symmetric (option 1)</a> encoder-attention ALiBi with 128 sequence length.</d-footnote>. The LittleBird architecture is designed for question answering and built with BiALiBi (Bidirectional ALiBi), a variation of option 3 (nonsymmetric with different slopes) where the model not only learned the forward and backward slopes \(m_{fwd}\) and \(m_{bwd}\), but also a special bias value for the attention weight of the global <code class="language-plaintext highlighter-rouge">[CLS]</code> token. Lee et al. evaluate LittleBird models on a collection of QA Benchmarks for both English and Korean and report favorable performance, but leave the question open whether they work well for other NLP tasks. Notably, we also found our ALiBi models capable of matching the baseline performance of the question answering task <code class="language-plaintext highlighter-rouge">QNLI</code>, so the reported performance is compatible with our experiments even without attributing to the other differences in architecture or pretraining task.</p> <p>Finally, what can we say about the original decoder-attention ALiBi and positional encodings in general? The original decoder-attention ALiBi has been shown to help not only perplexity, but also performance on evaluation suites consist of a diverse set of tasks like the EleutherAI Language Model Evaluation Harness <d-cite key="scao2022what"></d-cite>. This discrepancy may be explained by the causal mask, which has been proven to be sufficient for encoding positional information in theory <d-cite key="DBLP:journals/corr/abs-2305-19466"></d-cite><d-footnote>One caveat is that Proof C.1 of <d-cite key="DBLP:journals/corr/abs-2305-19466"></d-cite> for absolute positional encoding depends on distinguishing values of unit fractions 1/t, which eventually fails due to precision limit. For example, 1/1464 can't be distinguished from 1/1465 in float16, well within the context length of interest.</d-footnote>, if not quite matching the performance of models with additional positional encodings in practice <d-cite key="scao2022what"></d-cite><d-cite key="haviv-etal-2022-transformer"></d-cite>. Perhaps we can conclude that</p> <ol> <li>Decoder-attention positional encodings really should be considered causal mask + additional encodings and how they complement each other should be taken into account.</li> <li>Longer context length and certain downstream tasks are more challenging for positional encodings. One worthwhile direction may be to rank their difficulties systematically and iterate on the more challenging circumstances first for future positional encoding designs.</li> </ol> <h2 id="model-checkpoints">Model checkpoints</h2> <p>Final checkpoints for models trained on the Pile:</p> <h3 id="robertabase-1"><code class="language-plaintext highlighter-rouge">roberta.base</code></h3> <p><a href="https://drive.google.com/file/d/1r9VwJCU3AeuivNULRuY3Taq_3AEBg-v5/view?usp=share_link">baseline</a> <a href="https://drive.google.com/file/d/1KmO3FEaawz0tHW-s581NmrkL-OZklLYk/view?usp=share_link">learned-clap</a> <a href="https://drive.google.com/file/d/1s4Tcjnbawq1W6LBcknysj6NdpMfJdek6/view?usp=share_link">alibi</a> <a href="https://drive.google.com/file/d/1PwE_MASg4FinuKq6DX29A8c2lPP2B6nb/view?usp=share_link">zero-clap</a></p> <h3 id="robertalarge-1"><code class="language-plaintext highlighter-rouge">roberta.large</code></h3> <p><a href="https://drive.google.com/file/d/1XSStju8S9y1BCHpXqZ_fZcueH3A0yW2c/view?usp=share_link">baseline</a> <a href="https://drive.google.com/file/d/1UyFxC3XoQ5eAhhXaAUQznLbBLa0J_45U/view?usp=share_link">learned-clap</a> <a href="https://drive.google.com/file/d/1D22xJxJTI4gPAD4gHfKaN1ytjQTy2u_y/view?usp=share_link">alibi</a> <a href="https://drive.google.com/file/d/1ktiRIVqz46DbV261_WxA9RELR971_2iu/view?usp=share_link">zero-clap</a></p> <p>To load them, install <a href="https://github.com/EIFY/fairseq">EIFY/fairseq</a> following <a href="https://github.com/facebookresearch/fairseq/blob/b8ac3fa6cc95f9dc97085232d4faf125e5bcd2e7/README.md#requirements-and-installation">the original instructions</a> and download the GPT-2 fairseq dictionary:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt
</code></pre></div></div> <p>Then all of the checkpoints above except the <code class="language-plaintext highlighter-rouge">zero-clap</code> ones can load as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python
Python 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from fairseq.models.roberta import RobertaModel
&gt;&gt;&gt; roberta = RobertaModel.from_pretrained('/checkpoint-dir', 'learned-clap-large.pt', '/dict-dir')
(...)
&gt;&gt;&gt; roberta.fill_mask('The capital of China is &lt;mask&gt;.', topk=3)
[('The capital of China is Beijing.', 0.7009016871452332, ' Beijing'), ('The capital of China is Shanghai.', 0.23566904664039612, ' Shanghai'), ('The capital of China is Moscow.', 0.010170688852667809, ' Moscow')]
&gt;&gt;&gt;
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">zero-clap</code> ones were trained without the last two <code class="language-plaintext highlighter-rouge">madeupword</code>’s<d-footnote>This is due to the same bug that affected the WikiText-103 variation above and its only visible effect.</d-footnote>, so you need to delete them from <code class="language-plaintext highlighter-rouge">dict.txt</code> before loading, i.e.:</p> <pre>
(...)
50009 0
50256 0
madeupword0000 0
<strike>madeupword0001 0
madeupword0002 0</strike>
</pre> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python
Python 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from fairseq.models.roberta import RobertaModel
&gt;&gt;&gt; roberta = RobertaModel.from_pretrained('/checkpoint-dir', 'zero-clap-large.pt', '/dict-dir')
(...)
&gt;&gt;&gt; roberta.fill_mask('The capital of China is &lt;mask&gt;.', topk=3)
[('The capital of China is Beijing.', 0.7051425576210022, ' Beijing'), ('The capital of China is Shanghai.', 0.21408841013908386, ' Shanghai'), ('The capital of China is Taiwan.', 0.007823833264410496, ' Taiwan')]
&gt;&gt;&gt;
</code></pre></div></div> <p>The rest of the original <a href="https://github.com/facebookresearch/fairseq/blob/b8ac3fa6cc95f9dc97085232d4faf125e5bcd2e7/examples/roberta/README.md#example-usage">example usage</a> should also just work. While these checkpoints have only been tested with this fork, the <code class="language-plaintext highlighter-rouge">baseline</code> ones should also work with the <a href="https://github.com/facebookresearch/fairseq">original fairseq repo</a> with minimum changes to the state dict:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; path = '/checkpoint-dir/baseline-large.pt'
&gt;&gt;&gt; with open(path, 'rb') as f:
...   state = torch.load(f, map_location=torch.device("cpu"))
...
&gt;&gt;&gt;
&gt;&gt;&gt; del state['cfg']['task']['omit_mask']
(...)
&gt;&gt;&gt; torch.save(state, '/checkpoint-dir/compatible.pt')
</code></pre></div></div>]]></content><author><name>Jason Chuan-Chih Chou</name></author><summary type="html"><![CDATA[As a new approach to positional encoding, Attention with Linear Biases (ALiBi) uses linear biases of the attention weights to encode positional information, with capability of context length extrapolation. In their paper however, Press et al. focus on the perplexity of autoregressive decoder-only language models, leaving the question of downstream tasks and its applicability to encoder-attention open. In this blogpost, we attempt to bridge the gap by testing masked language models (MLMs) with encoder-attention ALiBi and prediction head similar to the counterparts of the original ALiBi models. We find that while simplified prediction head may be beneficial, performance of MLMs with encoder-attention ALiBi starts to deteriorate with 2048 sequence length at larger scales. We put our results in the context of related recent experiments and tentatively identify the circumstances more challenging to positional encoding designs. Finally, we open-source our MLMs, with BERT-level performance and 2048 context length.]]></summary></entry><entry><title type="html">How to compute Hessian-vector products?</title><link href="https://iclr-blogposts.github.io/2024/blog/bench-hvp/" rel="alternate" type="text/html" title="How to compute Hessian-vector products?"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/bench-hvp</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/bench-hvp/"><![CDATA[<p>Hessian-vector products (HVPs) play a central role in the study and the use of the geometric property of the loss function of deep neural networks<d-cite key="Foret2021SAM"></d-cite>, as well as in many recent bilevel optimizers<d-cite key="Arbel2022amigo"></d-cite>. However, computing such quantity is often considered prohibitive by practitioners, discouraging them from using algorithms that rely on HVPs.</p> <p>With this blog post, we aim to convince the practitioners that with modern automatic differentiation (AD) frameworks such as <code class="language-plaintext highlighter-rouge">JAX</code> or <code class="language-plaintext highlighter-rouge">PyTorch</code>, HVPs can be efficiently evaluated. Indeed, standard AD theory predicts that the computational cost of an HVP is of the same order as the cost of computing a gradient. After a brief introduction on why HVPs are useful for optimization and ML applications and on the basis of AD, we explain in detail the AD-based methods to compute an HVP and the reason for their efficiency. In particular, we show that one can compute HVPs without explicit Hessian computation. We then compare the different methods to compute HVPs for several deep neural network architectures in terms of time and memory for both <code class="language-plaintext highlighter-rouge">JAX</code> and <code class="language-plaintext highlighter-rouge">PyTorch</code>. Our results illustrate the complexity predicted by the theory, showing that computing an HVP is not much more expensive than computing a gradient. This opens an avenue to develop efficient second-order informed methods for neural networks.</p> <h2 id="what-are-hvps-and-where-are-they-useful">What are HVPs and where are they useful?</h2> <p>Let us first introduce the notion of Hessian and HVP. We will consider in this post a twice differentiable function \(f:\mathbb{R}^d\to\mathbb{R}\) that goes from a vector \(x\) in space \(\mathbb{R}^d\) to a real number in \(\mathbb{R}\). This typically corresponds to a function that maps the value of the parameters \(\theta\) of a neural network to the loss \(f(\theta)\). For such a function, standard AD can be used to efficiently compute the gradient of the loss \(\nabla f(\theta) = \left[ \frac{\partial f}{\partial \theta_i}(\theta)\right]_{1\le i \le d} \in \mathbb{R}^d\), using the backpropagation. The Hessian matrix of \(f\) at \(\theta\) is the matrix of its second-order partial derivatives</p> \[\nabla^2 f(\theta) = \left[\frac{\partial^2f}{\partial \theta_i\partial \theta_j}(\theta)\right]_{1\leq i,j\leq d}\in\mathbb{R}^{d\times d}\enspace.\] <p>This matrix corresponds to the derivative of the gradient and captures how the gradient will change when moving \(x\). To evaluate the variation of the gradient when moving \(\theta\) in the direction \(v\in\mathbb{R}^d\), one can compute the quantity \(\nabla^2 f(\theta) v\in\mathbb{R}^d\). This is the Hessian-vector product (HVP).</p> <p>Let us review some use cases of HVPs in optimization and machine learning.</p> <h3 id="inverse-hessian-vector-products-ihvps-in-optimization">Inverse Hessian-vector products (iHVPs) in optimization</h3> <p>When trying to find the minimum of the function \(f\), methods that account for the second-order information often rely on the product between the inverse Hessian and a vector to find a good update direction. For instance, Newton’s method relies on update rules of the form</p> \[\theta_{k+1} = \theta_k - \eta_k[\nabla^2f(\theta_k)]^{-1}\nabla f(\theta_k)\] <p>for some step-size \(\eta_k&gt;0\).</p> <p>When evaluating the term \([\nabla^2f(\theta_k)]^{-1}\nabla f(\theta_k)\), it would be very inefficient to first compute the full Hessian matrix \(\nabla^2f(\theta_k)\), then invert it and finally multiply this with the gradient \(\nabla f(\theta_k)\). Instead, one computes the inverse Hessian-Vector Product (iHPV) by solving the following linear system</p> <p>\begin{equation}\label{eq:linear_system} \nabla^2f(\theta)v = b\enspace. \end{equation}</p> <p>with \(b = \nabla f(\theta_k)\). This approach is much more efficient as it avoids computing and storing the full Hessian matrix, and only computes the inverse of the matrix in the direction \(v\).</p> <p>A second use case for the iHVP in optimization is with bilevel optimization. In bilevel optimization, one wants to solve the following problem</p> <p>\begin{equation}\label{eq:bilevel_pb} \min_{x\in\mathbb{R}^d} h(x) = F(x, y^* (x))\quad\text{with}\quad y^*(x) = \arg\min_{y\in\mathbb{R}^p} G(x, y)\enspace. \end{equation}</p> <p>The gradient of the function \(h\) can be computed using the implicit function theorem, giving the following expression</p> \[\nabla h(x) = \nabla_x F(x, y^* (x)) - \nabla_{xy}G(x, y^*(x))[\nabla_{yy}G(x, y^*(x))]^{-1}\nabla_y G(x, y^*(x))\enspace.\] <p>Here, the term \(\nabla^2_{yy} G(x, y)\) is the Hessian of the function \(G\) relatively to \(y\). Thus, this quantity also requires computing an iHVP.</p> <p>To compute the iHVP, there are many methods in the literature to solve \eqref{eq:linear_system}, like Neumann iterates<d-cite key="Ghadimi2018"></d-cite><d-cite key="Ji2021stocbio"></d-cite>, the Conjugate Gradient method<d-cite key="Hestenes1952CG"></d-cite><d-cite key="Nocedal2006"></d-cite> or gradient descent steps in the quadratic form \(v\mapsto \frac12\langle\nabla^2f(\theta)v, v\rangle - \langle b, v\rangle\)<d-cite key="Arbel2022amigo"></d-cite><d-cite key="Dagreou2022SABA"></d-cite>. These methods rely on HVPs, as illustrated by the highlighted terms in the Conjugate Gradient method. Thus, an efficient implementation of HVPs is crucial for the overall algorithm performance.</p> <p class="framed"> <b class="underline">Conjugate gradient to solve \eqref{eq:linear_system}</b><br/> <b>Input</b> Initialization \(v_0\)<br/> <b>Initialization</b> $$ r_0 = \textcolor{orange}{\nabla^2f(\theta) v_0} - b,\quad p_0 = -r_0,\quad t = 0 $$ <b>While</b> \(r_t \neq 0\) \begin{align*} \alpha_t &amp;=\frac{r_t^\top r_t}{p_t^\top \textcolor{orange}{\nabla^2f(\theta) p_t}} \\ v_{t+1} &amp;=v_t + \alpha_t p_t \\ r_{t+1} &amp;=r_t + \alpha_t\textcolor{orange}{\nabla^2f(\theta) p_t} \\ \beta_{t+1} &amp;=\frac{r_{t+1}^\top r_{t+1}}{r_t^\top r_t} \\ p_{t+1} &amp;=-r_{t+1} + \beta_{t+1} p_t\\ t &amp;=t + 1 \end{align*} </p> <h3 id="hvps-for-the-study-of-the-loss-landscape">HVPs for the study of the loss landscape</h3> <p>The study of the geometry of neural networks is an active field that aims at understanding the links between training dynamics, local geometry of the training loss and generalization<d-cite key="Keskar2017"></d-cite>. One way to study the local geometry of a neural network is to find the distribution of the eigenvalues of its Hessian matrix. Indeed, depending on the sign of the eigenvalues of the Hessian, one can for instance distinguish local minima, local maxima and saddle points. As an illustration, the following figure shows how the sign of the eigenvalues of the Hessian matrix of a function affects the shape of the function’s landscape around a stationary point.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/hess_eig-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/hess_eig-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/hess_eig-1400.webp"/> <img src="/2024/assets/img/2024-05-07-bench-hvp/hess_eig.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In several papers<d-cite key="Ghorbani2019"></d-cite><d-cite key="Dauphin2014"></d-cite><d-cite key="Foret2021SAM"></d-cite>, an approximation of the Hessian spectrum is computed thanks to the Lanczos algorithm<d-cite key="Lanczos1950"></d-cite>. This algorithm is a modification of the power method where each new iterate is taken in the orthogonal complement of the previous iterates. It outputs a factorization of the Hessian of the form $\nabla^2 f(\theta) = VTV^\top$ where \(V=(v_0,...,v_{k-1})\) is orthogonal and</p> \[T = \begin{pmatrix} \alpha_0&amp; \beta_1 &amp; 0 &amp; \cdots &amp; 0\\ \beta_1 &amp; \alpha_1 &amp; \beta_2 &amp; \ddots &amp; \vdots\\ 0 &amp; \beta_2 &amp; \alpha_2 &amp; \ddots &amp; 0\\ \vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \beta_{k-1}\\ 0 &amp; \cdots &amp; 0 &amp; \beta_{k-1} &amp; \alpha_{k-1} \end{pmatrix}\enspace.\] <p class="framed"> <b class="underline">Lanczos' algorithm</b><br/> <b>Input</b> Initial vector \(v_0\).<br/> <b>Initialization</b> $$ w'_0 = \textcolor{orange}{\nabla^2f(\theta)v_0},\quad \alpha_0 = w_0'^\top v_0,\quad w_0 = w_0' - \alpha_0 v_0 $$ <b>For</b> \(i = 1,\dots, k-1\):<br/> \begin{align*} \beta_i &amp;= \|w_{i-1}\|\\ v_{i} &amp;= \frac{w_{i-1}}{\beta_{i}}\\ w_i' &amp;= \textcolor{orange}{\nabla^2f(\theta)v_i}\\ \alpha_i &amp;= w_i'^\top v_i\\ w_i &amp;= w_i' - \alpha_i v_i - \beta_iv_{i-1} \end{align*} </p> <p>We observe once again that the Hessian information is accessed through HVPs rather than the full Hessian matrix itself.</p> <h2 id="a-quick-detour-by-automatic-differentiation">A quick detour by automatic differentiation</h2> <p>Automatic differentiation (AD) is an important tool to compute exactly the derivatives of differentiable functions obtained as the composition of simple operations. There are two modes in AD; the forward mode that computes Jacobian-vector products (JVPs) and the reverse mode that computes vector-Jacobian products (VJPs). Since the gradient of a scalar function is a special case of the VJP, the reverse mode is the most frequently used in machine learning. It is typically used to compute the gradients of deep learning cost functions, where it is called <em>backpropagation</em><d-cite key="Rumelhart1986"></d-cite>.</p> <p>In what follows, we briefly present the notion of computational graph and the two AD modes. For a more detailed explanation, we refer the reader to the excellent survey by Baydin et al.<d-cite key="Baydin2018"></d-cite>.</p> <h3 id="computational-graph">Computational graph</h3> <p>A key ingredient of AD is a computational graph associated with the code that evaluates a function. It is a directed acyclic graph that represents the succession of elementary operations required the evaluate a function.<br/> Simple computational graph of a function \(f:\mathbb{R}^d\to\mathbb{R}^p\) are typically</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/direct_graph-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/direct_graph-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/direct_graph-1400.webp"/> <img src="/2024/assets/img/2024-05-07-bench-hvp/direct_graph.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In this graph, the vertices \(z_i\in\mathbb{R}^{m_i}\) represent the intermediate states of the evaluation of \(f\). To get the vertex \(z_i\), we use the values of its parents in the graph \(z_{i-1}\), with simple transfer functions \(z_i(z_{i-1})\). The computational complexity of the function evaluation depends on the complexity of the considered graph, as one node might have more than one parent. The memory footprint of the evaluation of the function is also linked to the maximum number of parents that can have a vertex in the computational graph, as their value needs to be stored until all children nodes have been computed.</p> <p>Let us take an example with a multilayer linear perceptron (MLP) with 2 layers. The function \(f_x:\mathbb{R}^h\times \mathbb{R}^{h\times p}\to \mathbb{R}\) is defined for an input \(x\in\mathbb{R}^p\) by</p> <p>\begin{equation}\label{eq:mlp} f_x(U, W) = \frac12(UWx)^2\enspace. \end{equation}</p> <p>Here, the input \(\theta\) corresponds to the parameters of the network \((U, V)\) and the intermediate steps are \(z_1 = Wx\), \(z_2 = Uz_1\) and \(z_3 = \frac12 z_2^2\). A possible computational graph to get \(f_x(U, W)\) is the following</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/computational_graph-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/computational_graph-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/computational_graph-1400.webp"/> <img src="/2024/assets/img/2024-05-07-bench-hvp/computational_graph.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>and the associated Python code to compute \(f_x\) is</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">z3</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">z2</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">z3</span>
</code></pre></div></div> <p>Here, the feed-forward structure of the function makes the computational graph very simple, as each node has a single intermediate result parent.</p> <p>AD uses this computational graph to compute the function’s derivatives. Using the chain rule, the Jacobian \(\frac{\partial f}{\partial \theta}(\theta)\) of \(f\) is obtained as a product of the Jacobian of the intermediate states \(z_1, \dots, z_n\). \begin{equation}\label{eq:chain_rule} \underbrace{\frac{\partial f}{\partial \theta}(\theta)}_{p\times d} = \frac{\partial z_n}{\partial \theta} =\frac{\partial z_n}{\partial z_1}\frac{\partial z_1}{\partial \theta}=\cdots = \underbrace{\frac{\partial z_n}{\partial z_{n-1}}}_{p\times m_{n-1}}\underbrace{\frac{\partial z_{n-1}}{\partial z_{n-2}}}_{m_{n-1}\times m_{n-2}}\cdots\underbrace{\frac{\partial z_1}{\partial \theta}}_{m_1\times d}\enspace. \end{equation} Depending on the order of the multiplication, one can compute the derivative of \(f\) with respect to \(\theta\) in two ways: the forward mode and the reverse mode.</p> <h3 id="forward-mode">Forward mode</h3> <p>For a vector $v\in\mathbb{R}^d$, the Jacobian-vector product (JVP) corresponds to the directional derative of $f$ in the direction $v$. It can be computed by the forward mode AD</p> <p>\begin{equation}\label{eq:chain_rule_jvp} \frac{\partial f}{\partial \theta}(\theta)\times v = \frac{\partial z_n}{\partial z_{n-1}}\frac{\partial z_{n-1}}{\partial z_{n-2}}\cdots\frac{\partial z_1}{\partial \theta}v\enspace. \end{equation}</p> <p>It consists in doing the multiplications in \eqref{eq:chain_rule_jvp} from the right to the left. It is a forward pass in the computational graph where we propagate at the same time the states \(z_i\) and the partial derivatives \(\frac{\partial z_{i+1}}{\partial z_i}\). If \(f\) is real-valued, the \(i\)th coordinate of its gradient is exactly given by product of the Jacobian of \(f\) and the \(i\)th canonical basis vector \(e_i\) since \begin{equation} \frac{\partial f}{\partial \theta_i}(\theta) = \lim_{t\to 0}\frac{f(\theta+te_i)-f(\theta)}{t}\enspace. \end{equation} Thus, we can get its gradient by computing each of the \(d\) JVPs \(\left(\frac{\partial f}{\partial \theta_i}(\theta)\times e_i\right)_{1\leq i \leq d}\) with forward AD.</p> <p>To understand properly what is happening when using forward differentiation, let us go back to the linear MLP defined in \eqref{eq:mlp}. If we implement ourselves the forward differentiation to get the JVP, we obtain the following code</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">v_u</span><span class="p">,</span> <span class="n">v_w</span><span class="p">):</span>
    <span class="c1"># Forward diff of f
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">v_z1</span> <span class="o">=</span> <span class="n">v_w</span> <span class="o">@</span> <span class="n">x</span>  <span class="c1"># Directional derivative of W -&gt; W @ x in the direction v_w
</span>  
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">v_z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">v_z1</span> <span class="o">+</span> <span class="n">v_u</span> <span class="o">@</span> <span class="n">z1</span>  <span class="c1">#  Directional derivative of (U, z_1) -&gt; z2 in the direction (v_u, v_z1)
</span>  
    <span class="n">v_z3</span> <span class="o">=</span> <span class="n">v_z2</span> <span class="o">@</span> <span class="n">z2</span>  <span class="c1"># Directional derivative of z2 -&gt; .5*z2**2 in the direction v_z2 
</span>    <span class="k">return</span> <span class="n">v_z3</span>
</code></pre></div></div> <p>In comparison with the code of the evaluation of \(f_x\), there are two more operations corresponding to the computation of the dual variables <code class="language-plaintext highlighter-rouge">v_z1</code> and <code class="language-plaintext highlighter-rouge">v_z2</code>. In terms of memory, if we consider the computation of the JVP as coded in the previous snippet, the maximum number of parents of a vertex is four. This maximum is achieved by the vertex <code class="language-plaintext highlighter-rouge">v_z2</code> which has the vertices <code class="language-plaintext highlighter-rouge">U</code>, <code class="language-plaintext highlighter-rouge">v_z1</code>, <code class="language-plaintext highlighter-rouge">v_u</code> and <code class="language-plaintext highlighter-rouge">z1</code> as parents.</p> <p>In <code class="language-plaintext highlighter-rouge">JAX</code>, we get the JVP of a function \(f\) in the direction \(v\) with <code class="language-plaintext highlighter-rouge">jax.jvp(f, (params, ), (v, ))[1]</code>.</p> <h3 id="reverse-mode">Reverse mode</h3> <p>The reverse mode is also known as backpropagation in the context of deep learing. For $u\in\mathbb{R}^p$, it aims at computing VJPs</p> <p>\begin{equation}\label{eq:chain_rule_vjp} u^\top\frac{\partial f}{\partial \theta}(\theta) = u^\top\frac{\partial z_n}{\partial z_{n-1}}\frac{\partial z_{n-1}}{\partial z_{n-2}}\cdots\frac{\partial z_1}{\partial \theta}\enspace. \end{equation}</p> <p>In the reverse AD, the multiplications of \eqref{eq:chain_rule_jvp} are done from the left to the right. It requires doing one forward pass in the computational graph to compute the intermediate states \(z_i\) and then a backward pass to propagate the successive partial derivatives from the left to the right. Contrary to the forward mode, it has a more important memory footprint. Indeed, it requires storing the values of all the states. For instance, to compute the last term \(\frac{\partial z_3}{\partial z_2}\), one needs the value of \(z_2\) which was the first computed during the forward pass. If \(f\) is real-valued, \(u\) is a scalar and the VJP is the multiplication of the gradient of \(f\) by \(u\). Thus, one can get the gradient on \(f\) by using \(u=1\) and performing only one reverse differentiation. This makes this mode more efficient in computing gradients.</p> <p>Let us observe what happens if we code manually the backpropagation to get the gradient of the previous function \(f_x\) defined by \(f_x(U, W) = \frac12(UW x)^2\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="c1"># Forward pass
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">z3</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">z2</span><span class="o">**</span><span class="mi">2</span>

    <span class="c1"># Reverse pass
</span>    <span class="c1">## Transfer function: z3 = 0.5 * z2**2
</span>    <span class="n">dz2</span> <span class="o">=</span> <span class="n">z2</span>  <span class="c1"># derivative of z3 wrt z2
</span>  
    <span class="c1">## Transfer function: z2 = U @ z1
</span>    <span class="n">dU</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span>  <span class="c1"># derivative of z3 wrt U
</span>    <span class="n">dz1</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz2</span>  <span class="c1"># derivative of z3 wrt z1
</span>  
    <span class="c1">## Transfer function: z1 = W @ x
</span>    <span class="n">dW</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>   <span class="c1"># derivative of z3 wrt W
</span>    
    <span class="k">return</span> <span class="n">dU</span><span class="p">,</span> <span class="n">dW</span>
</code></pre></div></div> <p>This function returns the gradient of \(f_x\). At reading this code, we understand one needs to store all the intermediate values of the forward pass in the graph. Indeed, if we look at the case of <code class="language-plaintext highlighter-rouge">z1</code> which is the first node computed, it is used four steps later for the computation of <code class="language-plaintext highlighter-rouge">dU</code>.</p> <p>To get the gradient in JAX, one can use <code class="language-plaintext highlighter-rouge">jax.grad(f)(params)</code>.</p> <h2 id="naive-computation-of-hvps">Naive computation of HVPs</h2> <p>Since we are interested in computing \(\nabla^2 f(\theta)v\), the simplest way to do it is to compute the Hessian matrix and then multiply it by the vector \(v\). This can be achieved in <code class="language-plaintext highlighter-rouge">JAX</code> by calling <code class="language-plaintext highlighter-rouge">jax.hessian(f)(params) @ v</code>.</p> <p>This method is quite cumbersome making it impossible to use for deep neural networks. Indeed, the storage of the full Hessian matrix has \(\mathcal{O}(d^2)\) complexity where \(d\) is the dimension of the model’s parameters set.</p> <p>The good news is that we can compute HVP without computing the Hessian thanks to clever use of AD.</p> <h2 id="hvps-without-explicit-hessian-computation">HVPs without explicit Hessian computation</h2> <p>In 1994, Pearlmutter<d-cite key="Pearlmutter1994"></d-cite> proposed to leverage the following observation to compute HVP efficiently: the HVP is also the directional derivative of the gradient in the direction \(v\):</p> \[\nabla^2f(\theta) v = \lim_{\epsilon\to 0} \frac1\epsilon[\nabla f(\theta+\epsilon v)-\nabla f(\theta)] = \nabla [\langle \nabla f(.), v\rangle](\theta)\enspace.\] <p>Based on this identity, AD enables to compute HVPs in three ways, as described in the <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">JAX documentation</a>.</p> <h3 id="forward-over-reverse">Forward-over-reverse</h3> <p>The forward-over-reverse mode consists in doing forward differentiation in a computational graph of the gradient of \(f\).</p> <p>Its implementation in <code class="language-plaintext highlighter-rouge">JAX</code> is only two lines of code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hvp_forward_over_reverse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jvp</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">))[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <p>In this case, <code class="language-plaintext highlighter-rouge">jax.grad(f)(params)</code> is computed by backward AD, whose complexity is two times the complexity of evaluating \(f\). Thus, the temporal complexity of <code class="language-plaintext highlighter-rouge">hvp_forward_over_reverse</code> is roughly four times the complexity of the evaluation of \(f\).</p> <p>To better see what happens, let us consider again our function \(f_x\) defined by \eqref{eq:mlp}. The Python code of the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> HVP is the following.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_over_reverse</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">v_U</span><span class="p">,</span> <span class="n">v_W</span><span class="p">):</span>
    <span class="c1"># Forward through the forward pass through f
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">v_z1</span> <span class="o">=</span> <span class="n">v_W</span> <span class="o">@</span> <span class="n">x</span>
  
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">v_z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">v_z1</span> <span class="o">+</span> <span class="n">v_U</span> <span class="o">@</span> <span class="n">z1</span>
    
    <span class="c1"># z3 = 0.5 * z2**2
</span>    <span class="c1"># Forward through the backward pass through f
</span>    <span class="n">z4</span> <span class="o">=</span> <span class="n">z2</span>  <span class="c1"># dz2
</span>    <span class="n">v_z4</span> <span class="o">=</span> <span class="n">v_z2</span>  <span class="c1"># v_dz2
</span>  
    <span class="n">z5</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z4</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span>  <span class="c1"># dU
</span>    <span class="n">v_z5</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">v_z4</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z4</span><span class="p">,</span> <span class="n">v_z1</span><span class="p">)</span>  <span class="c1"># v_dU
</span>  
    <span class="n">z6</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z4</span>  <span class="c1"># dz1
</span>    <span class="n">v_z6</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">v_z4</span> <span class="o">+</span> <span class="n">v_U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z4</span>  <span class="c1"># v_dz1
</span>  
    <span class="n">z7</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z6</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># dW
</span>    <span class="n">v_z7</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">v_z6</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># v_dW
</span>  
    <span class="k">return</span> <span class="n">v_z5</span><span class="p">,</span> <span class="n">v_z7</span>  <span class="c1"># v_dU, v_dW
</span></code></pre></div></div> <p>The take-home message of this part is that, after computing the gradient of \(f_x\), one can consider a computational graph of this gradient and perform forward differentiation through this new computational graph. Here, the variables <code class="language-plaintext highlighter-rouge">z1</code>,…, <code class="language-plaintext highlighter-rouge">z7</code> are the vertices of a computational graph of the gradient of \(f_x\). The nice thing is that this mode enables getting at the same time the gradient and the HVP. Indeed, in the previous snippet, <code class="language-plaintext highlighter-rouge">z5</code> and <code class="language-plaintext highlighter-rouge">z7</code> are the components of the gradient of \(f_x\) which could be also returned if needed. This feature can be useful in bilevel optimization for instance.</p> <h3 id="reverse-over-reverse">Reverse-over-reverse</h3> <p>Instead of doing forward differentiation of the gradient, one can multiply the gradient by \(v\) and thus get a scalar. We can then backpropagate into this scalar product. This is the reverse-over-reverse mode.</p> <p>It can be implemented by these lines of code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hvp_reverse_over_reverse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">vdot</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">y</span><span class="p">),</span> <span class="n">v</span><span class="p">))(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div> <p>Since the gradients are computed by backpropagation, the complexity of <code class="language-plaintext highlighter-rouge">hvp_reverse_over_reverse</code> is twice the complexity of <code class="language-plaintext highlighter-rouge">jax.grad(f)</code>, which is roughly four times the complexity of the evaluation of \(f\).</p> <p>Writting down the code of the reverse-over-reverse HVP for our function \(f_x\) defined by \eqref{eq:mlp} makes us understand the differences between this mode and the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> mode. Particularly, one can notice that there are more elementary operations in the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> mode than in the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> mode. Moreover, in terms of memory footprint, the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> requires storing the values of the vertices of the computational graph of the gradient of \(f_x\), while the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> only needs to store the values of the vertices of the computational graph of \(f_x\). Thus, the former is less efficient than the latter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reverse_over_reverse</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">v_u</span><span class="p">,</span> <span class="n">v_w</span><span class="p">):</span>
    <span class="c1"># Forward through &lt;grad(f), v&gt;
</span>    <span class="c1">## Forward through f
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">z3</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
  
    <span class="c1">## Reverse through f
</span>    <span class="n">z4</span> <span class="o">=</span> <span class="n">z2</span>  <span class="c1"># dz2
</span>    <span class="n">z4</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z3</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> <span class="c1"># dU
</span>    <span class="n">z5</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z3</span> <span class="c1"># dz1
</span>    <span class="n">z6</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z5</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># dW
</span>  
    <span class="c1"># Output: dot product &lt;grad(f), v&gt;
</span>    <span class="n">z7</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">z4</span> <span class="o">*</span> <span class="n">v_u</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">z6</span> <span class="o">*</span> <span class="n">v_w</span><span class="p">)</span>
  
    <span class="c1"># Backward through z7 = &lt;grad(f),v&gt;
</span>    <span class="c1">## z7 = jnp.sum(z4 * v_u) + jnp.sum(z6 * v_w)
</span>    <span class="n">dz6</span> <span class="o">=</span> <span class="n">v_w</span>
    <span class="n">dz4</span> <span class="o">=</span> <span class="n">v_u</span>
  
    <span class="c1">## z6 = jnp.outer(z5, x)
</span>    <span class="n">dz5</span> <span class="o">=</span> <span class="n">dz6</span> <span class="o">@</span> <span class="n">x</span>
  
    <span class="c1">## z5 = U.T @ z3
</span>    <span class="n">dz3</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">dz5</span>
    <span class="n">ddU</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">z3</span><span class="p">,</span> <span class="n">dz5</span><span class="p">)</span>  <span class="c1"># Derivative of z7 wrt U
</span>  
    <span class="c1">## z4 = jnp.outer(z3, z1)
</span>    <span class="n">dz3</span> <span class="o">+=</span> <span class="n">dz4</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">dz1</span> <span class="o">=</span> <span class="n">dz4</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z3</span>
  
    <span class="c1">## z3 = z2
</span>    <span class="n">dz2</span> <span class="o">=</span> <span class="n">dz3</span>
  
    <span class="c1">## z2 = U @ z1
</span>    <span class="n">dz1</span> <span class="o">+=</span> <span class="n">dz2</span> <span class="o">*</span> <span class="n">U</span>
    <span class="c1"># As U appears multiple times in the graph, we sum its contributions
</span>    <span class="n">ddU</span> <span class="o">+=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> 
  
    <span class="c1">## z1 = W @ x
</span>    <span class="n">ddW</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># Derivative of z7 wrt W
</span>  
    <span class="k">return</span> <span class="n">ddU</span><span class="p">,</span> <span class="n">ddW</span>
</code></pre></div></div> <h3 id="reverse-over-forward">Reverse-over-forward</h3> <p>What about doing forward differentiation of \(f\) rather than reverse propagation? This is what is done in the reverse-over-forward mode. It consists in backpropagating in the computational graph of the JVP of \(f\) and \(v\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hvp_reverse_over_forward</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="n">jvp_fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="nf">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">),</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">))[</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">jvp_fun</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div> <p>This method is more efficient than the previous one. Indeed, since we backpropagate only once, the memory burden is lower than for the <code class="language-plaintext highlighter-rouge">reverse_over_reverse</code> fashion. In comparison with <code class="language-plaintext highlighter-rouge">forward-over-reverse</code>, the complexity is the same. However, one can notice that the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> enables computing at the same time the gradient of \(f\) and the HVP, which is not the case for the <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> mode.</p> <p>The code of the <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> HVP for the MLP \(f_x\) defined by \eqref{eq:mlp} is the following.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reverse_over_forward</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">v_U</span><span class="p">,</span> <span class="n">v_W</span><span class="p">):</span>
    <span class="c1"># Forward diff of f to  &lt;grad(f), v&gt;
</span>    <span class="n">z1</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">z6</span> <span class="o">=</span> <span class="n">v_W</span> <span class="o">@</span> <span class="n">x</span>  <span class="c1"># v_z1
</span>  
    <span class="n">z2</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z1</span>
    <span class="n">z5</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">z6</span> <span class="o">+</span> <span class="n">v_U</span> <span class="o">@</span> <span class="n">z1</span>  <span class="c1"># v_z2
</span>  
    <span class="c1"># output &lt;grad(f), v&gt;
</span>    <span class="n">z4</span> <span class="o">=</span> <span class="n">z5</span> <span class="o">@</span> <span class="n">z2</span>  <span class="c1"># v_z3
</span>  
    <span class="c1"># Backward pass through &lt;grad(f), v&gt;
</span>    <span class="c1">## z4 = z5 @ z2
</span>    <span class="n">dz2</span> <span class="o">=</span> <span class="n">z5</span>
    <span class="n">dz5</span> <span class="o">=</span> <span class="n">z2</span>  <span class="c1"># dv_z2
</span>  
    <span class="c1">## z5 = U @ z6 + v_U @ z1
</span>    <span class="n">dz1</span> <span class="o">=</span> <span class="n">v_U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz5</span>
    <span class="n">dz6</span> <span class="o">=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz5</span>  <span class="c1"># dv_z1
</span>    <span class="n">ddU</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz5</span><span class="p">,</span> <span class="n">z6</span><span class="p">)</span>  <span class="c1"># derivative of z4 wrt U
</span>  
    <span class="c1">## z2 = U @ z1
</span>    <span class="c1"># As U and dz1 appear multiple times, we sum their contributions
</span>    <span class="n">dz1</span> <span class="o">+=</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz2</span>
    <span class="n">ddU</span> <span class="o">+=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span>
    
    <span class="c1">## z1 = W @ x
</span>    <span class="n">ddW</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ddU</span><span class="p">,</span> <span class="n">ddW</span>
</code></pre></div></div> <h2 id="benchmark-with-deep-learning-architectures">Benchmark with deep learning architectures</h2> <p>While these three methods compute the same outputs, the different ways of traversing the computational graph change their overall time and memory complexities. We now compare the computation of HVPs with these three methods for various deep-learning architectures. To cover a broad range of use cases, we consider a residual network (<a href="https://huggingface.co/docs/transformers/model_doc/resnet">ResNet34</a><d-cite key="He2015resnet"></d-cite>) and a transformer-based architecture (<a href="https://huggingface.co/docs/transformers/model_doc/vit">ViT-base</a><d-cite key="Dosovitskiy2021"></d-cite>) for image classification as well as a transformer for natural language processing (<a href="https://huggingface.co/docs/transformers/model_doc/bert#transformers.FlaxBertForTokenClassification">Bert-base</a>.<d-cite key="Devlin2019"></d-cite>). We use the <code class="language-plaintext highlighter-rouge">Flax</code> and <code class="language-plaintext highlighter-rouge">PyTorch</code> implementations of these architectures available in the <a href="https://huggingface.co/docs/transformers/">transformers package</a> provided by <a href="https://huggingface.co">Hugging Face 🤗</a>.</p> <p>All computations were run on an Nvidia A100 GPU with 40 GB of memory. We used the version 0.4.21. of <code class="language-plaintext highlighter-rouge">Jax</code> and the version 2.1.1. of <code class="language-plaintext highlighter-rouge">torch</code>.</p> <p>The code of the benchmark is available on <a href="https://github.com/MatDag/bench_hvp/">this repo</a>.</p> <h3 id="time-complexity">Time complexity</h3> <p>The first comparison we make is a comparison in terms of wall-clock time between the different ways to compute HVPs and also the computation of a gradient by backpropagation. For each architecture, we compute the gradient of the model with respect to the parameters by backpropagation. We also compute the HVPs in <code class="language-plaintext highlighter-rouge">forward-over-reverse</code>, <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> and <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> modes. For each computation, we measure the time taken. Specifically for the HVPs, we subtract the time taken by a gradient computation, to get only the time of the overhead required by the HVP computation. The inputs for each architecture are generated randomly. For the ResNet34 architecture, we generated a batch of images of size 224x224x3. To limit out-of-memory issues in the experiments, we generated for the ViT architecture images of size 96x96x3. For the BERT architecture, we generated a batch of sequences of length 32.</p> <p>We first use <code class="language-plaintext highlighter-rouge">JAX</code> with just-in-time compilation. Each computation is run 90 times. We plot on the left of the figure, the median computation time and also the 20% and 80% percentile in black. The computations are done with a batch size of 128. We observe that, in practice, the overhead over the gradient computation for the HVP computation is between one and twice the time of a gradient computation for the three architectures. Consequently, a whole HVP computation takes between twice and three times the time of a gradient calculation. This is consistent with the theory. One can notice that the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> is slightly slower than the others in all the cases. The <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> and <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> are, as for them, very close in terms of time.</p> <p>We also report on the right figure the computational time of each method with respect to the batch size for the ResNet34 architecture. We observe, as expected, that the computational time scales linearly with the batch size.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax-1400.webp"/> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We run a similar experiment with the functional API available in <code class="language-plaintext highlighter-rouge">PyTorch</code> <a href="https://pytorch.org/docs/stable/func.html"><code class="language-plaintext highlighter-rouge">torch.func</code></a> similar to the one <code class="language-plaintext highlighter-rouge">JAX</code> has. The results we get are more contrasted.</p> <p>In the case of ResNet34, the scaling between the different methods is similar to the one we get with <code class="language-plaintext highlighter-rouge">JAX</code>. Also, during our experiments, we figured out that batch normalization made the forward computation slow and induced out-of-memory issues. Thus, we removed the batch normalization layers from the ResNet34 architecture.</p> <p>For ViT and BERT, the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> is surprisingly longer than the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> method. Moreover, the scaling between the gradient and HVP computational time differs from the one we get with <code class="language-plaintext highlighter-rouge">JAX</code>. Indeed, for these architectures, the HVP computations take between four and five more time than the gradient computations. This is a discrepancy with what we would expect in theory. This might be because, at the time we are writing this blog post, the functional API of <code class="language-plaintext highlighter-rouge">PyTorch</code> is still in its early stages. Particularly, we could not use the compilation with <code class="language-plaintext highlighter-rouge">torch.compile</code> because it does not work with some operators of <code class="language-plaintext highlighter-rouge">torch.func</code> such as <code class="language-plaintext highlighter-rouge">torch.func.jvp</code>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch-1400.webp"/> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="memory-complexity">Memory complexity</h3> <p>We also compare the memory footprint of each approach. The following figure provides the results we get with jax jitted code. On the left, we represent the result for each method and model with a batch size of 64. On the right, we show the evolution of the memory footprint of each method for the ResNet34 with the batch size. Surprisingly, we could observe that the memory footprint of the different methods to compute HVPs does not vary for a given model. This is counterintuitive since we expect that the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> method have a larger memory footprint due to the double backpropagation.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax-1400.webp"/> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>However, we do the same experiment by <em>disabling the JIT compilation</em>. The result we get corroborates the theory. Indeed, one can observe in the following figure that the memory footprint of the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> method is larger than the one of the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> and <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> methods. This is because the <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> involves two successive backward differentiations while the other two involve only one reverse differentiation. Moreover, it scales linearly with the batch size, which was not the case in the previous figure in the small batch size regime.</p> <p>In light of these two results, the clever memory allocation performed during just-in-time compilation reduces significantly the memory footprint of the HVP computations.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit-1400.webp"/> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In the following figure, we plot the results we get with the <code class="language-plaintext highlighter-rouge">PyTorch</code> implementation. One can observe that in all the cases the <code class="language-plaintext highlighter-rouge">forward-over-reverse</code> consumes more memory in comparison with the <code class="language-plaintext highlighter-rouge">reverse-over-forward</code> mode. It is almost at the same level as <code class="language-plaintext highlighter-rouge">reverse-over-reverse</code> mode, which is quite unexpected.</p> <p>The right plot of the evolution of the memory footprint with the batch size for the ResNet34 architecture evolves linearly as expected.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch-1400.webp"/> <img src="/2024/assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we have explored the different ways to compute HVP from theoretical and practical perspectives. The three take-home messages to keep in mind are the following:</p> <ul> <li> <p>We can compute HVPs without computing Hessian matrices.</p> </li> <li> <p>In practice, computing an HVP takes between twice and four times the time taken by a gradient computation and requires two to three times more memory than computing a gradient.</p> </li> <li> <p>The AD framework and the use or not of the just-in-time compilation affects the practical performances of HVPs computations in time and memory.</p> </li> </ul>]]></content><author><name>Mathieu Dagréou</name></author><summary type="html"><![CDATA[The product between the Hessian of a function and a vector, the Hessian-vector product (HVP), is a fundamental quantity to study the variation of a function. It is ubiquitous in traditional optimization and machine learning. However, the computation of HVPs is often considered prohibitive in the context of deep learning, driving practitioners to use proxy quantities to evaluate the loss geometry. Standard automatic differentiation theory predicts that the computational complexity of an HVP is of the same order of magnitude as the complexity of computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, JAX and PyTorch, allow for efficient computation of these HVPs in standard deep learning cost functions.]]></summary></entry><entry><title type="html">On Bayesian Model Selection: The Marginal Likelihood, Cross-Validation, and Conditional Log Marginal Likelihood</title><link href="https://iclr-blogposts.github.io/2024/blog/clml/" rel="alternate" type="text/html" title="On Bayesian Model Selection: The Marginal Likelihood, Cross-Validation, and Conditional Log Marginal Likelihood"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/clml</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/clml/"><![CDATA[<div style="display: none;"> $$\require{mathtools} \DeclareMathOperator{\opExpectation}{\mathbb{E}} \newcommand{\E}[2]{\opExpectation_{#1} \left [ #2 \right ]} \newcommand{\simpleE}[1]{\opExpectation_{#1}} \newcommand{\MidSymbol}[1][]{\:#1\:} \newcommand{\given}{\MidSymbol[\vert]} \DeclareMathOperator{\opmus}{\mu^*} \newcommand{\IMof}[1]{\opmus[#1]} \DeclareMathOperator{\opInformationContent}{H} \newcommand{\ICof}[1]{\opInformationContent[#1]} \newcommand{\xICof}[1]{\opInformationContent(#1)} \DeclareMathOperator{\opEntropy}{H} \newcommand{\Hof}[1]{\opEntropy[#1]} \newcommand{\xHof}[1]{\opEntropy(#1)} \DeclareMathOperator{\opMI}{I} \newcommand{\MIof}[1]{\opMI[#1]} \DeclareMathOperator{\opTC}{TC} \newcommand{\TCof}[1]{\opTC[#1]} \newcommand{\CrossEntropy}[2]{\opEntropy(#1 \MidSymbol[\Vert] #2)} \newcommand{\iCrossEntropy}[3]{\opEntropy_{#1 \Vert #2}[#3]} \DeclareMathOperator{\opKale}{D_\mathrm{KL}} \newcommand{\Kale}[2]{\opKale(#1 \MidSymbol[\Vert] #2)} \newcommand{\iKale}[3]{\opKale_{,\, #1 \Vert #2}[#3]} \DeclareMathOperator{\opJSD}{D_\mathrm{JSD}} \newcommand{\JSD}[2]{\opJSD(#1 \MidSymbol[\Vert] #2)} \DeclareMathOperator{\opp}{p} \newcommand{\pof}[1]{\opp(#1)} \newcommand{\hpof}[1]{\hat{\opp}(#1)} \newcommand{\pcof}[2]{\opp_{#1}(#2)} \newcommand{\hpcof}[2]{\hat\opp_{#1}(#2)} \DeclareMathOperator{\opq}{q} \newcommand{\qof}[1]{\opq(#1)} \newcommand{\hqof}[1]{\hat{\opq}(#1)} \newcommand{\qcof}[2]{\opq_{#1}(#2)} \newcommand{\varHof}[2]{\opEntropy_{#1}[#2]} \newcommand{\xvarHof}[2]{\opEntropy_{#1}(#2)} \newcommand{\varMIof}[2]{\opMI_{#1}[#2]} \newcommand{\w}{\boldsymbol{\theta}} \newcommand{\W}{\boldsymbol{\Theta}} \newcommand{\h}{\boldsymbol{\phi}} \newcommand{\hopt}{\boldsymbol{\h^\star}} \newcommand{\H}{\boldsymbol{\Phi}} \DeclareMathOperator{\opf}{f} \newcommand{\fof}[1]{\opf(#1)} \newcommand{\xset}[3]{(\x_n^{#1})_{n=#2}^{#3}} \newcommand{\xNset}{(\x_n)_{n=1}^N} \newcommand{\XNtuple}{(\X_n)_{n=1}^N} \newcommand{\xNtuple}{(\x_n)_{n=1}^N} \newcommand{\XNset}{\{\X_n\}_{n=1}^N} \newcommand{\xNset}{\{\x_n\}_{n=1}^N} \newcommand{\XNsetk}{\{\X_n\}_{n=N-k+1}^N} \newcommand{\xNsetk}{\{\x_n\}_{n=N-k+1}^N} \newcommand{\XNkset}{\{\X_n\}_{n=1}^{N-k}} \newcommand{\xNkset}{\{\x_n\}_{n=1}^{N-k}} \newcommand{\XNoset}{\{\X_n\}_{n=1}^{N-1}} \newcommand{\y}{y} \newcommand{\Y}{Y} \newcommand{\L}{\boldsymbol{L}} \newcommand{\x}{\boldsymbol{x}} \newcommand{\X}{\boldsymbol{X}} \newcommand{\oppdata}{\hat{\opp}_{\text{data}}} \newcommand{\pdata}[1]{\hpcof{\text{data}}{#1}} \newcommand{\normaldist}[1]{\mathcal{N}(#1)} \newcommand{\wstddev}{\sigma_\w} \newcommand{\noisestddev}{\sigma_\text{noise}} \newcommand{\Dataset}{\mathcal{D}} \newcommand{\Dtrain}{\Dataset_{\text{train}}} \newcommand{\Dval}{\Dataset_{\text{val}}} $$ </div> <h2 id="introduction">Introduction</h2> <p>Model selection is a crucial aspect of machine learning, as it allows us to choose the most appropriate model for a given task. In the Bayesian setting, the marginal likelihood has been a popular tool for model selection and hyperparameter learning, often motivated by the principle of Occam’s razor. However, the suitability of the marginal likelihood depends on the specific context and goals of the modeling task.</p> <p>Recently, the paper “Bayesian Model Selection, the Marginal Likelihood, and Generalization” by Lotfi et al. (2022)<d-cite key="lotfi2022bayesian"></d-cite>, which was accepted as Outstanding Paper and Long Oral at ICML 2022, examined the importance and challenges of model selection in machine learning, focusing on the log marginal likelihood (LML) and proposing a variant: the conditional log marginal likelihood (CLML). The authors argue that while LML is a useful tool for hypothesis testing, it may not be the best metric for model selection and for predicting the generalization performance of trained models or learning hyperparameters. They introduce the CLML as a potential improvement and demonstrate its effectiveness across various settings, including density models, Fourier features, Gaussian Processes, and deep neural networks.</p> <p>In this blog post, inspired by the above paper, we (re-)derive insights that challenge the conventional focus on the marginal likelihood and related quantities for Bayesian model selection. We argue that the quantities we examine are all consequences of Occam’s razor, and thus no single quantity should be considered universally superior. Instead, the choice of model selection criterion should be guided by the context and the desired outcomes. We highlight that many recently proposed metrics for model selection, including CLML, are closely related to cross-validation and have failure cases that can be explained by considering model misspecification and prior-data conflicts. Overall, the choice between these metrics should be based on the specific requirements of the task at hand.</p> <p>We begin by discussing the foundations of model selection, including the role of Occam’s razor and its relationship to maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation. We then introduce the concepts of log marginal likelihood (LML), cross-validation, and conditional log marginal likelihood (CLML), highlighting their connections and differences. Through a series of thought experiments and empirical observations, we explore the behavior of these model selection criteria in various scenarios, such as under model misspecification, prior-data conflict, and in different data regimes. We find that the conditional marginal cross-entropy, which is closely related to cross-validation, is often a more reliable choice when the primary objective is to select for generalization performance. On the other hand, the conditional <em>joint</em> marginal cross-entropy (permutation-invariant negative CLML) may be preferable when the focus is on sequential prediction and online learning. At the same time, the joint marginal information (negative LML) is rarely the right choice for model selection. We review relevant literature, including the work of Fong and Holmes (2020)<d-cite key="fong2020marginal"></d-cite> on the connection between the LML and cross-validation, the training speed estimators by Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite> and Ru et al. (2021)<d-cite key="ru2021speedy"></d-cite>, and the experiments of Lotfi et al. (2022) <d-cite key="lotfi2022bayesian"></d-cite>, comparing the CLML and validation loss for deep neural networks (DNNs). These studies provide valuable insights into the strengths and limitations of different model selection criteria.</p> <p>Throughout the post, we emphasize the importance of considering the context, available data, and desired outcomes when selecting the most appropriate metric for model selection and hyperparameter tuning. By questioning the primacy of the (conditional) joint marginal likelihood and encouraging critical thinking about the foundations of these quantities, we hope to foster a more nuanced understanding of Bayesian model selection.</p> <h2 id="bayesian-model-selection">(Bayesian) Model Selection</h2> <p>In our daily lives, we’re often faced with choices that require us to sift through competing explanations or decisions. Imagine you hear your doorbell ring. You might think it’s the delivery you’ve been waiting for, a neighbor dropping by, or perhaps you didn’t hear anything at all, and it was just your imagination. In deciding between these options, you’re likely to lean towards the simplest explanation that aligns with your expectations—say, the long-awaited delivery. This inclination towards simplicity has a formal counterpart in scientific discovery and machine learning, known as <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s razor</a>:</p> <aside class="box-important l-body"> <p><em>Occam’s razor</em> is the principle that, all else being equal, the simplest explanation tends to be the right one.</p> </aside> <p>This concept is further illustrated using <a href="https://www.inference.org.uk/itprnn/book.pdf#page=355">an example from chapter 28</a> of David MacKay’s seminal book, <a href="http://www.inference.org.uk/mackay/itila/book.html">“Information Theory, Inference, and Learning Algorithms”</a>, where the essence of selecting between models based on their evidence is laid out succinctly<d-footnote>But see also:<img src="https://preview.redd.it/jxob8cx6sbi21.jpg?width=706&amp;auto=webp&amp;s=9dcd32ad88c661ca033833b0686b28d2501941d1" width="50%" height="auto" style="display:block;"/>from this <a target="_blank" href="https://www.reddit.com/r/confusing_perspective/comments/atvu6s/long_cow/">Reddit thread</a> (with thanks to Freddie Bickford-Smith) that shows that the simplest explanation is always highly context- and prior-dependent.</d-footnote>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/mackay_343-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/mackay_343-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/mackay_343-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/mackay_343.png" class="img-fluid" width="auto" height="auto" alt="Occam’s razor --- How many boxes are in the picture (figure 28.1)? In particular, how many boxes are in the vicinity of the tree? If we looked with x-ray spectacles, would we see one or two boxes behind the trunk (figure 28.2)? (Or even more?) Occam’s razor is the principle that states a preference for simple theories. ‘Accept the simplest explanation that fits the data’. Thus according to Occam’s razor, we should deduce that there is only one box behind the tree. Is this an ad hoc rule of thumb? Or is there a convincing reason for believing there is most likely one box? Perhaps your intuition likes the argument ‘well, it would be a remarkable coincidence for the two boxes to be just the same height and colour as each other’. If we wish to make artificial intelligences that interpret data correctly, we must translate this intuitive feeling into a concrete theory." title="Occam’s razor --- How many boxes are in the picture (figure 28.1)? In particular, how many boxes are in the vicinity of the tree? If we looked with x-ray spectacles, would we see one or two boxes behind the trunk (figure 28.2)? (Or even more?) Occam’s razor is the principle that states a preference for simple theories. ‘Accept the simplest explanation that fits the data’. Thus according to Occam’s razor, we should deduce that there is only one box behind the tree. Is this an ad hoc rule of thumb? Or is there a convincing reason for believing there is most likely one box? Perhaps your intuition likes the argument ‘well, it would be a remarkable coincidence for the two boxes to be just the same height and colour as each other’. If we wish to make artificial intelligences that interpret data correctly, we must translate this intuitive feeling into a concrete theory." onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption"> Excerpt from page 343 in David MacKay’s "Information Theory, Inference, and Learning Algorithms.” </figcaption> </figure> <p>But how can we express this formally using mathematics?</p> <p>In the next section, we will use information-theoretic concepts to formalize Occam’s razor and connect it to the maximum likelihood estimation (MLE) and maximum-a-posteriori (MAP) estimation approaches. This formalization highlights that Occam’s razor, as a general principle favoring simplicity, can motivate various techniques, not just Bayesian ones. Therefore, using Occam’s razor as the sole justification for Bayesian model selection may not be as compelling as it initially appears.</p> <p>However, one could argue that when Occam’s razor is properly applied within a Bayesian framework, it captures a more nuanced notion of complexity. From this perspective, the Bayesian formulation of Occam’s razor favors models that strike a balance between goodness-of-fit and model complexity, where complexity is measured by the model’s ability to compress the data. This view is consistent with the <a href="https://www.wikiwand.com/en/Minimum_description_length">minimum description length (MDL)</a> principle, which posits that the best model is the one that minimizes the total description length of both the model and the data given the model.</p> <p><strong>From Philosophical Principle to Mathematical Statement</strong></p> <p>Let’s first connect Occam’s razor to <em>Maximum-A-Posteriori (MAP) Estimation</em> and <em>Maximum Likelihood Estimation (MLE)</em> before diving deeper into the background and (Bayesian) model selection.</p> <p>In information theory, the <strong>information content</strong> of an event \(x\) is defined as \(-\log_2 \pof{x}\), where \(\pof{x}\) is the probability of that event occurring according to a given model. This is also called <em>Shannon’s information content</em>. We use the base \(2\) for logarithms and measure information in <em>bits (binary digits)</em>, and for the rest of the post, we will drop the base of the logarithm. The information content measures the optimal encoding length in bits for the event \(x\) under the model specified by its probability distribution \(\pof{\cdot}\).</p> <p>Variables that cannot be directly observed are called <strong>latent variables</strong> within the context of probabilistic modeling. Occam’s razor suggests that we should prefer simpler explanations for latent variables, given the <em>observed data</em>.</p> <p>Consider a model with a latent variable \(z\) and observed data \(x\). The model specifies a probability distribution \(\pof{z \given x}\). According to Occam’s razor, we prefer simpler explanations, which correspond to smaller values of \(-\log \pof{z \given x}\). Using Bayes’ theorem, \(\pof{z \given x} = \frac{\pof{x \given z} \pof{z}}{\pof{x}}\), we can rewrite this as:</p> \[\text{minimize } z \text{ in } -\log \pof{z \given x} = -\log \pof{x \given z} - \log \pof{z} + \log \pof{x}.\] <p>Given that \(\pof{x}\) is independent of \(z\), we can omit it from our objective. Additionally, if we posit a uniform (or non-informative prior) for \(z\), implying that all potential values of \(z\) are equally probable before observing \(x\), then \(\pof{z}\) becomes constant and can also be dropped from our objective. This simplifies our preference to:</p> \[\text{minimize } z \text{ in } -\log \pof{x \given z}.\] <p>Equivalently, we can maximize \(\pof{x \given z}\), which is the <em>likelihood</em> of the observed data \(x\) given the latent variable \(z\). When making a decision and selecting a single value for \(z\), this leads to the maximum likelihood estimation (MLE) approach.</p> <div class="l-gutter"> <aside> As MacKay notes, \(\pof{x \given z}\) as a function in \(z\) is called a likelihood, while \(\pof{x \given z}\) as a function in \(x\) is called a probability. </aside> </div> <p>In summary, the connection between Occam’s razor and MLE relies on the following assumptions:</p> <ol> <li>Shannon’s information content is how we measure complexity.</li> <li>The prior distribution for the latent variables is uniform (or uninformative).</li> <li>Simpler explanations, as measured by the information content, are preferred (Occam’s razor).</li> </ol> <p>Under these assumptions, the preference for simpler explanations leads to the MLE approach, where more likely values of the latent variable given the observed data are preferred.</p> <p>Optimizing the MLE is common in machine learning because we can directly optimize the likelihood function. Still, this is not easy for deep learning models because they have a large number of parameters and the loss function is non-convex.</p> <h3 id="maximum-a-posteriori-estimation">Maximum-a-Posteriori Estimation</h3> <p>However, the assumption of a uniform or non-informative prior for the latent variables is not always valid or desirable. In many cases, we have prior knowledge about the latent variables that can be incorporated into the model. This leads to the <strong>Maximum-A-Posteriori (MAP) Estimation</strong> as an alternative to MLE.</p> <p>In MAP estimation, \(\pof{z}\) is not constant, so we cannot drop it—we can still drop \(\pof{x}\), however—and we maximize the joint distribution \(\pof{z, x}\), or equivalently:</p> \[\text{minimize } z \text{ in } -\log \pof{x, z}=-\log \pof{x \given z} - \log \pof{z}.\] <p>Before we go further, we need to introduce notation for information-theoretic quantities and concepts that we will use throughout the post<d-footnote>This next section is mostly shared with the sister post <a href="/2024/blog/dpi-fsvi/" target="_blank">"Bridging the Data Processing Inequality and Function-Space Variational Inference</a>.</d-footnote>.</p> <h2 id="background-information-theoretic-notation-and-concepts">Background: Information-Theoretic Notation and Concepts</h2> <p>Information theory deals with the communication and quantification of information<d-footnote>See the excellent <a href="https://colah.github.io/posts/2015-09-Visual-Information/">"Visual Information Theory"</a> by Chris Olah for a visual introduction to information theory.</d-footnote>. In this post, we use a unified information-theoretic notation to express various quantities related to probability distributions and their relationships<d-footnote>It largely follows "<a href="https://arxiv.org/abs/2106.12062">A Practical &amp; Unified Notation for Information-Theoretic Quantities in ML</a>".</d-footnote>. Here are some key concepts we will use:</p> <p>The <strong>information content</strong> of an event \(x\) is denoted as \(\Hof{x}\) and is defined as \(-\log_2 \pof{x}\), where \(\pof{x}\) is the probability of event \(x\) occurring. It represents the minimum amount of information needed to describe the occurrence of \(x\) given an underlying probability distribution. \(\Hof{x \given y}\) and \(\Hof{x, y}\) are analogously defined and denote the conditional and joint information content of random variables \(X\) and \(Y\), respectively. In machine learning, the information content is often used as a minimization objective, represented as the negative log-likelihood or cross-entropy when averaged over a dataset (see below).</p> <p>The <strong>entropy</strong> \(\Hof{X}\) of a random variable \(X\) is the expectation of its information content:</p> \[\Hof{X} \triangleq \E{\pof{x}}{\Hof{x}} = \E{\pof{x}}{-\log \pof{x}}.\] <p>The entropy measures the average amount of information needed to describe the random variable \(X\). It provides a measure of uncertainty or randomness associated with \(X\). We can similarly define the entropy of a conditional distribution \(\Hof{X \given Y}\) and the joint entropy \(\Hof{X, Y}\).</p> <p>We will also use the <strong>Kullback-Leibler divergence</strong> \(\Kale{\pof{X}}{\qof{X}}\) and the <strong>cross-entropy</strong> \(\CrossEntropy{\pof{X}}{\qof{X}}\):</p> \[\begin{aligned} \CrossEntropy{\pof{X}}{\qof{X}} &amp; = \E{\pof{x}}{-\log \qof{x}}\\ \Kale{\pof{X}}{\qof{X}} &amp; = \CrossEntropy{\pof{X}}{\qof{X}} - \Hof{X} \end{aligned}\] <p>The cross-entropy quantifies the average number of bits needed to encode samples drawn from the true distribution \(\pof{X}\) using a different distribution \(\qof{X}\). The Kullback-Leibler divergence measures the difference between two probability distributions and captures the additional bits needed to encode samples from \(\pof{X}\) compared to encoding them using the true distribution \(\qof{X}\).</p> <aside class="box-note l-body"> <p>To shorten notation, we will use the shorthand \(\iKale{\opp}{\opq}{X}\) for the Kullback-Leibler divergence and \(\iCrossEntropy{\opp}{\opq}{X}\) for the cross-entropy.</p> </aside> <h3 id="expressing-occams-razor-in-information-theoretic-terms">Expressing Occam’s Razor in Information-Theoretic Terms</h3> <p>Taking this notation into account, we can express Occam’s razor as:</p> \[\text{prefer small } z \text{ for } \Hof{z \given x},\] <p>where \(Z\) is the latent variable and \(X\) is the observed data. Note that \(x\) and \(z\) are individual realizations of the random variables \(X\) and \(Z\), respectively.</p> <p>The MLE and MAP objectives are accordingly:</p> \[\text{minimize } z \text{ in } \Hof{x \given z} \text{ for MLE and } \Hof{x, z} \text{ for MAP.}\] <p>This measures the number of bits we need to encode the observed data given the latent variable for MLE and the number of bits to encode both the observed data and the latent variable for MAP. This relates Occam’s razor to the minimum description length principle<d-footnote>See the <a href="https://en.wikipedia.org/wiki/Minimum_description_length">Wikipedia article on Minimum Description Length</a> for more details.</d-footnote>.</p> <h2 id="hyperparameter-learning-and-model-selection">Hyperparameter Learning and Model Selection</h2> <p>In many machine learning tasks, we need to determine the best hyperparameters for a model or select the most suitable model architecture from several discrete options. The primary goal is to find the hyperparameters or model that generalizes best to new, unseen data.</p> <p>Both cases can be viewed as inferring a random variable \(\H\), which represents either the model choice as a categorical distribution or the hyperparameters as a continuous distribution. In this sense, \(\H\) can be considered as another latent variable in the model.</p> <p>For consistency, we will continue using \(\x\) to denote data points throughout this post. Although it is common to use \(\y\) for predictions and \(\x\) for side channel information, we will not require this distinction here and will stick to \(\x\) for simplicity.</p> <p>The same arguments discussed previously also apply in this context, and we can express the objective as:</p> \[\text{minimize } \h \text{ in } \Hof{\x \given \h}.\] <h3 id="model-parameters">Model Parameters</h3> <p>In addition to the hyperparameters \(\H\), we usually have model parameters \(\W\) for a given \(\h\) with a parameter distribution \(\pof{\w \given \h}\) that we need to infer based on observed data. These parameters are the learnable components of the model, such as the weights and biases in a neural network. For given \(\w\) and \(\h\), we can easily compute the likelihood \(\pof{\x \given \w, \h}\), which represents the probability of observing the data \(\x\) given the specific values of the parameters and hyperparameters. However, to make predictions or compute the marginal likelihood, we will need to consider the uncertainty in the parameter values by integrating over all possible \(\w\).</p> <h3 id="bayesian-model-averaging">Bayesian Model Averaging</h3> <p>Bayesian Model Averaging (BMA) is a technique that integrates, or marginalizes, over the model parameters \(\W\) when making predictions. This accounts for the uncertainty in the model parameters, which is particularly useful when dealing with complex models, high-dimensional parameter spaces, and limited data. In contrast to the MLE or MAP estimate, which use a single parameter value \(\w\) for predictions, BMA provides a more robust and comprehensive approach. The probability of a new data point \(\x'\) under BMA is given by:</p> \[\pof{\x' \given \x, \h} = \int \pof{\x' \given \x, \w, \h} \pof{\w \given \x, \h} \, \mathrm{d}\w,\] <p>where \(\pof{\w \given \x, \h}\) is the posterior distribution of the parameters given the data, and \(\pof{\x' \given \x, \w, \h}\) is the likelihood of the new data point given the parameters, hyperparameters, and training data.</p> <aside class="box-note l-body"> <p>For this post, we refer to a <strong>model</strong> for some hyperparameters \(\h\) as the probability distribution \(\pof{\x, \w \given \h}\), which includes both the parameters \(\w\) and the data \(\x\).</p> <p>When we refer to a model’s predictions, we mean the marginal probability:</p> \[\pof{\x \given \h} = \int \pof{\x \given \w, \h} \pof{\w \given \h} \, d\w.\] <p>Depending on the context, we might discuss the prior distributions or posterior distributions after conditioning on additional data, but we will aim to be clear about that distinction.</p> </aside> <p>While BMA offers benefits, it is computationally challenging, particularly when dealing with high-dimensional parameter spaces commonly encountered in deep learning models. To make BMA tractable, various approximation methods, such as Markov Chain Monte Carlo (MCMC) and Variational Inference, have been proposed.</p> <h3 id="marginal-likelihood-and-estimation">Marginal Likelihood and Estimation</h3> <p>Let’s now discuss the marginal likelihood and its relation to BMA. The marginal likelihood, denoted as \(\pof{\x \given \h}\), is the likelihood of the observed data given the hyperparameters, marginalized over all possible parameter values \(\W\). It is also known as the <strong>model evidence</strong>. To compute the marginal likelihood, we integrate over all possible \(\w\):</p> \[\pof{\x \given \h} = \int \pof{\x \given \w, \h} \pof{\w \given \h} \, d\w,\] <p>where \(\pof{\x \given \w, \h}\) is the likelihood of the data given the parameters and hyperparameters, and \(\pof{\w \given \h}\) is the prior distribution of the parameters given the hyperparameters.</p> <p>Comparing BMA to the marginal likelihood, we see that they match for individual data points. However, for multiple data points (i.e., conditioning on datasets), the marginal likelihood is more complex. “BMA” typically refers to making predictions for a single new data point, while the marginal likelihood can be considered for many points simultaneously. Apart from this difference, the two are equivalent. Let’s discuss the case of multiple data points in more detail to understand why computing the marginal likelihood on datasets is even more challenging.</p> <aside class="box-note l-body"> <p>Going forward, we will focus on the marginalized quantities and condition on different hyperparameters \(\h\). Note that the separation between hyperparameters \(\H\) and parameters \(\W\) is somewhat artificial.</p> </aside> <h2 id="datasets-instead-of-individual-data-points">Datasets instead of Individual Data Points</h2> <p>So far, we have described everything as if we only had a single data point \(x\). However, in practice, we often have a dataset \(\xNtuple = (\x_1, \x_2, \ldots, \x_N)\).</p> <h3 id="joint-marginal-information-and-cross-entropy">Joint Marginal Information and Cross-Entropy</h3> <p>The easiest way to extend the previous definitions is to simply substitute \(\xNset\) for \(\x\) and assume we can compute a likelihood for the entire dataset using its joint predictive distribution:</p> \[\pof{\xNtuple \given \h} = \int \pof{\x_1, \x_2, \ldots, \x_N \given \w, \h} \, \pof{\w \given \h} \, d\w.\] <p>We can then maximize this likelihood or equivalently minimize the joint marginal information \(\Hof{\xNtuple \given \h}.\)</p> <aside class="box-note l-body"> <p>For multiple data points, \(\Hof{\xNset \given \h}\) is often referred to as the negative <em>marginal log likelihood</em> or (negative) <em>log marginal likelihood (LML)</em>. It is a key quantity in Bayesian model selection<d-footnote>Personally, I prefer log marginal likelihood as that matches the order of the terms, but that might just be the German in me.</d-footnote>.</p> <p>As the negative log likelihood is just Shannon’s information content, we will also use the term <strong>joint marginal information</strong> to be unambiguous.</p> </aside> <p>If our model is exchangeable, meaning the order of the \(\x_n\) does not matter, we can equivalently take an expectation over all permutations of the data to obtain the <strong>joint marginal cross-entropy</strong>:</p> \[\CrossEntropy{\pdata{\X_1, ...,\X_n}}{\pof{\X_1, ... \X_n \given \h}},\] <p>where \(\pdata{\cdot}\) is an empirical data distribution that allows us to draw samples <em>without replacement</em>. In this case, the joint marginal information and cross-entropy are equivalent.</p> <p>With exchangeability, we can simply write \(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNset}\) instead of using the tuple notation \(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNtuple}\) as the order of the data points does not matter.</p> <p>Conversely, if a model is not exchangeable, we can induce exchangeability by averaging over all permutations of the data points via ensembling. For example, deep learning models trained with stochastic gradient descent are generally <em>not</em> exchangeable, as the order and composition of the batches can impact the results. However, we can make them effectively exchangeable by training multiple models and averaging their predictions. In the limit of infinite models, the resulting ensemble will be exchangeable<d-footnote>The ensemble might not necessarily perform better though, as papers on training curricula have shown that batch order can be important<d-cite key="graves2017automated"></d-cite>.</d-footnote>.</p> <p>The joint marginal cross-entropy turns a potentially non-exchangeable joint information into an exchangeable one by taking an expectation.</p> <h3 id="marginal-information-and-cross-entropy">Marginal Information and Cross-Entropy</h3> <p>Before we try to understand these joint expressions, we should consider alternative ways to extend the previous definitions.</p> <p>For instance, we could take the average of the likelihoods for individual data points:</p> \[\frac{1}{N} \sum_{n=1}^N \pof{\x_n \given \h}.\] <p>Assuming an underlying data distribution \(\pdata{x}\), we can also express this as an attempt to estimate:</p> \[\E{\pdata{\x}}{\pof{\x \given \h}} = \int \pof{\x \given \h} \, \pdata{\x} \, d\x.\] <p>This provides an average score for the data likelihood.</p> <p>However, from the perspective of Occam’s razor, simply taking the average likelihood is not the most principled approach. Instead, we can leverage information theory, which has been our tool of choice thus far. Recall that we prefer small values of the <strong>marginal information</strong> \(\Hof{\x \given \h}\). By taking the expectation over the data distribution, we obtain the <em>individual</em> marginal cross-entropy:</p> \[\CrossEntropy{\pdata{\X}}{\pof{\X \given \h}} = \E{\pdata{\x}}{-\log \pof{\x \given \h}}.\] <p>This cross-entropy measures the average number of bits needed to encode the data using the model’s probability distribution. As it does not involve a joint distribution, we refer to it simply as the <strong>marginal cross-entropy</strong>.</p> <p>It is evident that the marginal cross-entropy and the average likelihood are not equivalent. Using the convexity of the negative logarithm and Jensen’s inequality, we see that the marginal cross-entropy is always larger than the negative logarithm of the average likelihood:</p> \[\begin{aligned} \CrossEntropy{\pdata{\X}}{\pof{\X \given \h}} &amp;= \E{\pdata{\x}}{-\log \pof{\x \given \h}} \\ &amp;\geq -\log \E{\pdata{\x}}{\pof{\x \given \h}} \\ &amp;\approx -\log \frac{1}{N} \sum_{n=1}^N \pof{\x_n \given \h}. \end{aligned}\] <aside class="box-note l-body"> <p>The cross-entropy \(\CrossEntropy{\pdata{\X}}{\pof{\X \given \h}}\) is a common loss function in machine learning, often referred to as the <strong>negative log-likelihood (NLL)</strong> or <strong>negative log-likelihood loss</strong>. Minimizing the cross-entropy is equivalent to minimizing the Kullback-Leibler divergence between the model’s distribution and the true data distribution. The context usually clarifies whether the NLL refers to marginalized likelihoods or likelihoods for individual parameter values \(\w\), depending on whether we optimize MLE and MAP estimates or follow a more fully Bayesian approach.</p> </aside> <p>The NLL is frequently used to evaluate a model’s performance <em>after</em> training, typically on a held-out <em>validation set</em>. This is equivalent to computing the cross-entropy between the empirical distribution of the validation set and the model’s predictive distribution, conditioned on the parameters learned from the training data:</p> \[\CrossEntropy{\hpcof{\text{val}}{\X'}}{\pof{\X' \given \xNtuple, \h}}\] <p>It is essential to distinguish this from the cross-entropy computed on the prior distribution of the model parameters before seeing any data, which is less useful for evaluating a trained model’s performance:</p> \[\CrossEntropy{\hpcof{\text{val}}{\X'}}{\pof{\X' \given \h}}\] <p>Only the NLL on a validation set <em>conditioned on the training data</em> provides an estimate of the model’s generalization ability after training. The same holds for the quantities marginalized over the model parameters.</p> <h3 id="marginal-cross-entropy-vs-joint-cross-entropy">Marginal Cross-Entropy vs Joint Cross-Entropy</h3> <p>Occam’s razor does not clearly specify which aggregate metric on \(\Hof{\x \given \h}\) we should prefer. Instead of the mean, we could use the median or a different quantile of the information content as a summary statistic to assess the model’s performance on the dataset. This might be more robust, as it is less sensitive to outliers.</p> <p>Crucially, the marginal cross-entropy and related summary statistics measure the model’s performance using the “prior” parameter distribution, not the posterior conditioned on data. However, the joint distribution captures something else, which can be seen more clearly using the chain rule:</p> \[\Hof{\xNset \given \h} = \sum_{k=1}^N \Hof{\x_n \given \x_1, \ldots, \x_{k-1}, \h}\] <p>Each term is a <strong>conditional marginal information</strong> on the previous data points. Similarly, when we take an expectation over the data distribution, we obtain a chain of <strong>conditional marginal cross-entropies</strong>:</p> \[\begin{aligned} &amp; \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNtuple} = \\ &amp;\quad = \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_1} + \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_2 \given \X_1} \\ &amp;\quad \quad + \ldots + \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{X_N \given \X_1, \X_2, \ldots, \X_{N-1}} \\ &amp;\quad = \sum_{n=1}^N \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_n \given \X_{n-1}, \ldots, \X_1}. \end{aligned}\] <p>Each term in the sum is a conditional marginal cross-entropy conditioned on the previous data points, which differs from the marginal cross-entropy (recognized in the first term).</p> <p>The following visualization summarizes the relationship between the conditional and joint marginal cross-entropies and information. The chain rule tells us that the area under the curve of the conditional quantities equals the joint quantity.</p> <figure class="l-page rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve_1.00-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve_1.00-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve_1.00-1400.webp"/> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/area_under_curve_1.00.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <p><em>The relationship between conditional and joint marginal cross-entropies and information.</em> <strong>Left</strong>: Conditional marginal cross-entropy (<span style="color: #1f77b4;">blue</span>) for a multi-class classification problem. The area under the curve (<span style="color: #ff7f0e;">orange</span>) represents the joint marginal cross-entropy. As the dataset size increases, the conditional marginal cross-entropy decreases and converges to the best achievable loss for the given model hypothesis \(\h\). <strong>Right</strong>: Conditional marginal information (<span style="color: #2ca02c;">green</span>). The area under the curve (<span style="color: #d62728;">red</span>) represents the joint marginal information. The conditional marginal information is a noisy estimate of the conditional marginal cross-entropy, as it is computed on individual data points.</p> </figcaption> </figure> <aside class="box-note l-body"> <p>The joint cross-entropy measures the model’s performance during progressive training, capturing its <em>online learning performance</em>. The conditional marginal cross-entropy measures the model’s performance without updating parameters, providing a more static view of its generalization ability.</p> </aside> <p>In summary, the marginal and joint cross-entropies offer different perspectives on a model’s performance<d-footnote>Recent works by Ian Osband et al., starting with <a href="https://arxiv.org/abs/2110.04629">The Neural Testbed: Evaluating Joint Predictions</a><d-cite key="osband2022neural"></d-cite> can help build intuitions for joint predictions. Similarly, a <em>gentler</em> introduction, comparing marginal and joint predictions, can also be found in the arXiv note <a href="https://arxiv.org/abs/2205.08766">Marginal and Joint Cross-Entropies &amp; Predictives for Online Bayesian Inference, Active Learning, and Active Sampling</a><d-cite key="kirsch2022marginal"></d-cite>.</d-footnote>:</p> <ul> <li>The marginal cross-entropy and related summary statistics assess the model’s performance using the prior parameter distribution, without considering the effect of the data on the model.</li> <li>The joint marginal cross-entropy, expressed as a sum of conditional marginal cross-entropies, captures the model’s online learning performance as it processes the data sequentially.</li> </ul> <p>While both metrics are useful for evaluating models, the joint marginal cross-entropy provides insight into how well the model learns from the data during training. The conditional marginal cross-entropy, on the other hand, is more suitable for assessing the model’s generalization ability at a given point in time, without the influence of parameter updates.</p> <h2 id="intermediate-comparison">Intermediate Comparison</h2> <p>Regarding the earlier question of what metric we should prefer and use for model selection, let’s consider:</p> <ol> <li> <p>The marginal cross-entropy, as in the first term, is likely not useful for model selection with deep learning models, as it is not conditioned on any data and thus cannot correlate well with the model’s performance after training.</p> </li> <li> <p>If we care about the model’s “generalization” performance <em>after training on \(N-1\) data points</em> without further adaptation, the marginal cross-entropy on the last data point is the more relevant quantity:</p> \[\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_N \given \X_{N-1}, \ldots, \X_1}\] <p>It measures the model’s performance on the last data point after having seen all previous data points, similar to a “leave-one-out” metric. Indeed, it is equivalent to <a href="https://www.wikiwand.com/en/Cross-validation_(statistics)#Leave-one-out_cross-validation">leave-one-out cross-validation</a> when we have an empirical data distribution consisting of \(N\) data points and sample without replacement.</p> </li> <li> <p>More generally, it is <em>equivalent</em> to cross-validation when we hold out more than one data point for evaluation from the empirical data distribution:</p> \[\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X' \given \X_{N-k}, ..., \X_{1}}.\] <p>This is the same expression as in <strong>(2.)</strong> but we assume there are more samples to draw from in the empirical data distribution \(\pdata{\x'}\). We call this term the conditional marginal cross-entropy and keep in mind its connection to cross-validation.</p> </li> <li> <p>On the other hand, if we care about the model’s performance as an online learner, or in the case of LLMs, as an in-context learner, the joint marginal cross-entropy becomes a more relevant metric. It measures the model’s ability to adapt and make accurate predictions as it sequentially processes new data points, conditioned on the information it has seen so far.</p> <p>In the context of online learning, the model receives data points one at a time and updates its predictions based on the cumulative knowledge gained from previous data points. The joint marginal cross-entropy captures how well the model incorporates this sequential information to make accurate predictions for future data points.</p> <p>Similarly, for in-context learning of LLMs, the model is provided with a prompt or context consisting of a sequence of data points, and it is expected to generate accurate completions or predictions based on this context. The joint marginal cross-entropy measures the model’s ability to effectively utilize the provided context to make accurate predictions for the next data point in the sequence.</p> </li> <li> <p>However, we would not want to use the unconditional joint marginal cross-entropy, but rather condition on some initial data to be closer to the actual use case of the model, which will have been (pre-)trained already. As such, we are interested in estimating a <strong>conditional joint marginal cross-entropy</strong>:</p> \[\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNsetk \given \XNkset}.\] <p>By conditioning on some initial data points, this metric assesses the model’s capacity to learn and adapt its predictions based on the evolving context after already having been trained to some degree. Compared to the conditional marginal cross-entropy above, this cross-entropy provides a more fine-grained evaluation of the model’s sequential prediction performance, taking into account the specific order and dependencies within the data.</p> <p>Moreover, the conditional joint marginal cross-entropy can be used to compare different models or hyperparameter settings in terms of their online learning or in-context learning capabilities. By evaluating this metric on held-out data sequences, we can determine which model or setting is better suited for tasks that require sequential adaptation and context-dependent predictions.</p> </li> <li> <p>If we have a preferred order of the data points (or a split in the case of exchangeability), we can also consider the <strong>conditional joint marginal information</strong>:</p> \[\Hof{\xNsetk \given \xNkset, \h}.\] </li> <li> <p>Both the conditional joint marginal cross-entropy and the conditional joint marginal information can be viewed as negative “<strong>conditional log marginal likelihood (CLML)</strong>” as defined by Lotfi et al. (2022)<d-cite key="lotfi2022bayesian"></d-cite>, depending on whether one takes the definition as given in the main paper, in which case it is equivalent to the conditional joint marginal information, or the permutation invariant version described in the appendix, which is equivalent to the conditional joint marginal cross-entropy.</p> </li> <li> <p>All these quantities are equally valid from the perspective of Occam’s razor.</p> </li> <li> <p>We have not discussed how to efficiently estimate these quantities, especially for deep learning models. More importantly, we have already considered that the joint marginal information (marginal likelihood), BMA, and the joint marginal cross-entropy (as an expectation over the marginal likelihood) are not easy to estimate.</p> </li> </ol> <p>This brings us to one of the main points:</p> <aside class="box-important l-body"> <p>The (conditional) marginal likelihood and its approximations are not the only tools we have for model selection and hyperparameter learning, and often not even the most appropriate ones. We must always consider the context in which we want to use a model and data, and then choose the most relevant metric for our use case.</p> </aside> <p>This is a crucial point that has not been sufficiently considered in the literature on model selection and hyperparameter learning previously, where the model evidence and marginal likelihood have been presented as the ultimate criteria. In practice, we rarely update a model on additional data during inference—this is changing with the advent of LLMs and strong in-context learners, but it is still not the norm.</p> <h3 id="all-in-one-table">All in One Table</h3> <p>There is value in contrasting all these quantities in a single table to make the differences more apparent and build understanding. Below is a tabular overview of the key quantities discussed in the context of model selection and evaluation. As a reminder, \(\H\) are the hyperparameters, \(\W\) are the model parameters, and \(\X\) are the data points, and the empirical data distribution \(\pdata{\cdot}\) samples <em>without replacement</em>. We will only consider marginalized quantities, so \(\W\) is integrated out.</p> <div class="l-page"> <table> <thead> <tr> <th>Quantity</th> <th>Formula</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Marginal Information</td> <td>\(\Hof{\x \given \h}\)</td> <td>The information content (surprise) of a single data point \(\x\) under the model’s prior predictive distribution.</td> </tr> <tr> <td>Marginal Cross-Entropy</td> <td>\(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X}\)</td> <td>The expected information content of a single data point \(\X\) under the model’s prior predictive distribution, averaged over the true data distribution.</td> </tr> <tr> <td>Joint Marginal Information</td> <td>\(\Hof{\xNset \given \h}\)</td> <td>The information content (surprise) of a dataset \(\xNset\) under the model’s joint prior predictive distribution. This is also known as (negative) log marginal likelihood (LML).</td> </tr> <tr> <td>Joint Marginal Cross-Entropy</td> <td>\(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNset}\)</td> <td>The expected joint information content of a dataset \((\X_1, ..., \X_n)\) under the model’s joint prior predictive distribution, averaged over the true data distribution. Equivalent to the joint marginal information for exchangeable models.</td> </tr> <tr> <td>Conditional Marginal Information</td> <td>\(\Hof{\x_n \given \x_1, \ldots, \x_{n-1}, \h}\)</td> <td>The information content of a single data point \(\x_n\) conditioned on the previous data points \((\x_1, \ldots, \x_{n-1})\) under the model’s predictive distribution. This is data-order dependent.</td> </tr> <tr> <td>Conditional Marginal Cross-Entropy</td> <td>\(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_n \given \X_{n-1}, \ldots, \X_1}\)</td> <td>The expected information content of a single data point \(\X_n\) conditioned on the previous data points \((\X_{n-1}, \ldots, \X_1)\) under the model’s predictive distribution, averaged over the true data distribution. Equivalent to leave-one-out cross-validation.</td> </tr> <tr> <td>Conditional Joint Marginal Information</td> <td>\(\Hof{\xNsetk \given \xNkset, \h}\)</td> <td>The joint information content of a dataset \(\xNsetk\) conditioned on a previous dataset \(\xNkset\) under the model’s joint predictive distribution. This is data-order dependent. Also known as the (negative) <em>conditional log marginal likelihood (CLML)</em> (main paper, Lotfi et al., 2022)<d-cite key="lotfi2022bayesian"></d-cite>.</td> </tr> <tr> <td>Conditional Joint Marginal Cross-Entropy</td> <td>\(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNsetk \given \XNkset}\)</td> <td>The expected joint information content of a dataset \(\XNsetk\) conditioned on a previous dataset \(\XNkset\) under the model’s joint predictive distribution, averaged over the true data distribution. Measures the model’s online learning or in-context learning performance. Also known as the (negative) <em>conditional log marginal likelihood (CLML)</em> (appendix, Lotfi et al., 2022)<d-cite key="lotfi2022bayesian"></d-cite>.</td> </tr> </tbody> </table> </div> <p>The building blocks of these quantities are thus:</p> <ul> <li>Marginal quantities assess the model’s performance after marginalizing over the parameter distribution;</li> <li>Joint quantities capture the model’s performance on multiple data points as a whole, taking into account the dependencies between them, which can be also viewed as the average performance during online learning when using a sequential decomposition via the chain rule;</li> <li>Conditional quantities measure the model’s performance conditioned on previously seen data points, providing insights into its generalization ability or online learning performance after some data fit has happened, which can be more meaningful.</li> </ul> <p>The choice of the most appropriate metric depends on the specific context and the intended use case of the model.</p> <p>But then, why has the marginal likelihood been the preferred choice for model selection so far then?</p> <h2 id="different-data-regimes">Different Data Regimes</h2> <p>To explore when the conditional marginal cross-entropy and joint marginal cross-entropy lead to different outcomes for model selection and hypothesis testing, let’s consider a few key scenarios.</p> <p>For the discrete case, we can reduce the question to one about ranking: if we have two possible hyperparameter choices \(\h_1\) and \(\h_2\), when do we get the same ranking \(\h_1 \succ \h_2\) for both metrics?</p> <h3 id="model-misspecification">Model Misspecification</h3> <p>First, let’s examine the case when we have a large amount of data available. Here, model misspecification, a common concern, plays a crucial role.</p> <p>As renowned statistician George Box famously stated:</p> <blockquote> <p>All models are wrong, but some are useful.</p> <footer> <p>— <cite>George Box, Science and Statistics (1976)</cite></p> </footer> </blockquote> <p>When working with real-world data, we must always assume that our models are misspecified to some degree. Models simplify complex systems and cannot capture every nuance of the data-generating process. Consequently, the goal of model selection is not to find the “true” model but rather to identify the most useful model that balances simplicity, interpretability, and predictive performance.</p> <aside class="box-note l-body"> <p><strong>Model misspecification</strong> occurs when the assumed model class does not contain the true data-generating process. In other words, no parameter values within the model class can perfectly capture the underlying reality that produced the observed data.</p> <p>Even with infinite data, a misspecified model will not converge to the true data-generating distribution, leading to biased parameter estimates, incorrect inferences, and suboptimal predictions. Model misspecification is common in real-world applications, as our models are often simplifications of complex phenomena.</p> </aside> <p>Without model misspecification, we would always converge to the maximum likelihood estimate (MLE) that matches the data-generating model in the infinite data limit as the <a href="https://www.wikiwand.com/en/Bernstein%E2%80%93von_Mises_theorem">Bernstein-von Mises’ theorem</a> tells us that posteriors converge to the MLE in the limit. However, in practice, we are always dealing with misspecified models, and the MLE will not converge to the true data-generating model.</p> <aside class="box-note l-body"> <p>When dealing with misspecified models $\H$, it becomes crucial to identify which model performs best. Since the models are misspecified, their best achievable performance in the infinite data limit will differ.</p> <p>Here, model selection aims to find the model class $\hopt$ that can achieve the highest performance given ample data. By selecting the best misspecified model, we can maximize our predictive capabilities within the limitations of our model classes.</p> </aside> <h3 id="infinite-data-limit">Infinite Data Limit</h3> <p>Let’s return to our question of when the different quantities lead to similar rankings.</p> <p>While a conditional joint marginal cross-entropy, as a sum of conditional marginal cross-entropies, is obviously larger than each individual term, if we divide the joint marginal cross-entropy by the number of samples in the conditional joint distribution, we obtain the <strong>rate</strong><d-footnote>In this context, "rate" refers to the average amount of cross-entropy or information per (training) sample, drawing parallels to the concept of entropy rate in Shannon's information theory. This usage is distinct from other common uses of "rate" in machine learning, such as learning rate or convergence rate.</d-footnote> of the conditional joint marginal cross-entropies as its per-sample average, which can be more easily related:</p> \[\begin{aligned} &amp; \frac{1}{N-k} \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNsetk \given \XNkset} \\ &amp;\quad = \sum_{n=N-k+1}^N \frac{1}{N-k} \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_n \given \X_{n-1}, ..., \X_1}. \end{aligned}\] <p>Bernstein-von Mises’ theorem tells us that the posterior distribution of the model parameters converges to a normal distribution around the MLE as the number of data points goes to infinity<d-footnote>There are likely fewer caveats to this statement than the naive interpretation of the theorem implies because we are usually not interested in converging towards some unique and identifiable parameters but rather in the predictions matching the data-generating process.</d-footnote>. This means that the later terms in the chain rule decomposition of the joint cross-entropy will converge to the same value in the infinite sample limit as the data we condition on becomes infinite. If we take the limit, we can ignore the first terms in the chain rule decomposition of the joint cross-entropy, and we will get the same average value for the terms of the joint cross-entropy (one per sample in the joint) and the conditional cross-entropy. This matches a similar result on entropy rates in “Elements of Information Theory” by Cover &amp; Thomas<d-cite key="cover1999elements"></d-cite>.</p> <p>Overall, we have (without formal proof):</p> \[\begin{aligned} &amp;\lim_{N \to \infty} \frac{1}{N} \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNset} = \\ &amp;\quad = \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^N \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_n \given \X_{n-1}, ..., \X_1} \\ &amp;\quad = \lim_{N \to \infty} \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X' \given \XNset}. \end{aligned}\] <p>Given sufficient data (in the infinite sample limit), we see that either of these quantities will lead to the same ranking of different hyperparameters/model hypotheses. Conversely, we can expect to see meaningful differences only in low-data regimes, where the model is not yet fully adapted to the data.</p> <p>Finally, in the infinite data limit, for the conditional marginal cross-entropy, we don’t need to take an expectation over the data we condition on (as the model parameters will still have converged):</p> \[\begin{aligned} &amp;\lim_{N \to \infty} \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNsetk \given \XNkset} \\ &amp;\quad = \lim_{N \to \infty} \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNsetk \given \xNset}, \end{aligned}\] <p>forany \(\xNset \sim \pdata{\xNset}\) as \(n \to \infty\). More importantly, this also holds for the joint marginal information, whose rate in the limit is the same as the rate of the joint marginal cross-entropy above (and thus also joint cross-entropy):</p> \[\begin{aligned} &amp;\lim_{N \to \infty} \frac{1}{N} \Hof{\xNset \given \h} = \\ &amp;\quad = \lim_{N \to \infty} \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X' \given \XNset}. \end{aligned}\] <p>We have previously mentioned the connection between cross-validation, leave-one-out validation, and the conditional marginal cross-entropy. This result also connects the marginal likelihood in the limit to these quantities.</p> <p>Thus:</p> <aside class="box-important l-body"> <p>In the infinite data limit, the rate of the (conditional) log marginal likelihood, or equivalently the (conditional) joint marginal information, the rate of the (conditional) joint marginal cross-entropy, and the conditional marginal cross-entropy converge to the same value when averaged over the data distribution. This means that given sufficient data, all these metrics will produce the same ranking of different model hypotheses or hyperparameter choices.</p> </aside> <p>The catch is that “sufficient data” might be a very large amount of data, especially for highly expressive models like neural networks.</p> <p>Hence, we only expect these quantities to be meaningfully different in the low-data regime. So let’s focus on the low-data regime now.</p> <h3 id="prior-data-conflict">Prior-Data Conflict</h3> <p>Even if different hyperparameter choices lead to the same generalization loss in the infinite data limit, they can induce different priors that affect the convergence speed and model performance in the low-data regime.</p> <aside class="box-note l-body"> <p>A <strong>prior-data conflict</strong> occurs when the prior distribution hinders the model from quickly converging to the maximum likelihood estimate (MLE) by not placing sufficient probability mass near the MLE. Hyperparameter tuning aims to resolve this conflict by finding a prior more aligned with the data distribution.</p> </aside> <p>In the low-data regime, assuming all models converge to the same validation loss given infinite data, we prefer the model that converges the fastest, i.e., with the least amount of training data. A model with a prior well-aligned with the data distribution learns efficiently and generalizes better with limited data.</p> <figure class="l-page rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification_0.67-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification_0.67-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification_0.67-1400.webp"/> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification_0.67.svg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <p><em>Conditional marginal cross-entropy vs. dataset size under different modeling scenarios.</em> <strong>Left: Model misspecification</strong> - Three model hypotheses (\(\h_1\), \(\h_2\), \(\h_3\)) converge to different losses due to the model class not containing the true data-generating process. The minimum achievable loss represents the misspecification error. <strong>Right: Prior-data conflict</strong> - Three model priors (\(\h_1\), \(\h_2\), \(\h_3\)) converge to the same loss but at different speeds due to varying alignment with the data distribution. Priors with more mass near the MLE converge faster. <em>Real-world models often face both prior-data conflict and model misspecification.</em></p> </figcaption> </figure> <p>In this scenario, the area under the conditional marginal cross-entropy or information curve (equivalent to the joint marginal cross-entropy, or joint marginal information) indicates the preferred model. The model with the lowest joint marginal information (highest log marginal likelihood) fits the available data best while having a prior enabling efficient learning and generalization.</p> <h3 id="anti-correlated-model-misspecification-and-prior-data-conflict">Anti-Correlated Model Misspecification and Prior-Data Conflict</h3> <p>Finally, what happens when there are both model misspecification and a prior-data conflict in the low-data regime? If both are correlated, the ranking will be preserved, but if they are anti-correlated, the ranking might change.</p> <p>Let’s visualize this: the curves will intersect at some point, and the model with the best achievable loss in the infinite data limit might not be the best choice in the low-data regime, depending on how much data we can train on. The optimal model choice may also change based on the amount of available data.</p> <figure class="l-body rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/anticorrelated_prior_conflict_and_model_misspecification_1.30-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/anticorrelated_prior_conflict_and_model_misspecification_1.30-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/anticorrelated_prior_conflict_and_model_misspecification_1.30-1400.webp"/> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/anticorrelated_prior_conflict_and_model_misspecification_1.30.svg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <p><em>The conditional marginal cross-entropy is plotted for three different model hypotheses (\(\h_0\), \(\h_1\), \(\h_2\)) as a function of dataset size. The models exhibit both prior-data conflict and model misspecification.</em> In the small data regime, \(\h_2\) has the lowest loss due to its prior aligning well with the data distribution, allowing for faster initial learning. However, as more data becomes available, the models’ asymptotic performance quickly plateaus. First, \(\h_1\) takes over, and then finally \(\h_0\), which converges to the lowest achievable loss in the infinite data limit, indicating it suffers the least from model misspecification. In contrast, \(\h_1\) and \(\h_2\) converge to higher loss values due to greater misspecification. Notably, the models’ performance ranking changes multiple times as the dataset grows, with \(\h_2\) being initially favored but ultimately having the worst infinite-data loss. Each model ranks best for the conditional joint marginal cross-entropy for some chosen range. <em>This illustrates how the interplay between prior-data conflict and model misspecification can lead to different model selection decisions depending on the amount of available data and the metric used to measure performance.</em></p> </figcaption> </figure> <p>Here, the joint marginal cross-entropy and the joint marginal information (log marginal likelihood) might not lead to the same decision because the area under the curve at the start might be larger than what the best model can save later. This could change the ranking of the models compared to the conditional marginal cross-entropy (leave-one-out cross-validation) at the end of training, which serves as a proxy for the model’s generalization performance.</p> <p>Instead, the conditional joint marginal cross-entropy and information can shine here by conditioning “away” the beginning of the curve, thus giving us a better estimate of the conditional marginal cross-entropy (or expected information) at the point of interest.</p> <p>To formalize this, we can use the chain rule to split the joint marginal cross-entropy into two terms:</p> \[\begin{aligned} &amp;\underbrace{\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNset}}_{\text{Joint Marginal Cross-Entropy}} = \\ &amp;\quad = \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNsetk} \\ &amp;\quad \quad + \underbrace{\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNset \given \XNsetk}}_{\text{Conditional Joint Marginal Cross-Entropy}}, \end{aligned}\] <p>Note that the per-sample averages of both terms converge to the same value in the infinite data limit—the conditional marginal cross-entropy (cross-validation loss), as discussed previously. However, the second term will converge faster because it does not include the constant \(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNsetk}\).</p> <p>We can also see both terms as approximating the conditional marginal cross-entropy (cross-validation loss) for a fixed \(N\) in the low-data regime. The per-sample average of the second term will provide a better approximation.</p> <p>In summary, the consistency of the ranking will depend on the size of \(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNsetk}\) for different \(\h\) and how it compares to the conditional joint marginal cross-entropy \(\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNset \given \XNsetk}\).</p> <p>This analysis highlights the importance of considering both prior-data conflict and model misspecification when selecting models in the low-data regime. The choice of performance metric and the amount of available data can significantly impact the ranking of models. The conditional joint marginal cross-entropy provides a more accurate estimate of the model’s generalization performance by conditioning away the initial part of the learning curve, which may be heavily influenced by prior-data conflict.</p> <h2 id="approximating-the-validation-loss">Approximating the Validation Loss</h2> <p>You may be wondering: why bother with the marginal likelihood or conditional joint marginal cross-entropy at all? Why not just always use leave-one-out cross-validation (i.e., the conditional marginal cross-entropy) or a simple validation loss?</p> <p>While that is a valid approach, the key question is: can we approximate the validation loss earlier in training, without fully training the model? Or can we do this more efficiently than performing inference on each element of a validation set?</p> <p>One option is to extrapolate the training loss to predict the validation loss. While potentially underexplored in this context, scaling laws have been found effective for predicting model performance.</p> <p>Alternatively, when training a model on a dataset for a single epoch—which is still surprisingly common for large language models, especially without active data sampling—the average training loss per batch provides a good approximation of the validation loss. With a cross-entropy loss, this is equivalent to estimating the conditional marginal cross-entropy.</p> <p>However, the batch size may not be large enough for a precise estimate. Averaging over the last few batches or using an exponential moving average can help, as the training losses on earlier batches were computed with older model parameters. Compared to using only the last batch’s loss, this smooths the estimate and reduces sensitivity to outliers.</p> <p>In the multi-epoch setting, revisiting data points multiple times prevents using the training loss as a validation loss estimate. Here, cross-validation offers a solution: train on the held-out data in the last epoch, compute the validation loss via the training losses, and obtain an ensemble of fully trained models without wasting data.</p> <p>In summary, while the validation loss is the gold standard, approximations based on the training loss or cross-validation can provide efficient estimates, especially in the early stages of training or with limited data.</p> <h2 id="the-big-comparison">The Big Comparison</h2> <p>In this post, we have explored various metrics for model selection and hyperparameter learning in the Bayesian context, focusing on the marginal likelihood, joint marginal cross-entropy, and conditional marginal cross-entropy. Our discussion has led to several key insights:</p> <ol> <li> <p><strong>Infinite Data Limit</strong>: As the dataset size approaches infinity, the rate of the log marginal likelihood (or equivalently, the joint marginal information), the joint marginal cross-entropy, and the conditional marginal cross-entropy converge to the same value when averaged over the data distribution. Given sufficient data, all these metrics will produce the same ranking of different model hypotheses or hyperparameter choices.</p> </li> <li> <p><strong>Connection to Cross-Validation</strong>: The conditional marginal cross-entropy is equivalent to the expected cross-validation loss. Cross-validation is the gold standard for model selection in machine learning practice, where a model’s generalization performance is estimated by evaluating it on held-out validation data after training on the remaining data.</p> </li> <li> <p><strong>Sufficient Data Requirement</strong>: The amount of data needed for the convergence of these metrics in the infinite data limit may be impractically large, especially for highly expressive models like deep neural networks. Therefore, the convergence property may not be directly relevant in many real-world scenarios.</p> </li> <li> <p><strong>Low-Data Regimes</strong>: When data is limited, the metrics can differ significantly. The conditional marginal cross-entropy (or cross-validation loss) is often the more reliable choice for model selection targeting generalization performance, as it directly measures the model’s ability to predict unseen data after being trained on the available data.</p> </li> <li> <p><strong>Sequential Prediction and Compression</strong>: The joint marginal cross-entropy, which corresponds to the negative log marginal likelihood, may be preferable if the focus is on a model’s overall sequential prediction performance or compression ability on the training data itself. It measures how well the model fits the entire training dataset jointly, without splitting into train and validation sets.</p> <p>Moreover, the conditional joint marginal information and cross-entropy are particularly relevant for measuring the performance of online learners and the in-context learning abilities of large language models (LLMs). These metrics capture the model’s ability to adapt and make accurate predictions based on the sequential information and evolving context after training on available data.</p> </li> <li> <p><strong>Model Misspecification and Prior-Data Conflict</strong>: In practice, models often face a combination of model misspecification (where the true data-generating process is not contained within the model class) and prior-data conflict (where the prior distribution does not align well with the data distribution). The interplay between these factors can lead to different rankings of models depending on the amount of available data and the specific metric used for evaluation.</p> </li> </ol> <p>While the marginal likelihood has been a popular tool for model selection and hyperparameter learning in the Bayesian community, its suitability depends on the specific context and goals. The conditional marginal cross-entropy, closely related to cross-validation, is often a more reliable choice when the primary objective is to optimize generalization performance. However, the conditional joint marginal cross-entropy (or conditional log marginal likelihood) may be preferable when the focus is on sequential prediction after training or measuring in-context learning abilities.</p> <p>Now, after having thought about all this in detail and mostly from first principles, let’s discuss the literature and how it supports or augments these considerations.</p> <h2 id="literature-review">Literature Review</h2> <p>Having discussed the key concepts, we will now look at several influential papers that have shaped the previous discussion on model selection and hyperparameter tuning in the Bayesian context or have provided valuable insights into the marginal likelihood and its connections to other metrics.</p> <h3 id="fong-and-holmes-2020-on-the-marginal-likelihood-and-cross-validation">Fong and Holmes (2020): “On the marginal likelihood and cross-validation”</h3> <p>Fong and Holmes (2020)<d-cite key="fong2020marginal"></d-cite> explore the connection between the log marginal likelihood (joint marginal information) and cumulative leave-p-out cross-validation. Under exchangeability, they show that the joint marginal information can be rewritten as a cumulative sum of leave-p-out cross-validation terms.</p> <p>The authors define the <em>leave-p-out cross-validation score</em> as:</p> \[S_{CV}(\xNset;p) = \frac{1}{\binom{N}{p}} \sum_{V \in \binom{[N]}{p}} \frac{1}{p} \sum_{i=1}^p \Hof{\x^{V}_i \given \{\x^{\bar{V}_k}\}_{k=1}^{N-p}}\] <p>where \(\binom{[N]}{p}\) denotes the set of all \(p\)-length subsets of \(\{1,...,N\}\)—the indices of the validation set—\(\x^V_i\) is the \(i\)-th validation data point, and \(\x^{\bar{V}}_k\) is the \(k\)-th training data point. This score measures the model’s performance using \(p\) validation points given the remaining data for training, equivalent to the respective conditional marginal cross-entropy.</p> <p>The <em>cumulative leave-P-out cross-validation score</em> is defined as:</p> \[S_{CCV}(\xNset; P) = \sum_{p=1}^P S_{CV}(\xNset; p)\] <p>This score focuses on the last \(P\) stages of the learning curve equally and is the same as the conditional joint marginal cross-entropy. For \(P=N\), the cumulative leave-N-out cross-validation score equals the joint marginal information:</p> \[S_{CCV}(\xNset; N) = \Hof{\xNset}\] <p>Comparing \(P&lt;N\) to \(P=N\), Fong and Holmes highlight the potential sensitivity of the marginal likelihood to the choice of prior. They argue for using cumulative cross-validation following a preparatory training phase with \(P&lt;N\) (e.g., \(10\%\) or \(50\%\)), demonstrating benefits over the full marginal likelihood for model selection, especially with vague priors or model misspecification.</p> <p>The paper also discusses the coherence of the log posterior predictive probability as a scoring rule in cross-validation and explores connections to prequential analysis and intrinsic Bayes factors.</p> <p>Fong and Holmes (2020) strongly support the ideas in this blog post, particularly the connections between marginal likelihood, cross-validation, and focusing on later learning curve stages for model selection. They establish the equivalence between the cumulative leave-p-out cross-validation score and conditional joint marginal information, aligning with our discussion of the conditional joint marginal cross-entropy as a more reliable metric compared to the full marginal likelihood.</p> <h3 id="lyle-et-al-2020-and-ru-et-al-2021-training-speed-and-model-selection">Lyle et al. (2020) and Ru et al. (2021): Training speed and model selection</h3> <p>In “A Bayesian Perspective on Training Speed and Model Selection”, Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite> establish a connection between training speed and the marginal likelihood in linear models. They propose using the sum of mini-batch training losses as a proxy for the log marginal likelihood to predict the generalization behavior of deep neural networks. This sum, referred to in later works as the <em>training speed estimator</em> (TSE), corresponds to the area under the learning curve. For 1-sample batches, the TSE is defined as:</p> \[\text{TSE}(\xNset) = \sum_{n=1}^N \Hof{\x_n \given \w_n},\] <p>where \(\Hof{\x_n \given \w_n}\) is the cross-entropy loss at training step \(n\) with model parameters \(\w_n\). Thus, an MLE estimate is used instead of conditioning on the data points \(\x_{&lt;n}\) and using the BMA.</p> <p>The authors provide an iterative algorithm for linear models to estimate a lower bound on the LML over multiple epochs of training. This allows capturing the model’s performance as it sees more data points over the course of training, rather than being limited to a single epoch. They also discuss extending their estimator to the infinite-width limit of neural networks.</p> <p>Building upon Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite>, Ru et al. (2021)<d-cite key="ru2021speedy"></d-cite> focus on using TSE for model selection in neural architecture search in “Speedy Performance Estimation for Neural Architecture Search”. They propose two variants of TSE: <em>TSE-E</em>, which focuses on the last few epochs, and <em>TSE-EMA</em>, which uses an exponential moving average to assign higher weights to later epochs:</p> \[\begin{aligned} \text{TSE-E}(\xNset) &amp;= \sum_{n=N-E+1}^N \Hof{\x_n \given \w_n}, \\ \text{TSE-EMA}(\xNset) &amp;= \sum_{n=1}^N \alpha^{N-n} \Hof{\x_n \given \w_n}, \end{aligned}\] <p>where \(\alpha \in (0, 1)\) is a hyperparameter controlling the decay rate.</p> <p>The authors hypothesize that assigning higher weights to later epochs may lead to better correlation with the true generalization performance of the final trained network, as the early epochs may be unstable and less informative.</p> <p>They demonstrate empirically that TSE-E and TSE-EMA can reliably estimate the generalization performance of neural architectures with a small training budget and remain effective for a large range of training epochs. TSE outperforms other efficient estimators, such as early stopping and learning curve extrapolation, in terms of rank correlation with the true test performance.</p> <p>The TSE estimators proposed by Ru et al. (2021) align closely with the ideas discussed in this blog post, as they prioritize the model’s performance in the later stages of learning. The empirical results presented by Ru et al. (2021) and Lyle et al. (2020) provide supporting evidence for the importance of going beyond the marginal likelihood.</p> <h3 id="lotfi-et-al-2022-bayesian-model-selection-the-marginal-likelihood-and-generalization">Lotfi et al. (2022): “Bayesian Model Selection, the Marginal Likelihood, and Generalization”</h3> <p>Lotfi et al. (2022)<d-cite key="lotfi2022bayesian"></d-cite> provide a comprehensive re-evaluation of the marginal likelihood as a metric for predicting the generalization performance of trained models and learning hyperparameters. They argue that while the marginal likelihood is well-suited for prior hypothesis testing, it is only peripherally related to generalization after training. The authors identify several practical and philosophical issues in using the marginal likelihood for selecting between trained models, such as its sensitivity to the choice of prior, potential to lead to both underfitting and overfitting, and negative correlation with generalization performance in some cases.</p> <p>To address these limitations, Lotfi et al. propose the conditional marginal likelihood (CLML) as a partial remedy. The CLML is computed by conditioning on a subset of the training data, which helps to mitigate the influence of the prior and focus on the model’s performance under this posterior. It is also less sensitive to the number of parameters in the model. The authors demonstrate that the CLML is better correlated with generalization than the marginal likelihood and provides promising performance for deep kernel hyperparameter learning and neural architecture search.</p> <p>The CLML shares significant similarities with the cumulative leave-p-out cross-validation score proposed by Fong and Holmes (2020)<d-cite key="fong2020marginal"></d-cite>. Both approaches essentially propose the same metric, which focuses on the model’s performance in the later stages of learning and provides a more reliable indication of generalization compared to the full marginal likelihood. Lotfi et al. also critically compare their work to that of Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite>, but do not discuss the work of Ru et al. (2021)<d-cite key="ru2021speedy"></d-cite>.</p> <p>Lotfi et al. conduct an extensive empirical evaluation of the CLML across various settings, comparing it to the marginal likelihood and other baselines under different conditions, such as varying dataset sizes, model complexities, and hyperparameter settings. They demonstrate that the CLML consistently outperforms the marginal likelihood in terms of selecting the hyperparameters that lead to better generalization performance. The authors also acknowledge some limitations of their work, such as the need for further theoretical analysis of the CLML’s properties and the potential challenges in estimating the CLML for more complex models.</p> <p>The key novelty of Lotfi et al.’s work lies in their comprehensive analysis of the limitations of the marginal likelihood for model selection and hyperparameter learning, as well as their proposal of the CLML as a practical alternative that addresses these limitations.</p> <h2 id="a-simple-toy-experiment">A Simple Toy Experiment</h2> <p>To illustrate the concepts discussed in this post, we conduct a simple toy experiment using a Bayesian linear regression model. The goal is to demonstrate how the various information metrics behave under different prior settings and dataset sizes, and to show that none of the metrics are universally reliable. In particular, the joint marginal information may not be the best choice when the primary concern is static performance after training on data.</p> <h3 id="experimental-setup">Experimental Setup</h3> <p>We generate a synthetic dataset with 64 features and 500 training and validation samples each. The true coefficients are drawn from a normal distribution with a mean of 2, and the target is the dot product between the features and the true coefficients.</p> <p>For the model, we use a Bayesian linear regression with an isotropic Gaussian prior on the weights (hyperparameter \(\wstddev\)) and independent Gaussian noise (hyperparameter \(\noisestddev\)). The model is misspecified when \(\noisestddev &gt; 0\). We consider three different prior settings:</p> <ul> <li>Model 1 (\(\h_1\)): \(\wstddev=0.1\), \(\noisestddev=0.8\)</li> <li>Model 2 (\(\h_2\)): \(\wstddev=100\), \(\noisestddev=1.0\)</li> <li>Model 3 (\(\h_3\)): \(\wstddev=1\), \(\noisestddev=1.2\)</li> </ul> <p>Thus, all three models are misspecified to varying degrees and exhibit different levels of prior-data conflict.</p> <p>We train the model on subsets of the training data of varying sizes, ranging from 1 to the full training set size, performing 5 trials with different splits. For each subset size, we compute the following metrics:</p> <ul> <li>Joint Marginal Information (JMI)</li> <li>Conditional Joint Marginal Information (CJMI) with half the data used for conditioning</li> <li>Marginal Cross-Entropy (MCE) on the training set</li> <li>Marginal Cross-Entropy (MCE) on the validation set</li> <li>Training Speed (Approximate)</li> <li>Joint Marginal Information Rate (JMI Rate)</li> </ul> <p>The JMI is equivalent to the negative log marginal likelihood, the CJMI to the negative conditional log likelihood, and the MCE corresponds to the cross-entropy loss. The Training Speed approximates an iterative algorithm by following the full data gradient. The JMI Rate is the JMI divided by the dataset size, which converges to the MCE in the infinite data limit.</p> <h3 id="results">Results</h3> <p>The results of the experiment are summarized in the following plots:</p> <figure class="l-page rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_information_metrics-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_information_metrics-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_information_metrics-1400.webp"/> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/binary_regression_information_metrics.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <p><em>Information metrics for the three Bayesian linear regression models as a function of dataset size.</em> The joint marginal information does not indicate the best performing model. The conditional joint marginal information (conditioned on half the dataset size, predicting on the other half) only finds the best model after 4/5 of the data are observed. <em>Metrics are reported in bits (log base 2), five trials each.</em></p> </figcaption> </figure> <p>The plots show the behavior of the information metrics as the dataset size increases for the three different prior settings. Some key observations:</p> <ul> <li>The marginal cross-entropy (MCE) metrics decrease as the dataset size increases, indicating improved model performance.</li> <li>The joint marginal information (JMI) increases with more data, as it is equivalent to the area under the curve of the MCE on the training set. (As we take the average over multiple trials, its mean is actually an estimate of the joint marginal cross-entropy.)</li> <li>The JMI rate, which is the JMI divided by the dataset size, decreases very slowly towards the same value as the MCE. This agrees with the previous discussion on the infinite data limit.</li> <li>The training losses also decrease, while their sum, equal to the training speed estimator (TSE), increases with the dataset size.</li> <li>The conditional joint marginal information (CJMI) with half the data used for conditioning shows a similar trend to the JMI but with lower values, as it focuses on the model’s performance on the held-back data. As we take an average over multiple trials, it is actually an estimate of the conditional joint marginal cross-entropy.</li> </ul> <p>To further analyze the model selection behavior, we computed the CJMI for different conditioning set sizes and selected the model with the lowest CJMI for each combination of dataset size and conditioning set size. The results are visualized in the following plot:</p> <figure class="l-body rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary-1400.webp"/> <img class="img-fluid mx-auto" src="/2024/assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary.png" style="max-width: 400px; display: block;" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <p><em>Decision boundary for the best model amongst three (\(\phi_1\), \(\phi_2\), \(\phi_3\)) with the lowest conditional joint marginal cross-entropy/information, as a function of dataset size and held-back size.</em> The three models \(\phi_1\), \(\phi_2\), and \(\phi_3\) correspond to different prior variances and noise levels. The white diagonal line shows where the conditional joint marginal information is computed using half the dataset size. In the region below this line, \(\phi_1\) (blue) has the lowest conditional joint marginal information, while \(\phi_2\) (orange) and \(\phi_3\) (green) are preferred for different dataset and held-back sizes.</p> </figcaption> </figure> <p>The plot shows which model is selected based on the lowest CJMI for different dataset sizes (x-axis) and conditioning set sizes (y-axis). The white line represents the case where half the data is used for conditioning (CJMI half in the previous plot). We observe that the model selection decision changes depending on the amount of available data and the size of the conditioning set/held-back data.</p> <h2 id="a-narrow-but-deep-dive-into-bayesian-model-selection-the-marginal-likelihood-and-generalization">A Narrow but Deep Dive into “Bayesian Model Selection, the Marginal Likelihood, and Generalization”</h2> <p>Now that we have introduced the necessary concepts and discussed the literature, let’s take a closer look at the paper by Lotfi et al. (2022)<d-cite key="lotfi2022bayesian"></d-cite>.</p> <h3 id="use-cases-and-pitfalls-of-the-lml">Use Cases and Pitfalls of the LML</h3> <p>Lotfi et al. (2022) present both the case for the log marginal likelihood (LML) as well as potential pitfalls when using it. They highlight the following use cases for the LML—<em>quoted and paraphrased from the paper</em>:</p> <ol> <li> <p><strong>Hypothesis testing:</strong> The LML provides an elegant mechanism to select between fixed prior hypotheses, even if each hypothesis is entirely consistent with observations. It automatically favors the most constrained hypothesis that fits the data, encoding a notion of Occam’s razor. The paper gives the example of the LML favoring general relativity over alternative explanations for Mercury’s orbit.</p> </li> <li> <p><strong>Hyperparameter learning:</strong> The LML is often successfully used in practice to learn hyperparameters of the prior, finding the hyperparameters \(\h\) that maximize \(\pof{\mathcal{D} \given \h}\), where \(\mathcal{D}\) is a dataset. The paper highlights Gaussian processes as a compelling example, where the LML chooses kernel hyperparameters that make the distribution over functions likely to generate the training data, rather than simply maximizing data fit. The LML can learn many kernel parameters and be used where cross-validation would be intractable.</p> </li> <li> <p><strong>Constraint learning:</strong> Unlike typical learning objectives like maximum likelihood, the LML is incentivized to select for constraints. It provides a consistent estimator for constraints, automatically selecting the most constrained solution that fits the data and collapsing to the true constraint value as the number of observations grows. Examples include the LML consistently estimating the true dimensionality in Bayesian PCA and automatically learning symmetries like rotation invariance.</p> </li> </ol> <p>However, the paper argues that the LML has several pitfalls for model selection and generalization:</p> <ol start="4"> <li> <p><strong>Not aligned with generalization:</strong> The LML answers “what is the probability a prior model generated the training data?” rather than “how likely is the posterior to have generated withheld points?”. A prior that initially explains the data well can still lead to a posterior that generalizes poorly.</p> </li> <li> <p><strong>Misaligned in model selection:</strong> The LML evaluates priors, while model selection should evaluate posteriors. Maximizing LML is not equivalent to selecting the best generalizing posterior.</p> </li> <li> <p><strong>Can overfit:</strong> The LML can favor “simple” priors concentrated around overfit maximum likelihood solutions that generalize poorly.</p> </li> <li> <p><strong>Underfitting bias in hyperparameter selection:</strong> The LML may not favor hyperparameters that make good parameters likely if they also make many poor parameters likely.</p> </li> </ol> <p>Relating these points to the previous discussions:</p> <p>For hypothesis testing and hyperparameter learning (<strong>1.</strong> &amp; <strong>2.</strong>), the LML favors the simpler hypothesis that converges faster, implying a smaller area under the learning curve. This aligns with the discussion on prior-data conflict for similarly misspecified models.</p> <p>At the same time, the paper also states about the case of Mercury’s orbit that:</p> <blockquote> <p>We emphasize here we are comparing fixed <em>prior</em> hypotheses. We are not interested in how parameters of general relativity update based on orbital data, and then deciding whether the updated general relativity is the correct description of orbital trajectories.</p> </blockquote> <p>This could be misconstrued at computing the marginal cross-entropy for the data under the prior, which is not what the LML is doing: it computes a joint marginal cross-entropy after all. The two questions in (<strong>4.</strong>) point to the joint and conditional marginal cross-entropies—the areas under the full and partial learning curves, respectively.</p> <p>However, neither LML nor CLML align with <em>static</em> evaluation, but rather with continued learning (<strong>5.</strong>).</p> <p>Points (<strong>6.</strong>) and (<strong>7.</strong>) relate to prior-data conflict and model misspecification when they are anti-correlated.</p> <p>Overall, all quantities can fail in the low-data regime. In the infinite data limit, model (mis-)specification dominates other factors, making the quantities less interesting.</p> <h3 id="the-conditional-marginal-likelihood-in-lotfi-et-al-2022">The “Conditional Marginal Likelihood” in Lotfi et al. (2022)</h3> <p>The paper introduces the conditional marginal likelihood (CLML) as a remedy for the pitfalls of the LML, matching the earlier definition of conditional joint marginal information:</p> \[\Hof{\xset{}{N-P+1}{N} \given \xset{}{1}{N-P}, \h}.\] <p>Unlike the LML which is invariant to data order, the CLML depends on how the data is split into a conditioning set and validation set. To make the CLML permutation-invariant, the paper proposes averaging over different permutations, equivalent to the joint marginal cross-entropy. However, this becomes computationally expensive, so the paper uses a single permutation with \(P=20\% \, N\) to ensure the posterior has sufficiently converged.</p> <aside class="box-note l-body"> <p>⚠️ The CLML matches the conditional joint marginal cross-entropy in its permutation-invarianet form, and thus exactly matches \(S_{CCV}(\xNset; P)\) from Fong and Holmes (2020)<d-cite key="fong2020marginal"></d-cite>.</p> </aside> <h3 id="estimating-the-clml-and-lml-via-laplace-approximation">Estimating the CLML and LML via Laplace Approximation</h3> <p>Computing the LML via sampling is intractable for deep neural networks. Estimating it from an uninformative prior leads to high-variance estimates, as most \(\w\) sampled from the prior will perform poorly on the data. While Monte Carlo sampling works well in high dimensions, it fails here because randomly sampling a good \(\w\) from the prior is incredibly unlikely, as illustrated in these tweets:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">How powerful is gradient descent exactly? For a small CNN on CIFAR-10 I&#39;ve looked at the typical loss change due to a random step of the same length as a gradient step starting at the same weights. The gradient step is literally a 185 sigma event =&gt; ~impossible~ at random ✅ <a href="https://t.co/oOQnkwCCG0">pic.twitter.com/oOQnkwCCG0</a></p>&mdash; Stanislav Fort (@stanislavfort) <a href="https://twitter.com/stanislavfort/status/1529865444701577216?ref_src=twsrc%5Etfw">May 26, 2022</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">How good is a gradient?<br/><br/>The top histogram shows the change in loss from 1000 random weight updates with a fixed norm. The bottom compares this histogram to the change in loss from a gradient descent step with the same norm.<br/><br/>It&#39;s 280 standard deviations away! <a href="https://t.co/iJ2SSESEJ0">pic.twitter.com/iJ2SSESEJ0</a></p>&mdash; Robert Rosenbaum RobertRosenbaum@neuromatch.social (@RobertRosenba14) <a href="https://twitter.com/RobertRosenba14/status/1517465854157500419?ref_src=twsrc%5Etfw">April 22, 2022</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>While sampling from the prior to estimate the LML is intractable, we can fare better when sampling from a posterior for computing a CLML, which is the approach taken by the paper for the CLML. The posterior is more concentrated around “good” \(\w\), and the paper uses a Laplace approximation to approximate it:</p> <aside class="box-note l-body"> <p>💡 A <strong>Laplace approximation (LA)</strong> estimates the posterior distribution by fitting a Gaussian on the second-order Taylor expansion of the MLE or MAP estimate <d-cite key="murphy2012machine"></d-cite>.</p> <p>The LA can be motivated by Bernstein-von Mises’ thereom as well, as it tells us that our posterior will concentrate around the maximum likelihood estimate in the infinite data limit (with the previously mentioned caveats).</p> </aside> <p>However, the LA only captures uncertainty around a single mode, underestimating the uncertainty before the model converges, as beautifully illustrated in the paper:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_fig3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_fig3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_fig3-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/bmsmlg_fig3.png" width="auto" height="auto" max-width=" " style="max-width: 35em; " onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This is especially relevant for overparameterized DNNs which have multiple diverse modes (<a href="https://arxiv.org/abs/2002.08791">Wilson, Izmailov, 2020</a><d-cite key="wilson2020bayesian"></d-cite>; <a href="https://cims.nyu.edu/~andrewgw/deepensembles/">2021, blog</a>).</p> <p>Furthermore, when computing the CLML, the LA may similarly struggle to find meaningful \(\w\) that perform well on the held-out data when that data would meaningfully change the model, as the CLML decomposes into conditional marginal information terms that condition on these additional data sequentially.</p> <h3 id="dnn-experiments-validation-loss-vs-clml">DNN Experiments: Validation Loss vs. CLML</h3> <p>The DNN experiments in Lotfi et al. (2022) compare the CLML to the validation loss for DNNs on CIFAR-10 and CIFAR-100 datasets. The results provide empirical evidence for the challenges of computing the CLML and beg the question whether these approximations are meaningfully different from a validation loss.</p> <p>The paper shows that while the CLML is better correlated with the generalization performance of the model than the LML, the validation loss is still better correlated with the generalization performance than the CLML. Interestingly, the initially published DNN experiments in the first arXiv version of the paper did not actually compute the CLML but instead computed the validation loss. This was fixed in the second arXiv revision.<d-footnote>This bug was found by yours truly, see the appendix of this post.</d-footnote></p> <p>However, given the previous discussions on the similarities between the CLML and cross-validation and difficulty of approximating the CLML meaningfully, this bug was not a major issue for the paper’s conclusions.</p> <p>Importantly, as we examine in the appendix of this post, when comparing the CLML using Monte Carlo sampling with the validation loss computed using Monte Carlo sampling for the Bayesian Model Average (BMA), the validation loss is still better correlated with the generalization performance than the CLML.</p> <h2 id="conclusion">Conclusion</h2> <p>In conclusion, this blog post has challenged the conventional focus on the marginal likelihood and related quantities for Bayesian model selection as a direct consequence of Occam’s razor. It highlights the importance of considering context and goals when choosing a model selection criterion. By motivating MLE and MAP using Occam’s razor and questioning the uniqueness of the (conditional) joint marginal likelihood, we hope to encourage critical thinking about the foundations of these quantities.</p> <p>However, it is important to acknowledge the limitations of our arguments and experiments. A more rigorous theoretical justification, a broader range of models and datasets, and a deeper engagement with philosophical implications are needed to strengthen the insights. As most of the presented methods ignore model complexity and assume a uniform model prior \(\pof{\h}\), we have not discussed it in the detail necessary, even though from the perspective of model description lengths (MDL), it would be crucial to take into account.</p> <p>Despite these limitations, our exploration of the connections between information-theoretic concepts and their behavior in different data regimes, along the lines of model misspecification and prior-data conflict, provides a necessary starting point for understanding recently proposed metrics.</p> <p>The toy experiment demonstrates that all discussed quantities can fail to reliably predict generalization under model misspecification and prior-data conflict, even for a basic setting using Bayesian linear regression. This emphasizes the need for caution when making claims about the superiority of any particular metric.</p> <p>Ultimately, the key takeaway is that there is no one-size-fits-all solution, and the choice of model selection criterion should be guided by a careful consideration of the specific context and goals at hand.</p> <hr/> <p><strong>Acknowledgements:</strong> We would like to thank the authors of the examined papers for their valuable contributions to the field and for inspiring this blog post, and to Freddie Bickford Smith for helpful comments and suggestions. Claude-3 and GPT-4 were used to edit and improve this blog post (via cursor.sh).</p> <p><strong>Reproducibility:</strong> The figures were created using matplotlib and seaborn in Python. The Bayesian linear regression model was implemented using numpy. The code for the toy experiment is available in this <a href="https://colab.research.google.com/drive/1rUnOvkFIxVrIJACxyjcQiGHo3nA77T4T?usp=sharing">Google colab</a>, and the code for the visualizations is available in this <a href="https://colab.research.google.com/drive/1q0esvQGSqd7d6zJfjbFcz-DGSKYi_WpC?usp=sharing">Google colab</a>.</p> <hr/> <h2 id="appendix">Appendix</h2> <h3 id="detailed-code-review-of-the-dnn-experiments-in-lotfi-et-al-2022">Detailed Code Review of the DNN Experiments in Lotfi et al. (2022)</h3> <p>The <a href="https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/main/Laplace_experiments/cifar"><code class="language-plaintext highlighter-rouge">logcml_</code> files in the repository</a> contain the code to compute the CLML for partially trained models. However, instead of computing</p> \[\begin{aligned} \log p(\mathcal D_{\ge m} \mid \mathcal D_{&lt; m}, \mathcal{M} ) \approx \log \sum_{k=1}^K \frac{1}{K}\, p(\mathcal{D}_{\ge m} \mid w_k, \mathcal M ) \\ = \log \sum_{k=1}^K \frac{1}{K}\, \prod_{j=m}^n p(y_j \mid x_j, w_k, \mathcal M ), \end{aligned}\] <p>the code computes:</p> \[\begin{aligned} &amp;\frac{1}{|\mathcal{D}_{\ge m}|}\,\sum_{j=m}^n \log p(\mathcal D_{j} \mid \mathcal D_{&lt; m}, \mathcal{M} ) \approx \\ &amp;\quad =\frac{1}{|\mathcal{D}_{\ge m}|}\,\sum_{j=m}^n \log \sum_{k=1}^K \frac{1}{K}\, p(y_j \mid x_j, w_k, \mathcal M ), \end{aligned}\] <p>which is the validation cross-entropy loss of the BMA (of the model trained with 80% of the training data).</p> <p>The high-level <a href="https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L295">code</a> that computes the CLML is:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre><span class="n">bma_accuracy</span><span class="p">,</span> <span class="n">bma_probs</span><span class="p">,</span> <span class="n">all_ys</span> <span class="o">=</span> <span class="nf">get_bma_acc</span><span class="p">(</span>
    <span class="n">net</span><span class="p">,</span> <span class="n">la</span><span class="p">,</span> <span class="n">trainloader_test</span><span class="p">,</span> <span class="n">bma_nsamples</span><span class="p">,</span> 
    <span class="n">hessian_structure</span><span class="p">,</span> <span class="n">temp</span><span class="o">=</span><span class="n">best_temp</span>
<span class="p">)</span>
<span class="n">cmll</span> <span class="o">=</span> <span class="nf">get_cmll</span><span class="p">(</span><span class="n">bma_probs</span><span class="p">,</span> <span class="n">all_ys</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><a href="https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L149"><code class="language-plaintext highlighter-rouge">get_bma_acc</code></a> marginalizes over the LA samples before returning <code class="language-plaintext highlighter-rouge">bma_probs</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="code"><pre><span class="p">[...]</span>
<span class="k">for</span> <span class="n">sample_params</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
    <span class="n">sample_probs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">all_ys</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="nf">vector_to_parameters</span><span class="p">(</span><span class="n">sample_params</span><span class="p">,</span> <span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
        <span class="n">net</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">()</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">sample_probs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">probs</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
            <span class="n">all_ys</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
        <span class="n">sample_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span><span class="n">sample_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">all_ys</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span><span class="n">all_ys</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">all_probs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">sample_probs</span><span class="p">)</span>

<span class="n">all_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">all_probs</span><span class="p">)</span>
<span class="n">bma_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">all_probs</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">bma_accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">bma_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">all_ys</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>

<span class="k">return</span> <span class="n">bma_accuracy</span><span class="p">,</span> <span class="n">bma_probs</span><span class="p">,</span> <span class="n">all_ys</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The important line is #18: <code class="language-plaintext highlighter-rouge">bma_probs = np.mean(all_probs, 0)</code> which marginalizes over the predictions and returns the BMA prediction for each sample.</p> <p>Finally, <a href="https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L170"><code class="language-plaintext highlighter-rouge">get_cmll</code></a> computes the validation loss for each sample independently (after applying a bit of label smoothing):</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">get_cmll</span><span class="p">(</span><span class="n">bma_probs</span><span class="p">,</span> <span class="n">all_ys</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
    <span class="n">log_lik</span> <span class="o">=</span> <span class="mi">0</span>      
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">all_ys</span><span class="p">):</span>
        <span class="n">probs_i</span> <span class="o">=</span> <span class="n">bma_probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">probs_i</span> <span class="o">+=</span> <span class="n">eps</span>
        <span class="n">probs_i</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probs_i</span><span class="p">)]</span> <span class="o">-=</span> <span class="n">eps</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">probs_i</span><span class="p">)</span>
        <span class="n">log_lik</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">probs_i</span><span class="p">[</span><span class="n">label</span><span class="p">]).</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">cmll</span> <span class="o">=</span> <span class="n">log_lik</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">all_ys</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">cmll</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The DNN experiments in Section 5 and Section 6 of the first arXiv revision of the paper (<em>v1</em>) thus did not estimate the CLML per-se but computed the BMA validation loss of a partially trained model (80%) and find that this correlates positively with the test accuracy and test log-likelihood of the fully trained model (at 100%). This is not surprising because it is well-known that the validation loss of a model trained 80% of the data correlates positively with the test accuracy (and generalization loss).</p> <h3 id="author-response-from-2022">Author Response from 2022</h3> <p>The following response sadly seems to target the first draft mainly. However, it is also helpful for the final blog post and provides additional context.</p> <blockquote> <p>Thanks for your interest in our paper and your comments. Here are our comments about the blog as it is currently framed:</p> <p>(1) Thank you for pointing out a bug in the CLML computation for Figure 5b. We note that this bug is only relevant to a single panel of a single figure in the main text. We have re-run this experiment with the right CLML, and the results, attached here, are qualitatively the same. In summary, it was a very minor part of the paper, and even for that part it did not affect the take-away. We also attach the results of the correlation between the BMA test accuracy and the negative validation loss. You suggest in your post that the validation loss might correlate better with the BMA test accuracy than the CLML given that we use 20 samples for NAS. Our empirical results show the opposite conclusion. Additionally, we are not suggesting the CLML as a replacement to cross-validation but rather as a minor way to modify the LML for improvements in predicting generalization. Finally, we attach results for different sample sizes (20 samples vs. 100 samples) to address your comments on the sample size used to estimate the CLML. As we can see in the figure, the Spearman correlation factor is quite similar. 20 samples appears to provide a reasonable estimate of the CLML for these purposes, and is different from validation loss.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_1-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/rebuttal_1.png" width="auto" height="auto" max-width=" " style="max-width: 20em; " onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_2-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/rebuttal_2.png" width="auto" height="auto" max-width=" " style="max-width: 20em; " onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_3-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/rebuttal_3.png" width="auto" height="auto" max-width=" " style="max-width: 20em; " onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>(2) Your post currently opens by suggesting that there is something wrong with our experiments, likely either an LML approximation or a CLML issue, because we note that the LML correlates more poorly with generalization for larger datasets (where “large” is relative in the context of a specific experiment). A few points here: (i) this result is actually completely expected. The LML is in fact non-monotonic in how well it predicts generalization. For small datasets, the prior should be reasonably predictive of generalization. For intermediate datasets, the first terms in the LML decomposition have a negative effect on the correlation with generalization. For asymptotically large datasets, the first terms have a diminishing effect, and we get a consistent estimator; (ii) almost all of our experiments are exact, and we see this behaviour in the exact experiments for the Fourier model. For example, for the Fourier feature experiment in Fig 4(d), LML picks the better generalizing model for n &lt; 50 and n &gt; 296. For n in [50, 296] it picks the wrong model. For large neural network models, it is reasonable that the exact LML could pick the wrong model for CIFAR-sized datasets. (iii) any potential issues with the CLML are not relevant to these considerations, which are about the behaviour of the LML.</p> <p>(3) Your post currently suggests that issues with approximate inference could be responsible for our take-aways, rather than issues with the LML in general. But as we note in (2), almost all of our experiments use the exact LML and CLML: the density model, Fourier features, Gaussian processes, and deep learning exps on DKL, and there was never any bug associated with CLML computation in these experiments. The takeaways for the Laplace experiments are consistent with the exact experiments, and also expected, as above. While it’s true that the CLML can be estimated more effectively than the LML for the Laplace experiments, this is actually an advantage of the CLML that we note in the paper. The LML results also stand on their own, as we discuss above.</p> <p>(4) Your post places a lot of importance on Figure 5, as if it is the main result of the paper and our main “DNN” experiments. We stand by the results of Figure 5, but it is a relatively minor component of the paper. As we’ve mentioned most of our results are exact, including our DKL experiments, which are certainly the most substantial DNN experiments, with practically exciting results for transfer and few-shot learning. The DKL experiments are actually where we expect the CLML to be practically useful, and currently they seem to be overlooked in the post.</p> <p>(5) The blog seems to question the learning curve experiments, but these experiments in Figure 4 are exact, with no Laplace approximation, and relatively straightforward.</p> <p>(6) Your post seems to be negative about the CLML, presenting its similarity with cross-validation as a potential drawback, and implying the skepticism about the CLML should affect the interpretation of our take-aways. Two points here: (i) as above, the CLML is independent of most of our take-aways, which are about the properties of the LML; (ii) our goal with the CLML was not to introduce something starkly different from cross-validation, but to show how a very minor modification to the LML could improve alignment with generalization. Moreover, the DKL CLML results are quite promising as an efficient way to do gradient based estimation of a large number of hyperparameters.</p> <p>(7) The blog opens as if it is leading up to some fatal flaw. But as above, (i) the LML considerations are independent of the CLML, (ii) most of the experiments are exact, (iii) the trends for the exact and approximate inference procedures are the same and are naturally understandable and explainable, such as the non-monotonic trend in how well the LML correlates with generalization, and (iv) the CLML bug only affected Figure 5, panel b, and when it’s corrected the qualitative take-away is the same as before.</p> <p>We appreciate your interest and effort in reading the paper, and we think your questions will improve the clarity of the paper, which we have updated with an acknowledgement to you. Given the above considerations, we do think there would need to be substantial revisions to the blog post to accurately and fairly reflect the paper. We would appreciate being able to see the revisions before it’s posted.</p> <p>Best wishes,<br/> Sanae, Pavel, Greg, Micah, Andrew</p> </blockquote> <h3 id="ablation-clml-vs-bma-validation-loss-vs-non-bma-validation-loss">Ablation: CLML vs. BMA Validation Loss vs. (non-BMA) Validation Loss</h3> <p>Let us examine the new results:</p> <p>In the three panels below, two panels show test accuracy vs. validation loss; one shows test accuracy vs. CLML. The left-most panel is the BMA test accuracy vs. (negative) BMA validation loss, the middle panel is vs. the CLML, and the right-most panel is vs. the (negative) non-BMA validation loss.</p> <p>Note that the left-most panel is from <em>v1</em>, which was accidentally computing the BMA validation loss, and whose axis label is adapted here from <em>v1</em> for clarity. The two other plots are from <em>v2</em> after fixing the bug. See commits <a href="https://github.com/Sanaelotfi/Bayesian_model_comparison/commit/a579aa292723dc20a6105ec8f4fff1045dd9a9fd">here</a> for fixing the CLML estimation and <a href="https://github.com/Sanaelotfi/Bayesian_model_comparison/commit/3fa8ca2ecb314ee881f6c95a602ef58b9ccd3620">here</a> for computing the non-BMA validation loss.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_bma_validation_loss.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_bma_validation_loss.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_bma_validation_loss.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/bmsmlg_bma_validation_loss.svg" class="img-fluid" width=" " style="width: 20em; " height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">BMA Neg Validation Loss</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_clml.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_clml.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_clml.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/bmsmlg_clml.svg" class="img-fluid" width=" " style="width: 20em; " height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">CLML</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_validation_loss.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_validation_loss.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_validation_loss.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/bmsmlg_validation_loss.svg" class="img-fluid" width=" " style="width: 20em; " height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Validation Loss</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_plot_legend.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_plot_legend.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_plot_legend.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/bmsmlg_plot_legend.svg" class="img-fluid" width=" " style="width: 5em; " height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Leg</figcaption> </figure> </div> </div> <p>At first glance, there might be an observer effect in the experiments for the validation loss. The BMA validation loss in <em>v1</em> performs better than the CLML in <em>v2</em>, while the non-BMA validation loss in <em>v2</em> underperforms the CLML in <em>v2</em>. When asked about it, the authors pushed the respective code (see link above) and explained that the updated, right-most panel computes the <strong>non-BMA</strong> validation loss, i.e., without LA samples. It seems surprising that there is such a difference between the non-BMA validation loss and BMA validation loss: <em>the non-BMA validation loss is more than one nat worse on average than the BMA validation loss, based on visual inspection</em>. Note that the plots here and in the paper compute the average CLML and average validation loss and are thus directly comparable.</p> <p>The authors said in their response that:</p> <blockquote> <p>You suggest in your post that the validation loss might correlate better with the BMA test accuracy than the CLML given that we use 20 samples for NAS. Our empirical results show the opposite conclusion.</p> </blockquote> <p>This is only partially true. The BMA validation loss (which was accidentally computed in <em>v1</em> instead of the CLML) correlates very well with the BMA test accuracy. This is not surprising given that this is the frequentist purpose of using validation sets. If validation sets were not correlating well with the test accuracy, we would not be using them in practice. 🤗 As such, this raises the question why the non-BMA validation loss correlates negatively with the BMA test accuracy for ResNets and overall in the <em>v2</em> results. Thus, only the non-BMA validation loss supports the now opposite conclusion in <em>v2</em> of the paper and in the authors’ response.</p> <p>Yet what is also surprising is how well the BMA validation loss does vs. the CLML:</p> <aside class="box-error l-body"> <p>🔥 The BMA validation loss correlates even better than the CLML with BMA test accuracy: the Spearman’s rank correlation is better for the BMA validation loss than for the CLML across all \(\lambda\)s.</p> <p>Does this mean we should use the BMA validation loss rather than estimating the CLML for DNNs?</p> </aside> <h3 id="ablation-la-sample-size">Ablation: LA Sample Size</h3> <p>Secondly, when we compare the reported values between BMA validation loss and CLML, we notice that the CLML is lower than the BMA validation loss by half a nat for \(\lambda=10^2\) and generally for CNNs.</p> <aside class="box-note l-body"> <p>👉 One would expect that the (average) CLML to be better (higher) than the negative BMA validation loss and not worse (lower) because the CLML ought to be able to compress the validation set better using the joint predictive distribution than the marginal predictive distribution for the validation loss.</p> <p>Intuitively, the CLML can take the validation data into account in a way that the validation loss cannot given the sequential conditioning of the former.</p> <p>Hence, the CLML should generally be lower-bounded by the (BMA) validation loss and upper-bounded by the (BMA) test loss up to sample variance, assuming there are not many outliers in the data. The fact that the CLML is worse points toward the LA samples not being able to capture the implicit posterior in the joint predictive distribution. (This is a bit handwavy but is a reasonable interpretation, as we will see below when taking into account additional evidence.)</p> </aside> <p>However, it seems, even though the new experiments in <em>v2</em> are supposed to reproduce the ones from <em>v1</em>, and we can assume that the same model checkpoints were used for re-evaluation (as retraining is not necessary), both CLML and non-BMA validation loss are off by about half a nat for the CNNs. As such, the above consideration might hold but might not provide the answer here.</p> <p>Instead, we overlay the non-BMA validation loss and the CLML plots, both from <em>v2</em>, with a “difference blend”: it shows the absolute difference between the colors for overlapping data points (the circles 🔴 and triangles 🔺), leading to black where there is a match, negative (green-ish) color for CLML, and positive (sepia) color for validation losses. The background grids were used to match the plots, but we hid the ones from CLML afterward—as such, the strong overlay is because the values are so close.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_difference_overlay_plot.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_difference_overlay_plot.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/bmsmlg_difference_overlay_plot.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/bmsmlg_difference_overlay_plot.svg" class="img-fluid" width=" " style="width: 25em; " height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Surprisingly—or rather as predicted when the LA does not really do much—it turns out that the validation loss for the CNNs (🔴) mostly fully matches the estimated CLML with 20 LA samples following a visual inspection. To be more precise, either the models have already sufficiently converged, or the CLML estimate is not actually capturing the correlations between points and thus ends up being very similar to the validation loss.</p> <aside class="box-warning l-body"> <p>As mentioned before, it could be insightful to actually compute the difference between CLML and BMA validation loss explicitly. Two things are not clear to me given the results in <em>v1</em> and <em>v2</em>:</p> <ol> <li> <p>Why is there a difference in nats between BMA validation loss in <em>v1</em> and the non-BMA validation loss and CLML in <em>v2</em>? Given that CLML matches the non-BMA validation loss for CNNs in <em>v2</em>, the BMA validation loss ought to be close to that, too.</p> </li> <li> <p>Why is there different behavior for ResNets (🔺) in the BMA validation loss in <em>v1</em> and the CLML in <em>v2</em> compared to the non-BMA validation loss in <em>v2</em>? The low correlation coefficients of the non-BMA validation loss seem to come from that, as <em>v2</em> also explains in an added paragraph in appendix H.</p> </li> </ol> </aside> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/rebuttal_3-1400.webp"/> <img src="/2024/assets/img/2024-05-07-clml/rebuttal_3.png" class="img-fluid" width=" " style="width: 25em; " height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This changes the interpretation of the sample ablation in the author’s response. The ablation shows no difference between 20 and 100 LA samples, with 100 LA even samples having a slightly lower rank correlation. So it seems 5 times more LA samples are not sufficient to make a difference, or the Laplace posterior cannot capture the posterior as well as hoped. It would be interesting to examine this further. Kirsch et al (2022)<d-cite key="kirsch2022marginal"></d-cite> reported running toy experiments on MNIST with 10,000 MC Dropout samples without achieving good adaptation. Laplace approximation is not MC Dropout, and this is speculation, but it seems in agreement. Notwithstanding the compute cost and feasibility, could posterior samples using HMC or similar more principled methods provide better estimates?</p> <p>All in all, given the above, it is fair to say that the estimate of the CLML is probably not as good as hoped, and further experiments might be needed to tease out when the CLML provides more value than the (BMA) validation loss. Note, however, that this question has not been explicitly examined in the paper. Instead, for DNNs, the paper only compares LML and CLML with distinct estimation methods.</p>]]></content><author><name>Andreas Kirsch</name></author><category term="Bayesian Neural Network"/><category term="Generalization"/><category term="Log Marginal Likelihood"/><category term="Conditional Log Marginal Likelihood"/><category term="Information Theory"/><category term="Model Evaluation"/><category term="Model Selection"/><summary type="html"><![CDATA[Bayesian model selection has long relied on the marginal likelihood and related quantities, often motivated by the principle of Occam's razor. Following the paper 'Bayesian Model Selection, the Marginal Likelihood, and Generalization' by Lotfi et al. (2022), this blog post critically examines the conventional focus on the marginal likelihood and related quantities for Bayesian model selection as a direct consequence of Occam's razor. We find that the suitability of these criteria depends on the specific context and goals of the modeling task. We revisit the concepts of log marginal likelihood (LML), cross-validation, and the recently introduced conditional log marginal likelihood (CLML), highlighting their connections and differences through an information-theoretic lens. Through thought experiments and empirical observations, we explore the behavior of these model selection criteria in different data regimes under model misspecification and prior-data conflict, finding that the conditional marginal cross-entropy, closely related to cross-validation, is often more reliable for optimizing generalization performance. We review relevant literature, compare the CLML and validation loss for deep neural networks, and using a toy Bayesian linear regression, we demonstrate that all the discussed quantities can fail to reliably predict generalization. Our takeaways are that: there is no one-size-fits-all solution; the choice of model selection quantity depends on the specific context and goals; and in the future, we should take into account model complexity as well and not assume a uniform model prior. While this work leaves scope for more rigorous theoretical justification and more wide-ranging empirical investigation (along with deeper engagement with philosophical implications), it nevertheless provides grounds for questioning the primacy of the (conditional) log marginal likelihood and encourages critical thinking about its foundations, aiming for a more nuanced understanding of Bayesian model selection.]]></summary></entry><entry><title type="html">Deep Equilibrium Models For Algorithmic Reasoning</title><link href="https://iclr-blogposts.github.io/2024/blog/deqalg-reasoning/" rel="alternate" type="text/html" title="Deep Equilibrium Models For Algorithmic Reasoning"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/deqalg-reasoning</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/deqalg-reasoning/"><![CDATA[<h2 id="what-is-algorithmic-reasoning">What is Algorithmic Reasoning?</h2> <p>Broadly, algorthmic reasoning <d-cite key="velickovic2020neural"></d-cite> studies how well neural networks can learn to execute classical computer science algorithms. In particular to measure how well an algorithm has been learned we look at size-generalisation, i.e. if we train on inputs of size \(N\) and check how well the Neural Network perform on inputs of size \(2N\) or \(10N\). The idea is that neural networks often learn shortcuts that work well in-distribution, but fail out-of-distribution, whereas classical computer science algorithms work no matter the input size. The purpose of this exercise is to study the generalisation of reasoning tasks, especially what tricks help to improve robustness and get the network closer to deducing logically rather than relying on statistical short cuts.</p> <h2 id="why-care-about-fixed-points">Why care about fixed-points?</h2> <p>First, let’s remember that for \(x_0\) to be a fixed-point of a function \(f\) it must satisfy \(f(x_0) = x_0\). Secondly, we can observe that many algorithms consist of an update rule that you apply until there is no more change. The final output can easily be seen to be a fixed-point! In a classical computer science algorithm some smart person will have sat down and shown that under some conditions on the input this convergence will happen and the final answer is correct.</p> <p>An example algorithm would be the Bellman-Ford algorithm to compute the shortest-distance to a given node in a graph. Here the update rule looks like \(x_i^{(t+1)} =\min(x_i^{(t)}, \min \{x_j^{(t)} + e_{ij}\}_{j\in N(i)})\), where \(x_i^{(t)}\) is the shortest distance estimate to the source node at time \(t\), \(e_{ij}\) is the distance between nodes \(i\) and \(j\), and \(\{j\}_{j\in N(i)}\) are the neighbours of node \(i\). The algorithm says to apply this rule until there is no more change—a fixed point.</p> <p>Interestingly, denotational semantics<d-cite key="scott1970"></d-cite>—a theoretical field of computer science—has shown you can represent Turing complete programming languages as mathematical functions. This is mostly quite trivial with the exception of the while loop (which is also the key ingredient to make it Turing complete). Here the trick is a special mathematical operator that returns the minimum fixed point of a function! (If there is no fixed point to a function then the corresponding while loop doesn’t terminate.) And thus we can see that fixed-points are reached by all programs that terminate, and yet they aren’t used in neural networks that try to learn how to do reasoning. A missed inductive bias perhaps?</p> <h2 id="the-details">The details</h2> <h3 id="task-specification">Task specification</h3> <p>The CLRS paper<d-cite key="velickovic2022clrs"></d-cite> provides us with a benchmark dataset for algorithmic reasoning. The general structure of the data is a sequence in time of intermediate states of a given algorithm. In other words, at timestep \(t\) we have a state \(x_t\) that describes various variables that the algorithm stores, e.g. in BellmanFord \(x_t\) will contain the current estimate of the shortest path in each node of the graph. At each timestep \(t\) we then try to predict the next time step, we do this by outputting some \(y_t\) from which we can extract \(x_{t+1}\). Note that \(y_t\) may be slightly different from \(x_{t+1}\), for instance because it has some state may never change by definition, e.g. the graph in BellmanFord, hence we don’t predict it again. This is all illustrated in the next figure, where we split the state into a state at each node \(x\) and at each edge \(e\) for a given graph \(G\) as an example.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/alg-reasoning-task-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/alg-reasoning-task-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/alg-reasoning-task-1400.webp"/> <img src="/2024/assets/img/2024-05-07-deqalg-reasoning/alg-reasoning-task.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Algorithmic Reasoning Task, diagram recreated from <d-cite key="velickovic2020neural"></d-cite> </div> <h3 id="the-architecture">The architecture</h3> <p>The high-level architecture is that of an encoder-processor-decoder. The motivation is that neural networks perform well in high-dimensional spaces but that classical algorithms tend to operate on very low-dimensional variables, e.g. in BellmanFord the shortest distance would be a single scalar. Thus the encoder projects the state into a high-dimensional space \(z_t\) where the main computation is then done by the processor network—typically a Graph Neural Network. The output of the processor \(z_{t+1}\) is then decoded back into the low-dimensional space by the decoder. The encoder and decoders mostly consist of linear layers with the occasional exception, e.g. a softmax for categorical variables. The processor will be a graph neural network, for which several different architectures have been explored, for example in<d-cite key="ibarz2022generalist"></d-cite>. We either use the TripletMPNN from<d-cite key="ibarz2022generalist"></d-cite> which adds edge message passing or a simple MPNN with a linear message layer.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/architecture-1400.webp"/> <img src="/2024/assets/img/2024-05-07-deqalg-reasoning/architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> High-level architecture employed </div> <p>The processor is supposed to do the main computation of the network, in particular, the hope is that one iteration of the processor is equal to one iteration of the algorithm. In our example of BellmanFord, it would be one iteration of the update rule \(x_i^{(t+1)} =\min(x_i^{(t)}, \min \{x_j^{(t)} + e_{ij}\}_{j\in n(i)})\) (see also the Figure below). Thus, the processor should indicate termination by no longer changing it’s output \(z\).</p> <h3 id="training">Training</h3> <p>Traditionally the training approach has been teacher-forcing. In teacher forcing we train each step of the algorithm independently by feeding the network the ground-truth \(x_t\) and computing the loss against \(y_t\) at all \(t\) simultaneously. This requires us to know the exact number of steps in the algorithm a priori. In other words, training with just teacher forcing will require us to tell the network the number of iterations it should run for at test time (which will vary depending on the input state). This is unrealistic in practice, where we would simply give our neural network the input state and ask it to run the algorithm on its own, which includes knowing when to stop the computation. While a termination network is suggested in <d-cite key="velickovic2020neural"></d-cite>, the issue is ignored in later papers such as <d-cite key="ibarz2022generalist"></d-cite>.</p> <p>Remember that neural networks are really good at learning in-distribution shortcuts. To more rigorously test whether the neural network has learned the underlying logical algorithm we introduce a shift between the training and test distribution. If the network has learned the classical algorithm, it should be able to overcome this shift. Throughout the CLRS algorithmic reasoning benchmark size generalisation is used, i.e. we train on examples of size 16 (i.e. the graph has 16 nodes) and at test time we will use an input size of 64.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/BFexplained-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/BFexplained-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-deqalg-reasoning/BFexplained-1400.webp"/> <img src="/2024/assets/img/2024-05-07-deqalg-reasoning/BFexplained.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An example algorithm: Bellman-Ford <d-cite key="velickovic2022clrs"></d-cite> </div> <h2 id="how-can-we-do-fixed-points-in-dnns">How can we do fixed-points in DNNs?</h2> <p>One approach to training neural networks that run until they reach a fixed point is deep equilibrium models<d-cite key="bai2019deep"></d-cite> (DEQ). We give a brief introduction to this approach next based on the blogpost <d-cite key="baiblog"></d-cite>.</p> <p>Given our input \(x\), our hidden state \(z\), and our processor \(f\), the goal is to optimise the fixed point \(z^*=f(z^*,x)\) we reach. The question how can we backprop through \(z^* = f(z^*,x)\).</p> <p>In backprop, we ultimately want to compute</p> \[\left(\frac{\partial z^*(.)}{\partial(.)}\right)^{\top} g\] <p>for some incoming gradient \(g\) from the layers after (in our case from the decoder) and \((.)\) being anything we want, but usually the weights of the network. We can show by implicit differentation of \(z^* = f(z^*,x)\) that</p> \[\left(\frac{\partial z^*(.)}{\partial(.)}\right)^{\top} g = \left(\frac{\partial f(z^*, x)}{\partial (.)}\right)^{\top}\left(I-\frac{\partial f(z^*, x)}{\partial z^*}\right)^{-\top}g\] <p>The difficult to term to solve in the above equation is \(\left(I-\frac{\partial f(z^*, x)}{\partial z^*}\right)^{-\top}g\), which is the solution of a linear system, namely:</p> \[\left(I-\frac{\partial f(z^*, x)}{\partial z^*}\right)^{\top}h = g\] <p>In general, we can try to solve it in two ways, use a linear system solver, like can be found torch.linalg, or by computing a fixed point to</p> \[h = \left(\frac{\partial f(z^*, x)}{\partial z^*}\right)^{-\top}h +g\] <p>In the DEQ blogpost <d-cite key="baiblog"></d-cite> they suggest solving the above fixed point. The reason to use implicit differentiation is that backpropagating through time may easily run into exploding or vanishing gradients or error accumulation due to the number of steps needed to reach a fixed point.</p> <p>We tried both: solving the linear system with torch.linalg.solve and finding the above fixed point. But we converged to computing the fixed point of the equation above as suggested by the deep equilibrium blogpost as it is computationally faster, while the added accuracy of linear system solvers wasn’t beneficial. Note this trade-off is heavily informed by what is readily implemented in PyTorch to run on GPU, hence the balance may shift in the future.</p> <h3 id="tricks-we-employ">Tricks we employ</h3> <p>To encourage convergence we change the update function in the MPNN<d-cite key="gilmer2017neural"></d-cite> to be a minimum update, i.e. \(z^{(t+1)} = \min(z^{(t)}, z^{'(t+1)})\). This update rule is motivated by the problem of getting neural networks to converge to a fixed point. We discuss the effect of this in more detail after the experimental section.</p> <p>Currently, gradient flows through the implicit differentiation explained above as well as back in time through standard backprop via \(z_t\). To enable more ways for the gradient to inform early steps in the algorithm, we propagate the gradient through \(y_t\) as well. For discrete \(y_t\), in other words, for categorical variables in the state \(x_t\) we employ the Rao-Blackwell straight-through gumbel softmax estimator<d-cite key="paulus2020raoblackwellizing"></d-cite> to allow gradients to flow.</p> <p>Finally, we also try adding a loss for the number of steps by adding the penalty \(\sum_{t=0}^{T} \|z_{t+1} - z_{t}\|^2\). The penalty will be larger as we take more steps and stay away from the fixed point, thus hopefully encouraging convergence to a fixed point more quickly.</p> <h2 id="how-well-does-it-work">How well does it work?</h2> <p>In the table below we show the accuracy<d-footnote>What exactly is measured for the accuracy depends on each algorithm, but usually is a pointer, e.g. in the Bellman-Ford algorithm it is a pointer to the previous node along the shortest path. For more details see the CLRS Benchmark paper.</d-footnote> of the algorithms when tested on graphs of size 64.</p> <p>DEQ is our approach of reaching a fixed point together with the implicit differentiation explained above. Hint propagation is simply reaching a fixed point and back propagating through time with no implicit differentiation. Teacher forcing is used for the baselines, where the first number is the simple MPNN architecture<d-cite key="gilmer2017neural"></d-cite> and the second number is the more complex TripletMPNN <d-cite key="ibarz2022generalist"></d-cite> (these numbers are taken from the paper <d-cite key="ibarz2022generalist"></d-cite>). For BellmanFord and BFS we use the simple MPNN and for all others we use the TripletMPNN.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">DEQ</th> <th style="text-align: center">Hint propagation</th> <th style="text-align: center">Teacher forcing</th> </tr> </thead> <tbody> <tr> <td>BellmanFord*</td> <td style="text-align: center">96.4%</td> <td style="text-align: center">96.7%</td> <td style="text-align: center">92%/97%</td> </tr> <tr> <td>Dijkstra</td> <td style="text-align: center">78.8%</td> <td style="text-align: center">84.4%</td> <td style="text-align: center">92%/96%</td> </tr> <tr> <td>BFS*</td> <td style="text-align: center">53.8%</td> <td style="text-align: center">57.1%</td> <td style="text-align: center">100%/100%</td> </tr> <tr> <td>DFS</td> <td style="text-align: center">5.0%</td> <td style="text-align: center">4.7%</td> <td style="text-align: center">7%/48%</td> </tr> <tr> <td>MST-Kruskal</td> <td style="text-align: center">82.3%</td> <td style="text-align: center">82.3%</td> <td style="text-align: center">71%/90%</td> </tr> <tr> <td>MST-Prim</td> <td style="text-align: center">75.2%</td> <td style="text-align: center">50.4%</td> <td style="text-align: center">71%/90%</td> </tr> </tbody> </table> <p>As we can see in the table above the approach works very well for simpler algorithms such as BellmanFord, where with simple MPNN we manage to achieve equal or better accuracy than the simple MPNN and match the TripletMPNN. Interestingly, this is a parallel algorithm, i.e. all node representations run the same code, in constrast sequential algorithms which go through the graph node by node. We did try gating to enable the GNN to better mimic a sequential algorithm, but this didn’t help.</p> <p>On the other algorithms while we are able to learn we cannot match the performance of teacher forcing where we assume to know the number of timesteps to run the neural network. This additional help makes the comparison slightly unfair, however, it shows how learning a fixed point is difficult for the network as we are not able to match the performance. We hypothesise about the reasons behind this in the next section.</p> <h2 id="whats-the-problem">What’s the problem?</h2> <p>There are a few major issues that we notice during training. The first is that the network is prone to underfitting, while we only show the test accuracy in the table above the training error doesn’t actually reach 0. It is unclear what causes this, however, trying to solve some issues with the DEQ may solve this. So let’s delve into them.</p> <h3 id="convergence-is-a-key-issue">Convergence is a key issue</h3> <p>Firstly, the network will often take a large number of steps to reach a fixed point. We can see on easier algorithms like the BellmanFord algorithm that the number of forward steps during training often reaches our set upper limit of 64 forwards steps (the actual algorithm would take on average 4-5, max 10 for this graph size). This is why we implement our architecture trick, where we update the next hidden representation only if it is smaller than the current one, i.e. \(z^{(t+1)} = \min(z^{(t)}, z^{'(t+1)})\) where \(z^{'(t+1)}\) is the output of our min aggregator in the message passing step (alternatives such as gating and an exponential moving average update function were also tried). This helps with convergence, which enables finding a fixed point in simple cases, but fails to work reliably for more complex architectures and problems, while also introducing a different issue.</p> <h3 id="the-problem-with-hard-constraints-to-achieve-convergence">The problem with hard constraints to achieve convergence</h3> <p>Remember that during the implicit differentiation we are trying to solve</p> \[h = \left(I-\frac{\partial f(z^*, x)}{\partial z^*}\right)^{-\top}g\] <p>i.e. in the linear system \(y = Ax\) our matrix \(A\) is equal to \(I-J\) where \(J\) is the Jacobian in the above equation. If the Jacobian is equal to the identity then our matrix $A=0$ and our system has no solution. In practice, \(z^{(t+1)} = \min(z^{(t)}, z^{'(t+1)})\) will reduce to \(f(z) = z\) in many dimensions of \(z\). This leads to many rows of the Jacobian being the identity due to the function effectively becoming \(f(x)=x\) in many dimensions. Thus leading to rows that are entirely zero in \(A\), which is ill-defined and has no solution causing the optimisation to break.</p> <p>One solution is to try a soft-min, i.e. \(softmin_{\tau}(a,b) = \frac{ae^{-a/\tau}+be^{-b/\tau}}{e^{-a/\tau}+e^{-b/\tau}}\). Here we get the ability to trade off between convergence and the Jacobian being interesting. For \(\tau&lt;&lt;1\) we basically recover the min operation and for \(\tau&gt;&gt;1\) we simply get an average, i.e. an exponential moving average. In practice, there was not a trade-off for which we consistently have an interesting Jacobian, while also converging sufficiently fast.</p> <h2 id="what-do-we-take-away">What do we take away?</h2> <ol> <li>Training to reach a fixed point can work as way to determine when to stop reasoning. But it gets increasingly more difficult as the underlying problem gets harder.</li> <li>It’s unclear what inductive bias to choose in order to ensure fast enough convergence to a fixed point. There are downsides such as uninformative gradients at the fixed point.</li> <li>Optimisation is tricky and stands in the way. In particular, with implicit differentiation through the fixed point.</li> </ol>]]></content><author><name>Sophie Xhonneux</name></author><summary type="html"><![CDATA[In this blogpost we discuss the idea of teaching neural networks to reach fixed points when reasoning. Specifically, on the algorithmic reasoning benchmark CLRS the current neural networks are told the number of reasoning steps they need, which they shouldn't be given. While a quick fix is to add a termination network that predicts when to stop, a much more salient inductive bias is that the neural network shouldn't change its answer any further once the answer is correct, i.e. it should reach a fixed point. This is supported by denotational semantics, which tells us that while loops that terminate are the minimum fixed points of a function. We implement this idea with the help of deep equilibrium models and discuss several hurdles one encounters along the way. We show on several algorithms from the CLRS benchmark the partial success of this approach and the difficulty in making it work robustly across all algorithms.]]></summary></entry><entry><title type="html">Building Diffusion Model’s theory from ground up</title><link href="https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/" rel="alternate" type="text/html" title="Building Diffusion Model’s theory from ground up"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/"><![CDATA[<h2 id="introduction">Introduction</h2> <h3 id="motivation">Motivation</h3> <p>Not only generative modeling has been around for decades, few promising model families emerged and dominated the field for several years in the recent past. VAEs<d-cite key="vae_kingma"></d-cite> dominated the generative modelling landscape from 2014 onwards, until GANs<d-cite key="GAN_goodfellow"></d-cite> took off in 2015-16; Normalizing Flows (NF)<d-cite key="normalizingflow"></d-cite> never really made it to the mainstream generative modeling due to its restrictive architectural requirement. However, it is quite clear at this point that the magnitude of impact they made is relatively less than barely 2-3 years of Diffusion Models. It is mostly attributed to one of the seminal papers (by Jonathan Ho et al.<d-cite key="diffusionmodel_ho"></d-cite>), now popularly referred to as “Denoising Diffusion Probabilistic Models” or DDPM. With the exponential explosion of works following DDPM, it is very hard, or rather unnecessary to look beyond this pivotal point.</p> <p>In this article, we look back into the conceptual and theoretical ideas that were in development for a long time, even outside the field of core machine learning. We will show in a later sections that, some of the theoretical ‘pillars’ holding Diffusion Models, have their roots deep into statistical physics and other fields. A significant part of this theory was presented afresh in the ICLR paper<d-cite key="song2021scorebased"></d-cite> (won best paper award). Lastly, even though the ideas presented in this article are quite theoretical, we made our best attempt to convey them with intuitive explanations, diagrams and figures, thereby expanding its potential audience. To encourage further exploration, we provide all codes used in producing the figures (and experiments) of this article in <a href="https://github.com/dasayan05/iclr24_blog_code"><span style="color:blue;">this repository</span></a>.</p> <p>This article notes that, historically, there were two distinct roads of development that merged in order for modern diffusion models to emerge – “scalable estimation of score” and “using the score for generative modelling”. The former is relatively short, while the latter traces its origin back to ~1900, if not earlier. This article explores these two paths independently – the latter one first while assuming the knowledge of the former. Rest of this introductory section is spent on defining the general modelling problem and the very notion of ‘score’ – the primary quantity of interest. The next section deals with how we can use score in generative modelling, assuming access to an oracle for the true score. The last section dives solely into the problem of estimating the score in a scalable manner. It is worth mentioning that, in this article, we explain only the “sufficient and necessary” concepts needed to build the diffusion model framework and hence may not directly resemble the typical formalism seen in most papers.</p> <h3 id="generative-modeling">Generative Modeling</h3> <p>The problem of generative modeling, in most cases, is posed as <em>parametric density estimation</em> using a finite set of samples \(\{ x^{(n)} \}_{n=1}^N\) from a “true but unknown” data distribution \(q_{data}(x)\). With a suitable model family chosen as \(p_{\theta}(x)\), with unknown parameters \(\theta\), the problem boils down to maximizing the average (log-)likelihood (w.r.t \(\theta\)) of all the samples under the model</p> \[\theta^* = arg\max_{\theta} \mathbb{E}_{x \sim q_{data}(x)} \left[ \log p_{\theta}(x) \right] \approx arg\max_{\theta} \frac{1}{N} \sum_{n=1}^N \log p_{\theta}(x^{(n)})\] <p>It turned out however, that defining an arbitrary parametric density \(p_{\theta}(x)\) is not as easy as it looks. There was one aspect of \(p_{\theta}\) that is widely considered to be the evil behind this difficulty – <em>the normalizing constant</em> that stems from the axiom of probability</p> \[p_{\theta}(x) = \frac{\tilde{p}_{\theta}(x)}{\color{purple} \int_x \tilde{p}_{\theta}(x)}\] <h3 id="existing-frameworks">Existing Frameworks</h3> <p>It was understood quite early on that any promising generative model family must have one property – <em>ease of sampling</em>, i.e. generating new data samples. Sampling was so essential to generative modeling, that the model families that followed were all geared towards effective sampling, even if it was at the expense of other not-so-important properties. It was also well understood that there was one common underlying principle most effective for crafting “sampling-centric” generative models – <em>transforming simple probability densities</em>. This formed the backbone of every single generative model family so far; be it VAEs, GANs or NFs, their generative process is a density transformation of this form</p> \[x = f_{\theta}(z),\text{ where } z \sim \mathcal{N}(0, I)\] <p>that suggests to start with a simple density (often just standard normal) followed by a functional transformation \(f_{\theta}\), typically a neural network with parameters \(\theta\). For VAEs, the function \(f_{\theta}\) is the decoder; for GANs, it’s the generator network and for NFs, it’s the entire flow model. It is to be noted however, that the way they differ is mostly <em>how they are trained</em>, which may involve more parametric functions (e.g. VAE’s encoder or GAN’s discriminator) and additional machinery. This way of building generative models turned out to be an effective way of sidestepping the notorious normalizing constant.</p> <h3 id="diffusion-is-no-different">Diffusion is no different</h3> <p>Diffusion Models, at its core, follow the exact same principle, but with a slightly clever design. For diffusion models, the transformation \(f_{\theta}\) is rather complicated. It is a sequence of invocations of a neural function (denoted as \(s_{\theta}\)) along with some additional computation (denoted as \(g(\cdot)\))</p> <p>\begin{equation} \label{eq:diffusion_general_parametric_structure} x = g_1(g_2(g_3(\cdots z \cdots, s_{\theta}), s_{\theta}), s_{\theta}), \text{ where } z \sim \mathcal{N}(0, I) \end{equation}</p> <p>This is a big difference between Diffusion Models and other generative model families. Prior generative families tried to learn the exact transformation directly via one parametric neural function \(f_{\theta}\). Diffusion Models on the other hand, try to learn \(s_{\theta}\), a quantity very <em>fundamental and intrinsic</em> to any true data distribution \(q_{data}(x)\). The quantity in question has historically been called the “<em>Score</em>”.</p> <h3 id="the-score">The ‘Score’</h3> <p>The term ‘Score’ is simply defined as the <em>gradient of the log-density of a distribution</em>, i.e. \(\nabla \log p(\cdot)\). In statistics, it is also known (but not very popular) as the ‘Informant’. One might argue that ‘Score’ is rather a strange name for such a quantity. It so happened that the origin of this term can be traced<d-footnote>Thanks to <a href="https://stats.stackexchange.com/a/342374">this</a> StackOverflow answer by @ben</d-footnote> to a 1935 paper<d-cite key="fisher1935detection"></d-cite> by Ronald Fisher, where he used the term in a very generic sense in order to “rank” some quantities. In the context of diffusion models however, we stick to the modern definition of score. The <em>true score</em> of our data distribution is therefore defined as the gradient of the log of <em>true density</em> of data, w.r.t the data variable</p> <p>\begin{equation} \label{eq:data_score_defn} \nabla_x \log q_{data}(x) \triangleq s(x) \end{equation}</p> <p>The quantity in Eq.\eqref{eq:data_score_defn} is unknown, just like the true data density \(q_{data}(x)\). It does have a meaning though: the “<em>true score</em>” refers to the <em>direction of steepest increase</em> in log-likelihood at any given point in the data space. See the gray arrows in the figure below.</p> <center> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/score_def-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/score_def-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/score_def-1400.webp"/> <img src="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/score_def.png" class="col-8" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </center> <p>Simply, at a point \(x\), it tell us the best direction to step into (with little step-size \(\delta\)) if we would like to see a point \(x'\) with slightly higher likelihood</p> <p>\begin{equation} \label{eq:naive_score_steps} x’ = x + \delta \cdot \left. \nabla_x \log q_{data}(x) \right|_{x = x} \end{equation}</p> <p>Please note that this stems just from the definition of the gradient operator \(\nabla\) in score. If you are familiar with gradient descent, you may find conceptual resemblance.</p> <p>Now, there are two burning questions here:</p> <ol> <li>Considering we have access to the true score, is Eq.\eqref{eq:naive_score_steps} enough to define a generative process with appropriate convergence guarantee ?</li> <li>How do we actually get the true score ?</li> </ol> <p>The following two sections answer these questions respectively. Luckily, as we now understand that these two questions are somewhat decoupled, that they can be studied independently. The first section analyzes the first question, <em>assuming</em> we have access to the true score \(\nabla_x \log q_{data}(x)\). The second section explores how to get the true score, or rather, an approximation of it.</p> <h2 id="generative-modeling-with-scores">Generative Modeling with Scores</h2> <p>As explained before, we would like to sample from the true data distribution \(q_{data}(x)\) but all we have access to (we assume) is its score \(s(x)\) as defined in Eq.\eqref{eq:data_score_defn}. One may define a naive generative process as the iterative application of Eq.\eqref{eq:naive_score_steps}. Intuitively, it is very similar to gradient descent, where we greedily climb the log-density surface to attain a local maxima. If so, we can already see a possible instance of the general structure of Diffusion’s generative process as hinted in Eq.\eqref{eq:diffusion_general_parametric_structure}, with \(g(\cdot)\) being</p> \[g(z, s(\cdot)) = z + \delta \cdot s(z) = z + \delta \cdot \nabla_x \log q_{data}(x)\] <p>With a little reshuffling of Eq.\eqref{eq:naive_score_steps} and considering \(\delta \rightarrow 0\), one can immediately reveal the underlying ODE<d-footnote>Ordinary Differential Equations, or ODEs describe how a process evolves over time by its infinitesimal change.</d-footnote> that describes the infinitesimal change</p> <p>\begin{equation} \label{eq:ode_with_score} dx = \nabla_x \log q_{data}(x) dt \end{equation}</p> <p>BUT, please note that this is only an intuitive attempt and is entirely based on the definition of score. It possesses <strong>absolutely no guarantee</strong> that this process can converge to samples from the true data distribution. In fact, this process is <strong>greedy</strong>, i.e. it only seeks to go uphill, converging exactly at the <em>modes</em><d-footnote>Local maxima of probability density</d-footnote>. You can see the below figure that shows the samples \(x\) subjected to the process in Eq.\eqref{eq:ode_with_score} and its density \(p_t(x)\) evolving over time. The density in red is the target density whose score (we assume we know it) is being used.</p> <center> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/greedy_wo_noise.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/greedy_wo_noise.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/greedy_wo_noise.gif-1400.webp"/> <img src="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/greedy_wo_noise.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </center> <p>In this case, at \(t=\infty\), all samples will converge to the state with <em>the highest</em> likelihood (i.e. exactly a the center). This isn’t really desirable as it doesn’t “explore” at all. Just like any other sampling algorithm, we need noise injection !</p> <h3 id="langevin-equation-and-brownian-motion">Langevin Equation and Brownian Motion</h3> <p>Turned out that this problem was explored long ago<d-cite key="lemons1997paul"></d-cite> in molecular dynamics by french physicist Paul Langevin in the context of analyzing movements of particles suspended in a fluid. He described the overall dynamics of particles, i.e how the position of the particle changes over time $t$ when in a <em>potential energy</em> field \(U(x)\)</p> <p>\begin{equation} \label{eq:original_langevin_dyn} dx = - \nabla_x U(x) dt + \sqrt{2} dB_t \end{equation}</p> <p>The term \(dB_t\) is called “Brownian Motion” and is effectively the source of noise – we will talk about this later in this subsection. Energy is considered “bad”, i.e. particles do not want to stay in a state with high energy. So they try to go downhill and settle in low-energy states using the gradient of the energy surface. The langevin equation (i.e. Eq.\eqref{eq:original_langevin_dyn}) happened to provide sufficient “exploration” abilities so that the particles visit states with probability \(\propto e^{-U(x)}\). This suggests that we can treat “negative energy” as log-likelihood</p> \[q_{data}(x) \propto e^{-U(x)} \implies \log q_{data}(x) = -U(x) + C \implies \nabla_x \log q_{data}(x) = - \nabla_x U(x)\] <p>By using the above substitution into the langevin equation, we can move out of physics and continue with out ML perspective</p> <p>\begin{equation} \label{eq:langevin_dyn} dx = \nabla_x \log q_{data}(x) dt + \sqrt{2} dB_t \end{equation}</p> <p>Note that this isn’t very different from our “intuitive” and greedy process in Eq.\eqref{eq:ode_with_score}, except for the noise term \(dB_t\) and a strange \(\sqrt{2}\). But this makes a difference! The brownian motion is an old construct from particle physics to describe random motion of particles in fluid/gas. It is simply a gaussian noise with infinitesimally small variance<d-footnote>In practice, the smaller step you take, the small noise you get.</d-footnote></p> \[dB_t = \mathcal{N}(0, dt) \implies dB_t = \sqrt{dt} \cdot z,\text{ where } z \sim \mathcal{N}(0, I)\] <p>With that, we can simulate our new langevin equation <em>with noise</em> (i.e. Eq.\eqref{eq:langevin_dyn}) just like the noiseless case. You can see now that the noise is keeping the process from entirely converging into the mode. If you notice carefully, we have added a little “tail” to each point to help visualize their movement.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/langevin_dyn_basic.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/langevin_dyn_basic.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/langevin_dyn_basic.gif-1400.webp"/> <img src="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/langevin_dyn_basic.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="fokker-planck-equation">Fokker-Planck Equation</h3> <p>The simulation is convincing; but it’d be even better if we can <em>theoretically verify</em> that the process in Eq.\eqref{eq:langevin_dyn} indeed converges to \(q_{data}(x)\). The key to this proof is figuring out \(p_t(x)\) and making sure that it stabilizes as \(t\rightarrow \infty\), i.e. \(p_{\infty}(x) = q_{data}(x)\). It turned out that a stochastic process of the form \(dx = \mu_t(x) dt + \sigma_t(x) dB_t\), acting on a random variable \(x\), induces a time-varying distribution that can be described by this ODE</p> <p>\begin{equation} \frac{\partial}{\partial t}p_t(x) = -\frac{\partial}{\partial x} \Big[ p_t(x)\mu_t(x) \Big] + \frac{1}{2} \frac{\partial^2}{\partial x^2} \Big[ p_t(x) \sigma^2_t(x) \Big] \end{equation}</p> <p>This is a well celebrated result know as the “Fokker-Planck equation” that even predates the Langevin Equation. So, the solution of this ODE is exactly what we are seeing in the above figure (middle). One can easily verify the convergence of Eq.\eqref{eq:langevin_dyn} by first observing \(\mu_t(x) = \nabla_x \log q_{data}(x), \sigma_t(x) = \sqrt{2}\) and then using \(\frac{\partial}{\partial t} p_{\infty}(x) = \frac{\partial}{\partial t} q_{data}(x) = 0\).</p> \[\begin{eqnarray*} \frac{\partial}{\partial t}p_{\infty}(x) &amp;=&amp; -\frac{\partial}{\partial x} \Big[ p_{\infty}(x) \nabla_x \log q_{data}(x) \Big] + \frac{(\sqrt{2})^2}{2} \frac{\partial^2}{\partial x^2} \Big[ p_{\infty}(x) \Big] \\ \frac{\partial}{\partial t} q_{data}(x) &amp;=&amp; -\frac{\partial}{\partial x} \Big[ q_{data}(x) \nabla_x \log q_{data}(x) \Big] + \frac{(\sqrt{2})^2}{2} \frac{\partial^2}{\partial x^2} \Big[ q_{data}(x) \Big] \\ 0 \text{ (LHS)} &amp;=&amp; -\frac{\partial}{\partial x} \Big[ \nabla_x q_{data}(x) \Big] + \frac{\partial}{\partial x} \Big[ \nabla_x q_{data}(x) \Big] = 0\text{ (RHS)} \end{eqnarray*}\] <p>The LHS holds due to the fact that after a long time (i.e. \(t = \infty\)) the distribution stabilizes<d-footnote>It's called a "stationary or equilibrium distribution"</d-footnote>. Please also note that the proof above is for the 1 dimensional case and included for illustrative purpose only – the general case is slightly more complicated.</p> <p>So, we’re all good. Eq.\eqref{eq:langevin_dyn} is a provable way of sampling given we have access to the true score. In fact, the very work<d-cite key="song2019generative"></d-cite> (by Song et al.) that immediately precedes DDPM, used exactly Eq.\eqref{eq:langevin_dyn} in its discrete form</p> <p>\begin{equation} x_{t+\delta} = x_t + \delta \cdot \nabla_x \log q_{data}(x) + \sqrt{2\delta} \cdot z \end{equation}</p> <p>where \(\delta\) (a small constant) is used as a practical proxy for the theoretical \(dt\).</p> <p>If you are already familiar with Diffusion Models, specifically their reverse process, you might be scratching your head. That is because, the generative process in Eq.\eqref{eq:langevin_dyn} isn’t quite same as what modern diffusion models do. We need to cross a few more hurdles before we get there.</p> <h3 id="a-probability-path">A probability path</h3> <p>More than just a proof, the Fokker-Planck ODE provides us with a key insight – i.e. gradually transforming one distribution into another is equivalent to traveling (over time) on a “path” in the <em>space of probability distributions</em>. Imagine a space of all possible probability distributions \(p\)<d-footnote>While each distribution vary in space (i.e. $x$) too, let's hide it for now and imagine them to be just a vectors.</d-footnote>. The Fokker-Planck ODE for Eq.\eqref{eq:langevin_dyn}, therefore, represents a specific dynamics on this probability space whose solution trajectory \(p_t\) ends at \(q_{data}\) at \(t = \infty\).</p> <p>Speaking of ODEs, there is something we haven’t talked about yet – the initial distribution at \(t=0\), i.e. \(p_0\). In the simulation above, I quietly used a standard normal \(\mathcal{N}(0, I)\) as starting distribution<d-footnote>You can notice this if you carefully see the first few frames of the animation.</d-footnote> without ever discussing it. Turns out that our Fokker-Planck ODE does not have any specific requirement for \(p_0\), i.e. it always converges to \(p_{\infty} = q_{data}\) no matter where you start. Here’s an illustration that shows two different starting distributions \(p_0\) and both of their “paths” over time, i.e. \(p_t\) in probability space ultimately converges to \(q_{data}\).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/fokker-plank-multiple.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/fokker-plank-multiple.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/fokker-plank-multiple.gif-1400.webp"/> <img src="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/fokker-plank-multiple.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So theoretically, given the score function \(\nabla_x \log q_{data}(x)\) of a target distribution \(q_{data}(x)\), one can “travel to” it from <em>any</em> distribution. However, keeping in mind our need for <em>sampling</em>, it’s best to choose an initial distribution that is sampling-friendly. Strictly speaking, there are couple of reasonable choices, but the diffusion model community ended up with the <em>Isotropic Gaussian</em> (i.e. \(\mathcal{N}(0, I)\)). This is not only due to its goodwill across machine learning and statistics, but also the fact that in the context of SDEs with Brownian motions<d-footnote>Remember, they are infinitesimal gaussian noises.</d-footnote>, Gaussians arise quite naturally.</p> <h3 id="estimating-the-score-is-hard">Estimating the “score” is hard</h3> <p>So far what we’ve talked about, is just the <em>generative process</em> or as diffusion model literature calls it, the “reverse process”. But we haven’t really talked about the “forward process” yet, in case you are familiar with it. The forward process, in simple terms, is an <em>ahead-of-time description</em> of the “probability path” that reverse process intends to take. But the question is, why do we need to know the path ahead of time – the reverse process seems quite spontaneous<d-footnote>In the sense that, given a score function, it just travels to the correct target distribution on its own.</d-footnote>, no ? Sadly, it can’t be answered with theory alone.</p> <p>The problem lies in Eq.\eqref{eq:langevin_dyn} – let’s write it again with a little more verbosity</p> <p>\begin{equation} dx_t = \nabla_x \left. \log q_{data}(x) \right|_{x = x_t}\ dt + \sqrt{2} dB_t \end{equation}</p> <p>Even though we wished to estimate \(\nabla_x \log q_{data}(x)\vert_{x = x_t}\) with neural network \(s_{\theta}(x = x_t)\), this turned out to be <strong>extremely hard</strong> in practice<d-cite key="song2019generative"></d-cite>. It was understood that one neural network is not enough to capture the richness of the score function at all values of \(x\). There were two options before the us – one, make the neural network expressive enough, or second, learn the network <strong>only where it’s needed</strong>. The community settled on the second one because it was easier to solve.</p> <p>So, what some of the pioneering works did, is first fixing a path<d-footnote>On probability space, like we showed above</d-footnote> and then learning the score only <em>on that path</em>. It’s all about specializing the neural network \(s_{\theta}(x_t, t)\) over \(t \in [0, \infty]\). The neural score estimator is capable of producing the right score if we provide the time \(t\), which we can of course. We will see in <a href="#estimating-the-score">the next section</a> that, to learn a score of any distribution, we need samples from it. This begs the question: how do we get samples \(x_t\) (for all \(t\)) for training purpose ? It certainly can’t be with Eq.\eqref{eq:langevin_dyn} since it requires the score. The answer is, we need to run this process in the other way – this is what Diffusion Models call the “Forward Process”.</p> <h3 id="the-forward-process">The “forward process”</h3> <p>Going <em>the other way</em> requires us to run a simulation to go from \(q_{data}(x)\) at \(t=0\) to \(t=\infty\), just the opposite of the animation above. Recall that we already saw how to do this. To go to any distribution at \(t=\infty\), all you need is its score and the langevin equation. So how about we start from \(q_0 = q_{data}(x)\) this time<d-footnote>Do you remember that starting point doesn't matter !</d-footnote> and run the langevin simulation again with a <em>known</em> end target \(q_{\infty} = \mathcal{N}(0, I)\) ?</p> \[\begin{eqnarray} dx &amp;=&amp; \nabla_x \log \mathcal{N}(0, I) dt + \sqrt{2} dB_t \\ \label{eq:forward_sde} &amp;=&amp; -x dt + \sqrt{2 dt} z \end{eqnarray}\] <p>It is interesting to note that due to the target distribution being known in its closed form, we do not see any awkward scores dangling around. The score of \(\mathcal{N}(0, I)\) is simply \(-x\)<d-footnote>We encourage the reader to verify this on their own as an exercise.</d-footnote>. The discretized version of Eq.\eqref{eq:forward_sde}, i.e.</p> \[\begin{eqnarray*} x_{t+dt} &amp;=&amp; x_t - x_t \cdot dt + \sqrt{2 dt}\ z \\ &amp;=&amp; (1 - dt) x_t + \sqrt{2 dt}\ z \end{eqnarray*}\] <p>.. may resemble DDPM’s<d-cite key="diffusionmodel_ho"></d-cite> forward process<d-footnote>Hint: compare $dt$ with DDPM's $\beta_t$.</d-footnote>.</p> <blockquote> <p>NOTE: A little subtlety here that we only fixed the <em>end point</em> of the forward process, but not the <em>exact path</em>. It seems that running the langevin equation in the forward direction chose one path on its own. Turns out that this is the “isotropic path” where all dimensions of the variable \(x\) evolves in time the exact same way. Some works<d-cite key="das2023spdiffusion"></d-cite><d-cite key="hoogeboom2023blurring"></d-cite> recently uncovered <em>non-isotropic</em> diffusion, where it is indeed possible to travel on other paths. But this is outside the scope of this article.</p> </blockquote> <p>We can simulate the above equation just like we did in the reverse process, in order to get samples \(x_t \sim q_t\). Below we show simulation of the forward process</p> <center> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/forward_process_2.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/forward_process_2.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/forward_process_2.gif-1400.webp"/> <img src="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/forward_process_2.gif" class="col-10" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </center> <p>While it is true that the reverse process in inherently sequential due to the arbitrary nature of the score, the forward process (in Eq.\eqref{eq:forward_sde}) is entirely known and hence can be exploited for easing the sequentiality. We can see a way out if we try to simplify<d-footnote>We use the standard assumption of $dt^2 = 0$.</d-footnote> the expression for \(x_{t+2dt}\) using \(x_{t+dt}\)</p> \[\begin{eqnarray*} x_{t+2dt} &amp;=&amp; (1 - dt) {\color{blue} x_{t+dt}} + \sqrt{2dt}\ z_2 \\ &amp;=&amp; (1 - dt) {\color{blue} \left[(1 - dt) x_t + \sqrt{2 dt}\ z_1\right]} + \sqrt{2dt}\ z_2 \\ &amp;=&amp; (1 - 2dt) x_t + \sqrt{2dt(1-dt)^2 + 2dt}\ z_{12} \\ &amp;=&amp; (1 - 2 \cdot dt) x_t + \sqrt{2 \cdot 2dt}\ z_{12} \\ \implies x_{t+2dt} &amp;\sim&amp; \mathcal{N}((1 - 2 \cdot dt) x_t, 2 \cdot 2dt I) \end{eqnarray*}\] <p>The above simplification suggests that we can jump to any time \(t\), without going through the entire sequence, in order to sample \(x_t \sim q_t\). In fact, \(q_t(x_t\vert x_0)\) is gaussian ! This result opens up an interesting interpretation – generating \(x_0 \sim q(x_0 \vert x_t)\) can be interpreted as solving a “gaussian inverse problems”, which we explore <a href="#denoising-as-inverse-problem">in a later section</a>.</p> <p>All good for now, but there is one more thing we need to deal with.</p> <h3 id="finite-time--the-schedule">Finite time &amp; the “schedule”</h3> <p>What we discussed so far, i.e. the forward and reverse process, require infinite time to reach its end state. This is a direct consequence of using the langevin equation. That, of course, is unacceptable in practice. But it so happened that there exists quite an elegant fix, which is well known to mathematics – we simply <em>re-define what time means</em>. We may choose a re-parameterization of time as, for example, \(t' = \mathcal{T}(t) = 1 - e^{-t} \in [0, 1]\)<d-footnote>You can see $t = 0 \implies t' = 0$ and $t = \infty \implies t' = 1$. Hence we converted the range $[0, \infty]$ to $[0, 1]$.</d-footnote>. Plugging \(dt = \mathcal{T}'(t)^{-1} dt' = e^t dt'\)<d-footnote>One can easily see that $t' = 1 - e^{-t} \implies dt' = e^{-t} dt \implies dt = e^t dt'$.</d-footnote> into the forward equation brings us even closer to DDPM’s forward process</p> \[x_{t' + dt'} = (1 - {\color{blue}e^t dt'}) x_t + \sqrt{2 {\color{blue}e^t dt'}}\ z\] <p>This suggests that in the world where time runs from \(t' = 0 \rightarrow 1\), we need to <em>escalate</em> the forward process by replacing \(dt\) with \(e^t dt'\). The quantity \(\mathcal{T}'(t)^{-1} dt' = e^t dt'\) is analogous to what diffusion models <d-cite key="diffusionmodel_ho"></d-cite><d-cite key="pmlr-v37-sohl-dickstein15"></d-cite> call a “schedule”. Recall that DDPM uses a small but increasing<d-footnote>$e^t dt'$ is small because of $dt'$, while increasing because of $e^t$.</d-footnote> “schedule” \(\beta_t\).</p> <center> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/ddpm_forward_kernel-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/ddpm_forward_kernel-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/ddpm_forward_kernel-1400.webp"/> <img src="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/ddpm_forward_kernel.png" class="col-6 z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </center> <p>Of course, our choice of the exact value of end time (i.e. \(t' = 1\)) and the re-parameterization \(\mathcal{T}\) are somewhat arbitrary. Different choices of \(\mathcal{T}\), and consequently \(\mathcal{T}'(t)^{-1} dt'\) lead to different schedules (e.g. linear, cosine etc.).</p> <blockquote> <p>NOTE: Choosing a different schedule does not mean the process takes a different path on the probability space, it simply changes its <em>speed</em> of movement over time towards the end state.</p> </blockquote> <h4 id="summary">Summary</h4> <p>To summarize, in this section, we started with the definition of ‘score’ and arrived at a stochastic process (thanks to an old result by Langevin) that, at infinite time, converges to the density associated with the score. We saw that this process is provably correct and can be interpreted as a “path” on the probability space. We argued that due to the difficulty of score estimation everywhere along the path, we need samples at the intermediate time \(t\) in order to specialize the score estimates. To do that, we had to travel backwards on the path, which can be done in closed form. We also saw how this process, even though theoretically takes infinite time, can be shrunk down to a finite interval, opening up a design choice known as “schedules”.</p> <h2 id="estimating-the-score">Estimating the Score</h2> <p>The last chapter, while explaining the “sampling” part of score-based diffusion models, assumed that we have access to the true score \(\nabla_x \log q_{data}(x)\) via some oracle. That is, of course, untrue in practice. In fact, accessing the true score for any arbitrary distribution is just not possible<d-footnote>We can only have access to the true score for distributions with closed-form, e.g. Gaussian.</d-footnote>. So the way forward, as mentioned before, is to estimate/learn it with a parametric neural network \(s_{\theta}(x)\). Recall however, that all we have access to is samples from \(q_{data}(x)\).</p> <p>If curious enough, one may question how realistic it is to estimate the score \(\nabla_x \log q_{data}(x)\), while we can NOT usually estimate the density \(q_{data}(x)\) itself ? After all, it is a quantity derived from the density ! The answer becomes clear once you make the <em>normalization constant</em> explicit</p> \[\begin{eqnarray*} \nabla_x \log q_{data}(x) &amp;=&amp; \nabla_x \log \frac{\tilde{q}_{data}(x)}{\int_{x} \tilde{q}_{data}(x) dx} \\ &amp;=&amp; \nabla_x \log \tilde{q}_{data}(x) - {\color{red}\nabla_x \log \int_{x} \tilde{q}_{data}(x) dx} \\ &amp;=&amp; \nabla_x \log \tilde{q}_{data}(x) \end{eqnarray*}\] <p>The part in red is zero due to not having dependence on \(x\). So, the score, very cleverly <strong>sidesteps the normalization constant</strong>. This is the reason score estimation gained momentum in the research community.</p> <h3 id="implicit-score-matching">Implicit Score Matching</h3> <p>The first notable attempt of this problem was by Aapo Hyvärinen<d-cite key="hyvarinen05a"></d-cite> back in 2005. His idea was simply to start from a loss function that, when minimized, leads to an estimator of the true score</p> <p>\begin{equation} J(\theta) = \frac{1}{2} \mathbb{E}_{x\sim q_{data}(x)}\Big[ \vert\vert s_{\theta}(x) - \nabla_x \log q_{data}(x) \vert\vert^2 \Big] \end{equation}</p> <p>It is simply an \(L_2\) loss between a parametric model and the true score, weighted by the probability of individual states (hence the expectation). But of course, it is not computable in this form as it contains the true score. Hyvärinen’s contribution was to simply show that, theoretically, the minimization problem is equivalent when the loss function is</p> <p>\begin{equation} \label{eq:impl_score_match} J_{\mathrm{I}}(\theta) = \mathbb{E}_{x\sim q_{data}(x)}\Big[ \mathrm{Tr}(\nabla_x s_{\theta}(x)) + \frac{1}{2} \vert\vert s_{\theta}(x) \vert\vert^2 \Big] \end{equation}</p> <p>In the literature, this is known as the “<em>Implicit Score Matching</em>”. The derivation is relatively simple and only involves algebraic manipulations – please see Appendix A of <d-cite key="hyvarinen05a"></d-cite>. The remarkable nature of this result stems from the fact that \(J_{\mathrm{I}}\) no longer contains the true score. The only dependency on \(q_{data}\) is via the expectation, which can be approximated by sample average over our dataset.</p> <p>But the key challenge with Implicit Score Matching was the \(\mathrm{Tr}(\nabla_x s_{\theta}(x))\) term, i.e. the trace of the hessian of the neural score model, which is costly to compute. This prompted several follow-up works for the race towards scalable score matching, one of which (namely De-noising score matching) is used in Diffusion Models till this day.</p> <p>For the sake of completeness, I would like to mention the work of Yang Song et al.<d-cite key="song2020sliced"></d-cite> around 2019, that proposed an engineering trick to alleviate the hessian computation. They simply used the “Hutchinson Trace estimator”<d-footnote>A stochastic way of computing trace: $\mathrm{Tr}(M) = \mathbb{E}_{v\sim p_v} \Big[ v^T M v \Big]$, where $p_v$ can be a lot of distributions, most notably $\mathcal{N}(0, I)$.</d-footnote> to replace the \(\mathrm{Tr}(\cdot)\) in Eq.\eqref{eq:impl_score_match}, which eased the computation a bit. This approach however, did not end up being used in practice.</p> <h3 id="denoising-score-matching">Denoising Score Matching</h3> <p>The most valuable contribution came from Vincent Pascal in 2011, when he showed <d-cite key="vincent2011connection"></d-cite> that the score matching problem has yet another equivalent objective, which was called “Denoising” score matching</p> <p>\begin{equation} \label{eq:deno_score_match} J_{\mathrm{D}}(\theta) = \mathbb{E}_{x\sim q_{data}(x), \epsilon\sim\mathcal{N}(0, I)}\left[ \frac{1}{2} \left|\left| s_{\theta}(\ \underbrace{x + \sigma\epsilon}_{\tilde{x}}\ ) - (- \frac{\epsilon}{\sigma}) \right|\right|^2 \right] \end{equation}</p> <p>We deliberately wrote it in a way that exposes its widely accepted interpretation. Denoising score matching simply adds some <em>known</em> noise \(\sigma\epsilon\) to the datapoints \(x\) and learns (in mean squeared sense), from the “noisy” point \(\tilde{x}\), the direction of comeback, i.e. \((-\epsilon)\), scaled by \(\frac{1}{\sigma}\). In a way, it acts like a “de-noiser”, hence the name. It is theoretically guaranteed <d-cite key="vincent2011connection"></d-cite> that \(J_{\mathrm{D}}\) leads to an unbiased estimate of the true score. Below we show a visualization of the score estimate as it learns from data.</p> <center> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/deno_score_learning.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/deno_score_learning.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/deno_score_learning.gif-1400.webp"/> <img src="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/deno_score_learning.gif" class="col-10" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </center> <p>A little algebraic manipulation of Eq.\eqref{eq:deno_score_match}, demonstrated by Ho et al. <d-cite key="diffusionmodel_ho"></d-cite>, leads to an equivalent form which turned out to be training friendly.</p> \[\begin{eqnarray} J_{\mathrm{D}}(\theta) &amp;=&amp; \mathbb{E}_{x\sim q_{data}(x), \epsilon\sim\mathcal{N}(0, I)}\left[ \frac{1}{2\sigma^2} \left|\left| {\color{blue} - \sigma s_{\theta}}(\tilde{x}) - \epsilon \right|\right|^2 \right] \\ &amp;=&amp; \mathbb{E}_{x\sim q_{data}(x), \epsilon\sim\mathcal{N}(0, I)}\left[ \frac{1}{2\sigma^2} \left|\left| {\color{blue} \epsilon}_{\theta}(\tilde{x}) - \epsilon \right|\right|^2 \right]\label{eq:deno_eps_match} \end{eqnarray}\] <p>We simply change the <em>interpretation</em> of what the network learns. In this form, the “noise estimator” network learns <em>just</em> the original pure gaussian noise vector \(\epsilon\) that was added while crafting the noisy sample. So, from a noisy sample, the network \(\epsilon_{\theta}\) learns roughly an unit variance direction that points towards the clean sample.</p> <p>There is yet another re-interpretation of Eq.\eqref{eq:deno_score_match} that leads to a slightly different perspective</p> \[\begin{eqnarray} J_{\mathrm{D}}(\theta) &amp;=&amp; \mathbb{E}_{x\sim q_{data}(x), \epsilon\sim\mathcal{N}(0, I)}\left[ \frac{1}{2\sigma^4} \left|\left| {\color{blue}\tilde{x} + \sigma^2 s_{\theta}}(\tilde{x}) - (\underbrace{\tilde{x} - \sigma\epsilon}_{x}) \right|\right|^2 \right] \\ &amp;=&amp; \mathbb{E}_{x\sim q_{data}(x), \epsilon\sim\mathcal{N}(0, I)}\left[ \frac{1}{2\sigma^4} \left|\left| {\color{blue} x_{\theta}}(\tilde{x}) - x \right|\right|^2 \right]\label{eq:deno_endpoint_match} \end{eqnarray}\] <p>Eq.\eqref{eq:deno_endpoint_match} shows, that instead of the noise direction towards clean sample, we can also have the clean sample directly as a learning target. This is like doing “denoising” in its true sense. We will get back to this in <a href="#probing-the-learning-objective">the next subsection</a>.</p> <h3 id="probing-the-learning-objective">Probing the learning objective</h3> <p>If you are still puzzled about how Eq.\eqref{eq:deno_eps_match} is related to learning the score, there is a way to probe exactly what the network is learning at an arbitrary input point \(\tilde{x}\). We note that the clean sample \(x\) and the noisy sample \(\tilde{x}\) come from a joint distribution that factorizes</p> \[q(x, \tilde{x}) = q(\tilde{x} \vert x) q_{data}(x) = \mathcal{N}(\tilde{x}; x, \sigma I) q_{data}(x).\] <p>We then factorize this joint in a slightly different way, i.e.</p> \[q(x, \tilde{x}) = q(x \vert \tilde{x}) q(\tilde{x})\] <p>where \(q(x \vert \tilde{x})\) can be thought of as a distribution of all clean samples which could’ve led to the given \(\tilde{x}\). Eq.\eqref{eq:deno_eps_match} can therefore be written as</p> \[\begin{eqnarray*} J_{\mathrm{D}}(\theta) &amp;=&amp; \mathbb{E}_{(x, \tilde{x}) \sim q(x,\tilde{x})}\left[ \frac{1}{2\sigma^2} \left|\left| \epsilon_{\theta}(\tilde{x}) - \epsilon \right|\right|^2 \right] \\ &amp;=&amp; \mathbb{E}_{\tilde{x} \sim q(\tilde{x}), x \sim q(x\vert \tilde{x})}\left[ \frac{1}{2\sigma^2} \left|\left| \epsilon_{\theta}(\tilde{x}) - \frac{\tilde{x} - x}{\sigma} \right|\right|^2 \right] \\ &amp;=&amp; \mathbb{E}_{\tilde{x} \sim q(\tilde{x})}\left[ \frac{1}{2\sigma^2} \left|\left| \epsilon_{\theta}(\tilde{x}) - \frac{\tilde{x} - \mathbb{E}_{x \sim q(x\vert \tilde{x})}[x]}{\sigma} \right|\right|^2 \right] \\ \end{eqnarray*}\] <p>In the last step, the expectation \(\mathbb{E}_{q(x\vert\tilde{x})}\left[ \cdot \right]\) was pushed inside, up until the only quantity that involves \(x\). Looking at it, you may realize that the network \(\epsilon_{\theta}\), given an input \(\tilde{x}\), learns the <em>average noise direction</em> that leads to the given input point \(\tilde{x}\). It also exposes the quantity \(\mathbb{E}_{x \sim q(x\vert \tilde{x})}[x]\), which is the <em>average clean sample</em> that led to the given \(\tilde{x}\).</p> <p>Below we visualize this process with a toy example, followed by a short explanation.</p> <center> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/probing_deno_estimation.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/probing_deno_estimation.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/probing_deno_estimation.gif-1400.webp"/> <img src="/2024/assets/img/2024-05-07-diffusion-theory-from-scratch/probing_deno_estimation.gif" class="col-10" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </center> <p>Explanation: We have 10 data points \(x\sim q_{data}(x)\) in two clusters (big red dots) and we run the learning process by generating noisy samples \(\tilde{x}\sim q(\tilde{x})\) (small red dots). Instead of learning a neural mapping over the entire space, we learn a tabular map with only three chosen input points \(\tilde{x}_1, \tilde{x}_2, \tilde{x}_3\) (blue, magenta and green cross). Every time we sample one of those<d-footnote>Practically it's impossible to randomly sample a specific point. So we assume a little ball around each point.</d-footnote> three chosen input points, we note which input data point it came from (shown by connecting a dotted line of same color) and maintain a running average (bold cross of same color) of them, i.e. which is nothing but \(\mathbb{E}_{x \sim q(x\vert \tilde{x})}[x]\). We also show the average noise direction at each \(\tilde{x}\), i.e. \(\frac{\tilde{x} - \mathbb{E}_{x \sim q(x\vert \tilde{x})}[x]}{\sigma}\), with gray arrows. The gray arrows, as the training progresses, start to resemble the score estimate of the data.</p> <h3 id="denoising-as-inverse-problem">Denoising as inverse problem</h3> <p>A similar treatment, when applied on Eq.\eqref{eq:deno_endpoint_match}, yields the following</p> \[\begin{eqnarray*} J_{\mathrm{D}}(\theta) &amp;=&amp; \mathbb{E}_{(x, \tilde{x}) \sim q(x,\tilde{x})}\left[ \frac{1}{2\sigma^4} \left|\left| {\color{blue}x_{\theta}}(\tilde{x}) - x \right|\right|^2 \right] \\ &amp;=&amp; \mathbb{E}_{\tilde{x} \sim q(\tilde{x})}\left[ \frac{1}{2\sigma^4} \left|\left| {\color{blue}\tilde{x} + \sigma^2 s_{\theta}}(\tilde{x}) - \mathbb{E}_{x \sim q(x\vert \tilde{x})}[x] \right|\right|^2 \right] \\ \end{eqnarray*}\] <p>Notice that I brought back the original form of \(x_{\theta}(\cdot)\) that involves the score. If we had the true score instead of an learned estimate, we would have</p> \[\mathbb{E}_{x \sim q(x\vert \tilde{x})}[x] = \tilde{x} + \sigma^2 \nabla_{\tilde{x}} \log p(\tilde{x})\] <p>In “Inverse problem” and Bayesian literature, this is a very well celebrated result named “<em>Tweedie’s Formula</em>”, first published by Robbins <d-cite key="robbins1992empirical"></d-cite> but credited to statistician Maurice Tweedie. This theorem is applied in the context of bayesian posterior estimation of a “true” quantity \(x\) which we only observe through a (gaussian) noisy measurement \(\tilde{x}\). Tweedie’s formula tells us that the <em>posterior mean</em> of the inverse problem \(q(x\vert \tilde{x})\) can be computed without ever knowing the actually density, as long as we have access to the score at the noisy measurement.</p> <h4 id="summary-1">Summary</h4> <p>In this section, we explored the problem of scalable score matching. We looked at the notable attempts in the literature and learned that score can be estimated from samples only. We also looked at several interpretations of the learning objective and the connections they expose.</p> <h2 id="last-few-bits">Last few bits</h2> <h4 id="incorporating-time">Incorporating time</h4> <p>In the last section, we expressed and explained everything in terms of one known noise level \(\sigma\) and the noisy sample \(\tilde{x}\). We did so to avoid cluttering of multiple concepts that aren’t necessary to explain each other. In <a href="#estimating-the-score-is-hard">a previous section</a> however, we learned that the score must be estimated along every timestep of the forward process. By simply augmenting Eq.\eqref{eq:deno_score_match} with an additional time variable \(t \in \mathcal{U}[0, 1]\) is sufficient to induce the time dependency in the score matching problem</p> <p>\begin{equation} \label{eq:deno_score_match_with_time} J_{\mathrm{D}}(\theta) = \mathbb{E}_{x_0, \epsilon, t \sim \mathcal{U}[0, 1], x_t\sim q_t(x_t\vert x_0) }\left[ \frac{1}{2} \left|\left| s_{\theta}(x_t, t) - (- \frac{\epsilon}{\sigma_t}) \right|\right|^2 \right] \end{equation}</p> <p>.. where \(q_t(x_t \vert x_0)\) is defined in a <a href="#the-forward-process">previous section</a> and \(\sigma_t\) is the standard deviation of it.</p> <h4 id="we-took-an-different-approach">We took an different approach</h4> <p>We would like to highlight that, in this article, we first explored the reverse process and then showed why the forward process emerges out of necessity. Typical diffusion models papers start from a forward process specification of the form</p> \[dx_t = f(t)x_t dt + g(t) {dB}_t\] <p>.. and then use Anderson’s SDE reversal <d-cite key="ANDERSON1982313"></d-cite> to explain the reverse process, which also involves the score</p> \[dx_t = \left[ f(t) x_t - g(t)^2 \underbrace{\nabla_{x_t} \log q_t(x_t)}_{s_{\theta}(x_t, t)} \right] dt + g(t) dB_t\] <p>We argue that our approach is more “organic” in the sense that it builds up the theory <em>chronologically</em>, exploring the exact path the community went through over time.</p> <h4 id="conclusion">Conclusion</h4> <p>In this article, we dived deep into the theoretical fundamentals of Diffusion Models, which are often ignored by practitioners. We started from the ‘heart’ of diffusion models, i.e. scores, and built the concepts up almost chronologically. We hope this article will serve as a conceptual guide toward understanding diffusion models from the score SDE perspective. We intentionally avoid the ‘probabilistic markov model’ view of diffusion since more and more works have been seen to embrace the SDE formalism.</p>]]></content><author><name>Ayan Das</name></author><summary type="html"><![CDATA[Diffusion Models, a new generative model family, have taken the world by storm after the seminal paper by Ho et al. [2020]. While diffusion models are often described as a probabilistic Markov Chains, their underlying principle is based on the decade-old theory of Stochastic Differential Equations (SDE), as found out later by Song et al. [2021]. In this article, we will go back and revisit the 'fundamental ingredients' behind the SDE formulation and show how the idea can be 'shaped' to get to the modern form of Score-based Diffusion Models. We'll start from the very definition of the 'score', how it was used in the context of generative modeling, how we achieve the necessary theoretical guarantees and how the critical design choices were made to finally arrive at the more 'principled' framework of Score-based Diffusion. Throughout this article, we provide several intuitive illustrations for ease of understanding.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2024/blog/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2024-05-07-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/iclr-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2024-05-07-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/9-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/8-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/10-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/11-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/12-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2024-05-07-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2024-05-07-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2024/assets/html/2024-05-07-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1728426050746" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1728426050746 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1728426050746 .node circle,#mermaid-1728426050746 .node ellipse,#mermaid-1728426050746 .node polygon,#mermaid-1728426050746 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1728426050746 .node.clickable{cursor:pointer}#mermaid-1728426050746 .arrowheadPath{fill:#333}#mermaid-1728426050746 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1728426050746 .edgeLabel{background-color:#e8e8e8}#mermaid-1728426050746 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1728426050746 .cluster text{fill:#333}#mermaid-1728426050746 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1728426050746 .actor{stroke:#ccf;fill:#ececff}#mermaid-1728426050746 text.actor{fill:#000;stroke:none}#mermaid-1728426050746 .actor-line{stroke:grey}#mermaid-1728426050746 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1728426050746 .messageLine0,#mermaid-1728426050746 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1728426050746 #arrowhead{fill:#333}#mermaid-1728426050746 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1728426050746 .messageText{fill:#333;stroke:none}#mermaid-1728426050746 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1728426050746 .labelText,#mermaid-1728426050746 .loopText{fill:#000;stroke:none}#mermaid-1728426050746 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1728426050746 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1728426050746 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1728426050746 .section{stroke:none;opacity:.2}#mermaid-1728426050746 .section0{fill:rgba(102,102,255,.49)}#mermaid-1728426050746 .section2{fill:#fff400}#mermaid-1728426050746 .section1,#mermaid-1728426050746 .section3{fill:#fff;opacity:.2}#mermaid-1728426050746 .sectionTitle0,#mermaid-1728426050746 .sectionTitle1,#mermaid-1728426050746 .sectionTitle2,#mermaid-1728426050746 .sectionTitle3{fill:#333}#mermaid-1728426050746 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1728426050746 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1728426050746 .grid path{stroke-width:0}#mermaid-1728426050746 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1728426050746 .task{stroke-width:2}#mermaid-1728426050746 .taskText{text-anchor:middle;font-size:11px}#mermaid-1728426050746 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1728426050746 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1728426050746 .taskText0,#mermaid-1728426050746 .taskText1,#mermaid-1728426050746 .taskText2,#mermaid-1728426050746 .taskText3{fill:#fff}#mermaid-1728426050746 .task0,#mermaid-1728426050746 .task1,#mermaid-1728426050746 .task2,#mermaid-1728426050746 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1728426050746 .taskTextOutside0,#mermaid-1728426050746 .taskTextOutside1,#mermaid-1728426050746 .taskTextOutside2,#mermaid-1728426050746 .taskTextOutside3{fill:#000}#mermaid-1728426050746 .active0,#mermaid-1728426050746 .active1,#mermaid-1728426050746 .active2,#mermaid-1728426050746 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1728426050746 .activeText0,#mermaid-1728426050746 .activeText1,#mermaid-1728426050746 .activeText2,#mermaid-1728426050746 .activeText3{fill:#000!important}#mermaid-1728426050746 .done0,#mermaid-1728426050746 .done1,#mermaid-1728426050746 .done2,#mermaid-1728426050746 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1728426050746 .doneText0,#mermaid-1728426050746 .doneText1,#mermaid-1728426050746 .doneText2,#mermaid-1728426050746 .doneText3{fill:#000!important}#mermaid-1728426050746 .crit0,#mermaid-1728426050746 .crit1,#mermaid-1728426050746 .crit2,#mermaid-1728426050746 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1728426050746 .activeCrit0,#mermaid-1728426050746 .activeCrit1,#mermaid-1728426050746 .activeCrit2,#mermaid-1728426050746 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1728426050746 .doneCrit0,#mermaid-1728426050746 .doneCrit1,#mermaid-1728426050746 .doneCrit2,#mermaid-1728426050746 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1728426050746 .activeCritText0,#mermaid-1728426050746 .activeCritText1,#mermaid-1728426050746 .activeCritText2,#mermaid-1728426050746 .activeCritText3,#mermaid-1728426050746 .doneCritText0,#mermaid-1728426050746 .doneCritText1,#mermaid-1728426050746 .doneCritText2,#mermaid-1728426050746 .doneCritText3{fill:#000!important}#mermaid-1728426050746 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1728426050746 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1728426050746 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1728426050746 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1728426050746 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1728426050746 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1728426050746 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1728426050746 #compositionEnd,#mermaid-1728426050746 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1728426050746 #aggregationEnd,#mermaid-1728426050746 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1728426050746 #dependencyEnd,#mermaid-1728426050746 #dependencyStart,#mermaid-1728426050746 #extensionEnd,#mermaid-1728426050746 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1728426050746 .branch-label,#mermaid-1728426050746 .commit-id,#mermaid-1728426050746 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1728426050746{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> <ul> <li>Unordered lists can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://iclr-blogposts.github.io/2024/blog/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/distill-example2</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/distill-example2/"><![CDATA[<p> This is a sample blog post written in HTML (while the other <a href="/2024/blog/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead. </p> <p> Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling. </p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph. Here is an example: $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$ </p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. You can display images from this repository using the following code:</p> <pre><code>{% include figure.html path="assets/img/2024-05-07-distill-example/iclr.png" class="img-fluid" %}</code></pre> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/iclr-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> To ensure that there are no namespace conflicts, you must save your asset to your unique directory `/assets/img/2024-05-07-[SUBMISSION NAME]` within your submission. </p> <p> Please avoid using the direct HTML method of embedding images; they may not be properly resized. Some below complex ways to load images (note the different styles of the shapes/shadows): </p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/9-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/8-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/10-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/11-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/12-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-distill-example/7-1400.webp"/> <img src="/2024/assets/img/2024-05-07-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3>Interactive Figures</h3> <p> Here's how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work. All that's required is for you to export your figure into HTML format, and make sure that the file exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory. To embed it into any page, simply insert the following code anywhere into your page. </p> <pre><code>{% include [FIGURE_NAME].html %}</code></pre> <p> For example, the following code can be used to generate the figure underneath it. </p> <pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2024-05-07-distill-example/plotly_demo_1.html')
</code></pre> And then include it with the following: <pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2024-05-07-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre> Voila! <div class="l-page"> <iframe src="/2024/assets/html/2024-05-07-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p> Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas. </p> <p> The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it. </p> <p> Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work. </p> <h2 id="footnotes">Footnotes</h2> <p> Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote> </p> <h2 id="code-blocks">Code Blocks</h2> <p> This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag as follows: </p> <pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre> The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below: <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h2 id="diagrams">Diagrams</h2> <p> This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc. </p> <p> <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README. </p> <p> <b>Note:</b> This is not supported for local rendering! </p> <p> The diagram below was generated by the following code: </p> <pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre> <div class='jekyll-diagrams diagrams mermaid'> <svg id="mermaid-1728426051467" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1728426051467 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1728426051467 .node circle,#mermaid-1728426051467 .node ellipse,#mermaid-1728426051467 .node polygon,#mermaid-1728426051467 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1728426051467 .node.clickable{cursor:pointer}#mermaid-1728426051467 .arrowheadPath{fill:#333}#mermaid-1728426051467 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1728426051467 .edgeLabel{background-color:#e8e8e8}#mermaid-1728426051467 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1728426051467 .cluster text{fill:#333}#mermaid-1728426051467 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1728426051467 .actor{stroke:#ccf;fill:#ececff}#mermaid-1728426051467 text.actor{fill:#000;stroke:none}#mermaid-1728426051467 .actor-line{stroke:grey}#mermaid-1728426051467 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1728426051467 .messageLine0,#mermaid-1728426051467 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1728426051467 #arrowhead{fill:#333}#mermaid-1728426051467 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1728426051467 .messageText{fill:#333;stroke:none}#mermaid-1728426051467 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1728426051467 .labelText,#mermaid-1728426051467 .loopText{fill:#000;stroke:none}#mermaid-1728426051467 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1728426051467 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1728426051467 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1728426051467 .section{stroke:none;opacity:.2}#mermaid-1728426051467 .section0{fill:rgba(102,102,255,.49)}#mermaid-1728426051467 .section2{fill:#fff400}#mermaid-1728426051467 .section1,#mermaid-1728426051467 .section3{fill:#fff;opacity:.2}#mermaid-1728426051467 .sectionTitle0,#mermaid-1728426051467 .sectionTitle1,#mermaid-1728426051467 .sectionTitle2,#mermaid-1728426051467 .sectionTitle3{fill:#333}#mermaid-1728426051467 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1728426051467 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1728426051467 .grid path{stroke-width:0}#mermaid-1728426051467 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1728426051467 .task{stroke-width:2}#mermaid-1728426051467 .taskText{text-anchor:middle;font-size:11px}#mermaid-1728426051467 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1728426051467 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1728426051467 .taskText0,#mermaid-1728426051467 .taskText1,#mermaid-1728426051467 .taskText2,#mermaid-1728426051467 .taskText3{fill:#fff}#mermaid-1728426051467 .task0,#mermaid-1728426051467 .task1,#mermaid-1728426051467 .task2,#mermaid-1728426051467 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1728426051467 .taskTextOutside0,#mermaid-1728426051467 .taskTextOutside1,#mermaid-1728426051467 .taskTextOutside2,#mermaid-1728426051467 .taskTextOutside3{fill:#000}#mermaid-1728426051467 .active0,#mermaid-1728426051467 .active1,#mermaid-1728426051467 .active2,#mermaid-1728426051467 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1728426051467 .activeText0,#mermaid-1728426051467 .activeText1,#mermaid-1728426051467 .activeText2,#mermaid-1728426051467 .activeText3{fill:#000!important}#mermaid-1728426051467 .done0,#mermaid-1728426051467 .done1,#mermaid-1728426051467 .done2,#mermaid-1728426051467 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1728426051467 .doneText0,#mermaid-1728426051467 .doneText1,#mermaid-1728426051467 .doneText2,#mermaid-1728426051467 .doneText3{fill:#000!important}#mermaid-1728426051467 .crit0,#mermaid-1728426051467 .crit1,#mermaid-1728426051467 .crit2,#mermaid-1728426051467 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1728426051467 .activeCrit0,#mermaid-1728426051467 .activeCrit1,#mermaid-1728426051467 .activeCrit2,#mermaid-1728426051467 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1728426051467 .doneCrit0,#mermaid-1728426051467 .doneCrit1,#mermaid-1728426051467 .doneCrit2,#mermaid-1728426051467 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1728426051467 .activeCritText0,#mermaid-1728426051467 .activeCritText1,#mermaid-1728426051467 .activeCritText2,#mermaid-1728426051467 .activeCritText3,#mermaid-1728426051467 .doneCritText0,#mermaid-1728426051467 .doneCritText1,#mermaid-1728426051467 .doneCritText2,#mermaid-1728426051467 .doneCritText3{fill:#000!important}#mermaid-1728426051467 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1728426051467 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1728426051467 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1728426051467 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1728426051467 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1728426051467 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1728426051467 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1728426051467 #compositionEnd,#mermaid-1728426051467 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1728426051467 #aggregationEnd,#mermaid-1728426051467 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1728426051467 #dependencyEnd,#mermaid-1728426051467 #dependencyStart,#mermaid-1728426051467 #extensionEnd,#mermaid-1728426051467 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1728426051467 .branch-label,#mermaid-1728426051467 .commit-id,#mermaid-1728426051467 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1728426051467{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <h2 id="tweets">Tweets</h2> <p> An example of displaying a tweet: <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> An example of pulling from a timeline: <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> </p> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <h2 id="layouts">Layouts</h2> The main text column is referred to as the body. It's the assumed layout of any direct descendants of the `d-article` element. <div class="fake-img l-body"> <p>.l-body</p> </div> For images you want to display a little larger, try `.l-page`: <div class="fake-img l-page"> <p>.l-page</p> </div> All of these have an outset variant if you want to poke out from the body text a little bit. For instance: <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> Occasionally you'll want to use the full browser width. For this, use `.l-screen`. You can also inset the element a little from the edge of the browser by using the inset variant. <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes. <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <h2 id="other-typography">Other Typography?</h2> <p> Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>. </p> <p> Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>. </p> <p> Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s> </p> <ul> <li>First ordered list item</li> <li>Another item</li> <ol> <li>Unordered sub-list. </li> </ol> <li>And another item.</li> </ul> <p> For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code. </p> <pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre> <pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre> <pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre> <p> A table can be created with the <code>&lt;table&gt;</code> element. Below is an example </p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p> <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote> </p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Double Descent Demystified</title><link href="https://iclr-blogposts.github.io/2024/blog/double-descent-demystified/" rel="alternate" type="text/html" title="Double Descent Demystified"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/double-descent-demystified</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/double-descent-demystified/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Machine learning models, while incredibly powerful, can sometimes act unpredictably. One of the most intriguing behaviors is when the test loss suddenly diverges at the interpolation threshold, a phenomenon distinctly observed in <strong>double descent</strong> <d-cite key="vallet1989hebb"></d-cite><d-cite key="krogh1991simple"></d-cite><d-cite key="geman1992neural"></d-cite><d-cite key="krogh1992generalization"></d-cite><d-cite key="opper1995statistical"></d-cite><d-cite key="duin2000classifiers"></d-cite><d-cite key="spigler2018jamming"></d-cite><d-cite key="belkin2019reconciling"></d-cite><d-cite key="bartlett2020benign"></d-cite><d-cite key="belkin2020twomodels"></d-cite><d-cite key="nakkiran2021deep"></d-cite><d-cite key="poggio2019double"></d-cite><d-cite key="advani2020high"></d-cite><d-cite key="liang2020just"></d-cite><d-cite key="adlam2020understanding"></d-cite><d-cite key="rocks2022memorizing"></d-cite><d-cite key="rocks2021geometry"></d-cite><d-cite key="rocks2022bias"></d-cite><d-cite key="mei2022generalization"></d-cite><d-cite key="hastie2022surprises"></d-cite><d-cite key="bach2023highdimensional"></d-cite><d-cite key="curth2024u"></d-cite>.</p> <div id="fig_unablated_all"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/unablated-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/unablated-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/unablated-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/unablated.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/unablated-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/unablated-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/unablated-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/unablated.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/unablated-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/unablated-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/unablated-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/unablated.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/unablated-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/unablated-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/unablated-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/unablated.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. <b>Double descent in ordinary linear regression.</b> Three real datasets (California Housing, Diabetes, and WHO Life Expectancy) and one synthetic dataset (Student-Teacher) all exhibit double descent, with test loss spiking at the interpolation threshold. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>While significant theoretical work has been done to comprehend why double descent occurs, it can be difficult for a newcomer to gain a general understanding of why the test loss behaves in this manner, and under what conditions one should expect similar misbehavior. In this blog post, when we say double descent, we mean the divergence at the interpolation threshold, and not whether overparameterized models generalize (or fail to generalize).</p> <p>In this work, we intuitively and quantitatively explain why the test loss diverges at the interpolation threshold, with as much generality as possible and with as simple of mathematical machinery as possible, but also without sacrificing rigor. To accomplish this, we focus on the simplest supervised model - ordinary linear regression - using the most basic linear algebra primitive: the singular value decomposition. We identify three distinct interpretable factors which, when collectively present, trigger the divergence. Through practical experiments on real data sets, we confirm that both model’s test losses diverge at the interpolation threshold, and this divergence vanishes when even one of the three factors is removed. We complement our understanding by offering a geometric picture that reveals linear models perform representation learning when overparameterized, and conclude by shedding light on recent results in nonlinear models concerning superposition.</p> <h2 id="double-descent-in-ordinary-linear-regression">Double Descent in Ordinary Linear Regression</h2> <h3 id="empirical-evidence-of-double-descent-in-ordinary-linear-regression">Empirical Evidence of Double Descent in Ordinary Linear Regression</h3> <p>Before studying ordinary linear regression mathematically, does our claim that it exhibits double descent hold empirically? We show that it indeed does, using one synthetic and three real datasets: World Health Organization Life Expectancy <d-cite key="gochiashvili_2023_who"></d-cite>, California Housing <d-cite key="pace1997sparse"></d-cite>, Diabetes <d-cite key="efron2004least"></d-cite>; these three real datasets were selected on the basis of being easily accessible through sklearn <d-cite key="scikit-learn"></d-cite> or Kaggle. As shown in <a href="#fig_unablated_all">Fig 1</a>, all display a spike in test mean squared error at the interpolation threshold. Our simple Python code is <a href="">publicly available</a>.</p> <h3 id="notation-and-terminology">Notation and Terminology</h3> <p>Consider a regression dataset of $N$ training data with features $\vec{x}_n \in \mathbb{R}^D$ and targets $y_n \in \mathbb{R}$. We sometimes use matrix-vector notation to refer to the training data:</p> \[X \in \mathbb{R}^{N \times D} \quad , \quad Y \in \mathbb{R}^{N \times 1}.\] <p>In ordinary linear regression, we want to learn parameters $\hat{\vec{\beta}} \in \mathbb{R}^{D}$ such that:</p> \[\vec{x}_n \cdot \hat{\vec{\beta}} \approx y_n.\] <p>We will study three key parameters:</p> <ol> <li>The number of model parameters $P$</li> <li>The number of training data $N$</li> <li>The dimensionality of the data $D$</li> </ol> <p>We say that a model is <em>overparameterized</em> if $N &lt; P$ and <em>underparameterized</em> if $N &gt; P$. The <em>interpolation threshold</em> refers to $N=P$, because when $N\leq P$, the model can perfectly interpolate the training points. Recall that in ordinary linear regression, the number of parameters $P$ equals the dimension $D$ of the covariates. Consequently, rather than thinking about changing the number of parameters $P$, we’ll instead think about changing the number of data points $N$.</p> <h3 id="mathematical-analysis-of-ordinary-linear-regression">Mathematical Analysis of Ordinary Linear Regression</h3> <p>To understand under what conditions and why double descent occurs at the interpolation threshold in linear regression, we’ll study the two parameterization regimes. If the regression is <em>underparameterized</em>, we estimate the linear relationship between covariates $\vec{x}_n$ and target $y_n$ by solving the least-squares minimization problem:</p> \[\begin{align*} \hat{\vec{\beta}}_{under} \, &amp;:= \, \arg \min_{\vec{\beta}} \frac{1}{N} \sum_n ||\vec{x}_n \cdot \vec{\beta} - y_n||_2^2\\ \, &amp;:= \, \arg \min_{\vec{\beta}} ||X \vec{\beta} - Y ||_2^2. \end{align*}\] <p>The solution is the ordinary least squares estimator based on the second moment matrix $X^T X$:</p> \[\hat{\vec{\beta}}_{under} = (X^T X)^{-1} X^T Y.\] <p>If the model is overparameterized, the optimization problem is ill-posed since we have fewer constraints than parameters. Consequently, we choose a different (constrained) optimization problem that asks for the minimum norm parameters that still perfectly interpolate the training data:</p> \[\begin{align*} \hat{\vec{\beta}}_{over} \, &amp;:= \, \arg \min_{\vec{\beta}} ||\vec{\beta}||_2^2\\ \text{s.t.} \quad \quad \forall \, n \in &amp;\{1, ..., N\}, \quad \vec{x}_n \cdot \vec{\beta} = y_n. \end{align*}\] <p>We choose this optimization problem because it is the one gradient descent implicitly minimizes. The solution to this optimization problem uses the Gram matrix $X X^T \in \mathbb{R}^{N \times N}$:</p> \[\hat{\vec{\beta}}_{over} = X^T (X X^T)^{-1} Y.\] <p>One way to see why the Gram matrix appears is via constrained optimization: define the Lagrangian $\mathcal{L}(\vec{\beta}, \vec{\lambda}) \, := \, \frac{1}{2}||\vec{\beta}||_2^2 + \vec{\lambda}^T (Y - X \vec{\beta})$ with Lagrange multipliers $\vec{\lambda} \in \mathbb{R}^N$, then differentiate with respect to the parameters and Lagrange multipliers to obtain the overparameterized solution.</p> <p>After being fit, for test point $\vec{x}_{test}$, the model will make the following predictions:</p> \[\hat{y}_{test, under} = \vec{x}_{test} \cdot \hat{\vec{\beta}}_{under} = \vec{x}_{test} \cdot (X^T X)^{-1} X^T Y\] \[\hat{y}_{test, over} = \vec{x}_{test} \cdot \hat{\vec{\beta}}_{over} = \vec{x}_{test} \cdot X^T (X X^T)^{-1} Y.\] <p>Hidden in the above equations is an interaction between three quantities that can, when all grow extreme, create a divergence in the test loss!</p> <p>To reveal the three quantities, we’ll rewrite the regression targets by introducing a slightly more detailed notation. Unknown to us, there are some ideal linear parameters $\vec{\beta}^* \in \mathbb{R}^P = \mathbb{R}^D$ that truly minimize the test mean squared error. We can write any regression target as the inner product of the data $\vec{x}_n$ and the ideal parameters $\vec{\beta}^*$, plus an additional error term $e_n$ that is an “uncapturable” residual from the “viewpoint” of the model class</p> \[y_n = \vec{x}_n \cdot \vec{\beta}^* + e_n.\] <p>In matrix-vector form, we will equivalently write:</p> \[Y = X \vec{\beta}^* + E,\] <p>with $E \in \mathbb{R}^{N \times 1}$. To be clear, we are <em>not</em> imposing assumptions. Rather, we are introducing notation to express that there are (unknown) ideal linear parameters, and possibly non-zero errors $E$ that even the ideal model might be unable to capture; these errors $E$ could be random noise or could be fully deterministic patterns that this particular model class cannot capture. Using this new notation, we rewrite the model’s predictions to show how the test datum’s features $\vec{x}_{test}$, training data’s features $X$ and training data’s regression targets $Y$ interact.</p> <p>Let $y_{test}^* := \vec{x}_{test} \cdot \vec{\beta}^*$. In the underparameterized regime:</p> \[\begin{align*} \hat{y}_{test,under} &amp;= \vec{x}_{test} \cdot \hat{\vec{\beta}}_{under}\\ &amp;=\vec{x}_{test} \cdot (X^T X)^{-1} X^T Y\\ &amp;=\vec{x}_{test} \cdot (X^T X)^{-1} X^T (X \vec{\beta}^* + E)\\ &amp;=\vec{x}_{test} \cdot \vec{\beta}^* + \, \vec{x}_{test} \cdot (X^T X)^{-1} X^T E\\ \hat{y}_{test,under} - y_{test}^* &amp;= \vec{x}_{test} \cdot (X^T X)^{-1} X^T E. \end{align*}\] <p>This equation is important, but opaque. To extract the intuition, replace $X$ with its singular value decomposition $X = U S V^T$. Let $R \, := \, \text{rank}(X)$ and let $\sigma_1 &gt; \sigma_2 &gt; … &gt; \sigma_R &gt; 0$ be $X$’s (non-zero) singular values. Let $S^+$ denote the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose inverse</a>; in this context, this means that if a singular value $\sigma_r$ is non-zero, then in $S^+$, it becomes its reciprocal $1/\sigma_r$, but if the singular value is zero, then in $S^+$, it remains $0$. We can decompose the underparameterized prediction error along the orthogonal singular modes:</p> \[\begin{align*} \hat{y}_{test, under} - y_{test}^* &amp;= \vec{x}_{test} \cdot V S^{+} U^T E\\ &amp;= \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E). \end{align*}\] <p>This equation will be critical! The same term will appear in the overparameterized regime (plus one additional term):</p> \[\begin{align*} \hat{y}_{test,over} &amp;= \vec{x}_{test} \cdot \hat{\vec{\beta}}_{over}\\ &amp;= \vec{x}_{test} \cdot X^T (X X^T)^{-1} Y\\ &amp;= \vec{x}_{test} \cdot X^T (X X^T)^{-1} (X \beta^* + E)\\ \hat{y}_{test,over} - y_{test}^* &amp;= \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^* \\ &amp;\quad\quad + \quad \vec{x}_{test} \cdot X^T (X X^T)^{-1} E\\ &amp;= \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^* \\ &amp;\quad\quad + \quad \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E), \end{align*}\] <p>where the last step again replaced $X$ with its SVD $X = U S V^T$. Thus, the prediction errors in the overparameterized and underparameterized regimes will be:</p> \[\begin{align*} \hat{y}_{test,over} - y_{test}^* &amp;= \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E)\\ &amp;\quad \quad + \quad \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^*\\ \hat{y}_{test,under} - y_{test}^* &amp;= \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E). \end{align*}\] <p>The shared term in the two prediction errors causes the divergence:</p> \[\begin{equation} \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E). \label{eq:variance} \end{equation}\] <p>Eqn. \ref{eq:variance} is critical. It reveals that our test prediction error (and thus, our test squared error!) will depend on an interaction between 3 quantities:</p> <ol> <li> <p>How much the training features vary in each direction. More formally, the inverse (non-zero) singular values of the <em>training features</em> $X$:</p> \[\frac{1}{\sigma_r}\] </li> <li> <p>How much, and in which directions, the test features vary relative to the training features. More formally: how $\vec{x}_{test}$ projects onto $X$’s right singular vectors $V$:</p> \[\vec{x}_{test} \cdot \vec{v}_r\] </li> <li> <p>How well the best possible model in the model class can correlate the variance in the training features with the training regression targets. More formally: how the residuals $E$ of the best possible model in the model class (i.e. insurmountable “errors” from the “perspective” of the model class) project onto $X$’s left singular vectors $U$:</p> \[\vec{u}_r \cdot E\] </li> </ol> <p>We use the term “vary” when discussing $\vec{v}_r$ because $V$ can be related to the empirical (or sample) covariance matrix oftentimes studied in Principal Component Analysis. That is, if the SVD of $X$ is $U S V^T$, then $\frac{1}{N} X^T X = \frac{1}{N} V S^2 V^T$. If the training data are centered (a common preprocessing step), then this is the empirical covariance matrix and its eigenvectors $\vec{v}_1, …, \vec{v}_R$ identify the orthogonal directions of variance. We’ll return to this in <a href="#fig_geometric_smallest_nonzero_singular_value">Fig 6</a>.</p> <p><strong>Why does the test error diverge?</strong> When (1) and (3) are both present in the learning problem, the model’s parameters along this singular mode are likely incorrect. When (2) is added to the mix by a test datum $\vec{x}_{test}$ with a large projection along this mode, the model is forced to extrapolate significantly beyond what it saw in the training data, in a direction where the training data had an error-prone relationship between its predictions and the training targets, using parameters that are likely wrong. As a consequence, the test squared error explodes!</p> <h3 id="factor-1---low-variance-in-training-features">Factor 1 - Low Variance in Training Features</h3> <div id="fig_factor_1_small_singular_values"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_small_singular_values-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_small_singular_values-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_small_singular_values-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_small_singular_values.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_small_singular_values-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_small_singular_values-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_small_singular_values-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_small_singular_values.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_small_singular_values-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_small_singular_values-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_small_singular_values-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_small_singular_values.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_small_singular_values-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_small_singular_values-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_small_singular_values-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_small_singular_values.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. <b>Required Factor #1: How much training features vary in each direction.</b> The test loss diverges at the interpolation threshold only if training features $X$ contain small (non-zero) singular values. Ablation: By removing all singular values below a cutoff, the divergence at the interpolation threshold is diminished or disappears entirely. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>The test loss will not diverge if any of the three required factors are absent. What could cause that? One way is if small-but-nonzero singular values do not appear in the training data features. One way to accomplish this is by setting all singular values below a selected threshold to exactly 0. To test our understanding, we independently ablate all small singular values in the training features. Specifically, as we run the ordinary linear regression fitting process, and as we sweep the number of training data, we also sweep different singular value cutoffs and remove all singular values of the training features $X$ below the cutoff (<a href="#fig_factor_1_small_singular_values">Fig 2</a>).</p> <h3 id="factor-2---test-features-in-training-feature-subspace">Factor 2 - Test Features in Training Feature Subspace</h3> <div id="fig_test_feat_in_train_feat_subspace"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_feat_in_train_feat_subspace-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_feat_in_train_feat_subspace-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_feat_in_train_feat_subspace-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_feat_in_train_feat_subspace.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_feat_in_train_feat_subspace-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_feat_in_train_feat_subspace-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_feat_in_train_feat_subspace-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_feat_in_train_feat_subspace.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_feat_in_train_feat_subspace-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_feat_in_train_feat_subspace-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_feat_in_train_feat_subspace-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_feat_in_train_feat_subspace.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_feat_in_train_feat_subspace-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_feat_in_train_feat_subspace-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_feat_in_train_feat_subspace-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_feat_in_train_feat_subspace.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. <b>Required Factor #2: How much, and in which directions, test features vary relative to training features.</b> The test loss diverges only if the test features $\vec{x}_{test}$ have a large projection onto the training features $X$'s right singular vectors $V$. Ablation: By projecting the test features into the subspace of the leading singular modes, the divergence at the interpolation threshold is diminished or disappears entirely. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>Double descent should not occur if the test datum does not vary in different directions than the training features. Specifically, if the test datum lies entirely in the subspace of just a few of the leading singular directions, then the divergence is unlikely to occur. To test our understanding, we force the test data features to lie in the training features subspace: as we run the ordinary linear regression fitting process, and as we sweep the number of training data, we project the test features $\vec{x}_{test}$ onto the subspace spanned by the training features $X$ singular modes (<a href="#fig_test_feat_in_train_feat_subspace">Fig 3</a>).</p> <h3 id="factor-3---errors-from-best-possible-model">Factor 3 - Errors from Best Possible Model</h3> <div id="fig_no_residuals_in_ideal"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_residuals_in_ideal-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_residuals_in_ideal-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_residuals_in_ideal-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/no_residuals_in_ideal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_residuals_in_ideal-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_residuals_in_ideal-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_residuals_in_ideal-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/no_residuals_in_ideal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_residuals_in_ideal-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_residuals_in_ideal-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_residuals_in_ideal-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/no_residuals_in_ideal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_residuals_in_ideal-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_residuals_in_ideal-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_residuals_in_ideal-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/no_residuals_in_ideal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. <b>Required Factor #3: How well the best possible model in the model class can correlate variance in training features with training targets.</b> The test loss diverges only if the residuals $E$ from the best possible model in the model class on the training data have a large projection onto the training features $X$'s left singular vectors $U$. Ablation: By ensuring the true relationship between features and targets is within the model class i.e. linear, the divergence at the interpolation threshold disappears. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>Double descent should not occur if the best possible model in the model class makes no errors on the training data. For example, if we use a linear model class on data where the true relationship is a noiseless linear relationship, then at the interpolation threshold, we will have $D=P$ data, $P=D$ parameters, our line of best fit will exactly match the true relationship, and no divergence will occur. To test our understanding, we ensure no residual errors exist in the best possible model: we first use the entire dataset to fit a linear model, then replace all target values with the predictions made by the ideal linear model. We then rerun our typical fitting process using these new labels, sweeping the number of training data (<a href="#fig_no_residuals_in_ideal">Fig 4</a>).</p> <p>As a short aside, what could cause residual errors in the best possible model in the model class?</p> <ol> <li><strong>Noise</strong>: If the data is noisy, then the best possible model in the model class will have residual errors.</li> <li><strong>Model Misspecification</strong>: If the data is generated by a nonlinear model, but we use a linear model class (or vice versa), then the best possible model in the model class will have residual errors.</li> <li><strong>Missing Features</strong>: Even if the data is noiseless and our model belongs to the correct model class, but we are missing covariates, then the best possible model in the model class will still have residual errors.</li> </ol> <h3 id="divergence-at-the-interpolation-threshold">Divergence at the Interpolation Threshold</h3> <div id="fig_least_informative_singular_value"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/least_informative_singular_value-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/least_informative_singular_value-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/least_informative_singular_value-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/least_informative_singular_value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/least_informative_singular_value-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/least_informative_singular_value-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/least_informative_singular_value-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/least_informative_singular_value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/least_informative_singular_value-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/least_informative_singular_value-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/least_informative_singular_value-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/least_informative_singular_value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/least_informative_singular_value-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/least_informative_singular_value-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/least_informative_singular_value-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/least_informative_singular_value.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5. <b>The training features are most likely to obtain their smallest non-zero singular value when approaching the interpolation threshold.</b> </div> </div> <p>Why does this divergence happen near the interpolation threshold? The answer is that the first factor (small non-zero singular values in the training features $X$) is likely to occur at the interpolation threshold (<a href="#fig_least_informative_singular_value">Fig 5</a>), but why?</p> <p>Suppose we’re given a single training datum \(\vec{x}_1\). So long as this datum isn’t exactly zero, that datum varies in a single direction, meaning we gain information about the variance in that direction, but the variance in all orthogonal directions is exactly 0. With the second training datum \(\vec{x}_2\), so long as this datum isn’t exactly zero, that datum varies, but now, some fraction of \(\vec{x}_2\) might have a positive projection along \(\vec{x}_1\); if this happens (and it likely will, since the two vectors are unlikely to be exactly orthogonal), the shared direction gives us <em>more</em> information about the variance in this shared direction, but <em>less</em> information about the second orthogonal direction of variation. Ergo, the training data’s smallest non-zero singular value after 2 samples is probabilistically smaller than after 1 sample. As we approach the interpolation threshold, the probability that each additional datum has large variance in a new direction orthogonal to all previous directions grows unlikely (<a href="#fig_geometric_smallest_nonzero_singular_value">Fig 5</a>), but as we move beyond the interpolation threshold, the variance in each covariate dimension becomes increasingly clear.</p> <div id="fig_geometric_smallest_nonzero_singular_value"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=1-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=2-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=3-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=8-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=100-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=100-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=100-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/smallest_nonzero_singular_value/data_distribution_num_data=100.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6. <b>Geometric intuition for why the smallest non-zero singular value reaches its lowest value near the interpolation threshold.</b> If $1$ datum is observed, variance exists in only 1 direction. If $2$ data are observed, a second axis of variation appears, but because the two data are likely to share some component, the second axis is likely to have less variance than the first. At the interpolation threshold (here, $D=P=N=3$), because the three data are likely to share components along the first two axes, the third axis is likely to have even less variance. Beyond the interpolation threshold, additional data contribute additional variance to these three axes. </div> </div> <h3 id="generalization-in-overparameterized-linear-regression">Generalization in Overparameterized Linear Regression</h3> <p>You might be wondering why three of the datasets have low test squared error in the overparameterized regime (California Housing, Diabetes, Student-Teacher) but one (WHO Life Expectancy) does not. Recall that the overparameterized regime’s prediction error has another term \(\hat{y}_{test,over} - y_{test}^*\) not present in the underparameterized regime:</p> \[\begin{equation} \vec{x}_{test} \cdot (X^T (X X^T)^{-1} X - I_D) \beta^*. \label{eq:bias} \end{equation}\] <p>To understand why this bias exists, recall that our goal is to correlate fluctuations in the covariates $\vec{x}$ with fluctuations in the targets $y$. In the overparameterized regime, there are more parameters than data; consequently, for $N$ data points in $D=P$ dimensions, the model can “see” fluctuations in at most $N$ dimensions, but has no ``visibility” into the remaining $P-N$ dimensions. This causes information about the optimal linear relationship $\vec{\beta}^*$ to be lost, thereby increasing the overparameterized prediction error.</p> <div id="fig_overparameterized_generalization"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/overparameterized_generalization-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/overparameterized_generalization-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/overparameterized_generalization-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/overparameterized_generalization.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7. <b>Geometry of Generalization in Overparameterized Ordinary Linear Regression.</b> The rowspace of the training features $X$ forms a subspace (here, $\mathbb{R}^1$) of the ambient space (here, $\mathbb{R}^2$). For test datum $\vec{x}_{test}$, the linear model forms an internal representation of the test datum $\hat{\vec{x}}_{test}$ by orthogonally projecting the test datum onto the rowspace via projection matrix $X^T (X X^T)^{-1} X$. The generalization error will then increase commensurate with the inner product between $\hat{\vec{x}}_{test} - \vec{x}_{test}$ and the best possible parameters for the function class $\vec{\beta}^*$. Three different possible $\vec{\beta}^*$ are shown with <span style="color:blue;">low (blue)</span>, <span style="color:green;">medium (green)</span> and <span style="color:red;">high (red)</span> generalization errors. </div> </div> <p>We previously saw that away from the interpolation threshold, the variance is unlikely to affect the discrepancy between the overparameterized model’s predictions and the ideal model’s predictions, meaning most of the discrepancy must therefore emerge from the bias (Eqn. \ref{eq:bias}). This bias term yields an intuitive geometric picture (<a href="#fig_overparameterized_generalization">Fig 7</a>) that also reveals a surprising fact: <em>overparameterized linear regression does representation learning!</em> Specifically, for test datum \(\vec{x}_{test}\), a linear model creates a representation of the test datum \(\hat{\vec{x}}_{test}\) by orthogonally projecting the test datum onto the row space of the training covariates \(X\) via the projection matrix \(X^T (X X^T)^{-1} X\):</p> \[\begin{equation*} \hat{\vec{x}}_{test} := X^T (X X^T)^{-1} X \; \vec{x}_{test}. \end{equation*}\] <p>Seen this way, the bias can be rewritten as the inner product between (1) the difference between its representation of the test datum and the test datum and (2) the ideal linear model’s fit parameters:</p> \[\begin{equation}\label{eq:overparam_gen_bias} (\hat{\vec{x}}_{test} - \vec{x}_{test}) \cdot \vec{\beta}^*. \end{equation}\] <div id="fig_test_bias_squared"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_bias_squared-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_bias_squared-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_bias_squared-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/california_housing/test_bias_squared.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_bias_squared-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_bias_squared-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_bias_squared-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/diabetes/test_bias_squared.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_bias_squared-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_bias_squared-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_bias_squared-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/student_teacher/test_bias_squared.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_bias_squared-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_bias_squared-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_bias_squared-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_ablations/who_life_expectancy/test_bias_squared.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 8. <b>Test Error of Overparameterized Models.</b> Large inner product between the ideal model's parameters and the difference between the fit model's internal representations of the test data and the test data creates large test squared error for overparameterized models. </div> </div> <p>Intuitively, an overparameterized model will generalize well if the model’s representations capture the essential information necessary for the best model in the model class to perform well (<a href="#fig_test_bias_squared">Fig. 8</a>).</p> <h2 id="adversarial-test-data-and-adversarial-training-data">Adversarial Test Data and Adversarial Training Data</h2> <p>Our key equation (Eqn. \ref{eq:variance}) also reveals <em>why</em> adversarial test data and adversarial training data exist (at least in linear regression) and <em>how</em> mechanistically they function. For convenience, we repeat the equation:</p> \[\begin{equation*} \sum_{r=1}^R \frac{1}{\sigma_r} (\vec{x}_{test} \cdot \vec{v}_r) (\vec{u}_r \cdot E). \end{equation*}\] <p>Adversarial test examples are a well-known phenomenon in machine learning <d-cite key="szegedy2013intriguing"></d-cite> <d-cite key="goodfellow2014explaining"></d-cite> <d-cite key="kurakin2018adversarial"></d-cite> <d-cite key="athalye2018synthesizing"></d-cite> <d-cite key="xie2022word"></d-cite> that we can see in this equation. The adversarial test features correspond to \(\vec{x}_{test} \cdot \vec{v}_r\) being large, where one can drastically increase the test squared error by moving the test example in the direction of the right singular vector(s) with the smallest non-zero singular values (<a href="#fig_adversarial_train_data">Fig 9</a>).</p> <div id="fig_test_bias_squared"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_test_datum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_test_datum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_test_datum-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_test_datum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_test_datum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_test_datum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_test_datum-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_test_datum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_test_datum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_test_datum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_test_datum-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_test_datum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_test_datum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_test_datum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_test_datum-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_test_datum.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 9. <b>Adversarial Test Examples in Linear Regression.</b> Adversarial examples arise by pushing $\vec{x}_{test}$ far along the trailing singular modes in the training features $X$. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <p>Less well-known are adversarial training data, akin to dataset poisoning <d-cite key="biggio2012poisoning"></d-cite> <d-cite key="steinhardt2017certified"></d-cite> <d-cite key="wallace2020concealed"></d-cite> <d-cite key="carlini2021contrastive"></d-cite> <d-cite key="carlini2021poisoning"></d-cite> <d-cite key="schuster2021you"></d-cite> or backdoor attacks <d-cite key="chen2017targeted"></d-cite> <d-cite key="gu2017badnets"></d-cite> <d-cite key="carlini2021contrastive"></d-cite>. Adversarial training examples correspond to \(\vec{u}_r \cdot E\) being large, where one can drastically increase the test squared error by moving the training errors $E$ in the direction of the left singular vector(s) with the smallest non-zero singular value. This gives a practical way to construct <em>adversarial training data</em>: training features and targets whose training loss is unchanged from unaltered training data, but causes the test loss to be 1-3 orders of magnitude larger (<a href="#fig_adversarial_train_data">Fig 10</a>).</p> <div id="fig_adversarial_train_data"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_train_data-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_train_data-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_train_data-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/california_housing/adversarial_train_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_train_data-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_train_data-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_train_data-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/diabetes/adversarial_train_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_train_data-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_train_data-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_train_data-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/student_teacher/adversarial_train_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_train_data-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_train_data-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_train_data-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/real_data_adversarial/who_life_expectancy/adversarial_train_data.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 10. <b>Adversarial Training Dataset in Linear Regression.</b> By manipulating the residual errors $E$ that the best possible model in the model class achieves on the training data, we construct training datasets that increase the test error of the learned model by 1-3 orders of magnitude without affecting its training error. <span style="color:blue;">Blue is training error.</span> <span style="color:orangered;">Orange is test error.</span> </div> </div> <h2 id="intuition-for-nonlinear-models">Intuition for Nonlinear Models</h2> <p>Although we mathematically studied ordinary linear regression, the intuition for why the test loss diverges extends to nonlinear models, such as polynomial regression and including certain classes of deep neural networks <d-cite key="jacot2018neural"></d-cite> <d-cite key="lee2017deep"></d-cite> <d-cite key="bordelon2020spectrum"></d-cite>. For a concrete example about how our intuition can shed light on the behavior of nonlinear models, Henighan et al. 2023 <d-cite key="henighan2023superposition"></d-cite> recently discovered interesting properties of shallow nonlinear autoencoders: depending on the number of training data, (1) autoencoders either store data points or features, and (2) the test loss increases sharply between these two regimes (<a href="#fig_henighan">Fig. 11</a>).</p> <div id="fig_henighan"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/henighan2023superposition-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/henighan2023superposition-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-double-descent-demystified/henighan2023superposition-1400.webp"/> <img src="/2024/assets/img/2024-05-07-double-descent-demystified/henighan2023superposition.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 11. <b>Superposition, Memorization and Double Descent in Nonlinear Shallow Autoencoders.</b> Figure from Henighan et al. 2023 <d-cite key="henighan2023superposition"></d-cite>. </div> </div> <p>Our work sheds light on the results in two ways:</p> <ol> <li> <p>Henighan et al. 2023 write, “It’s interesting to note that we’re observing double descent in the absence of label noise.” Our work clarifies that noise, in the sense of a random quantity, is <em>not</em> necessary to produce double descent. Rather, what is necessary is <em>residual errors from the perspective of the model class</em> ($E$, in our notation). Those errors could be entirely deterministic, such as a nonlinear model attempting to fit a noiseless linear relationship, or other model misspecifications.</p> </li> <li> <p>Henighan et al. 2023 write, “[Our work] suggests a naive mechanistic theory of overfitting and memorization: memorization and overfitting occur when models operate on ‘data point features’ instead of ‘generalizing features’.” Our work hopefully clarifies that this dichotomy is incorrect: when overparameterized, data point features are akin to the Gram matrix $X X^T$ and when underparameterized, generalizing features are akin to the second moment matrix $X^T X$. Our work hopefully clarifies that data point features can and very often do generalize, and that there is a deep connection between the two, i.e., their shared spectra.</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <p>In this work, we intuitively and quantitatively explained why the test loss misbehaves based on three interpretable factors, tested our understanding via ablations, connected our understanding to adversarial test examples and adversarial training datasets, and added conceptual clarity of recent discoveries in nonlinear models.</p>]]></content><author><name>Rylan Schaeffer</name></author><summary type="html"><![CDATA[Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle]]></summary></entry><entry><title type="html">Bridging the Data Processing Inequality and Function-Space Variational Inference</title><link href="https://iclr-blogposts.github.io/2024/blog/dpi-fsvi/" rel="alternate" type="text/html" title="Bridging the Data Processing Inequality and Function-Space Variational Inference"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/dpi-fsvi</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/dpi-fsvi/"><![CDATA[<div style="display: none;"> $$\require{mathtools} \DeclareMathOperator{\opExpectation}{\mathbb{E}} \newcommand{\E}[2]{\opExpectation_{#1} \left [ #2 \right ]} \newcommand{\simpleE}[1]{\opExpectation_{#1}} \newcommand{\MidSymbol}[1][]{\:#1\:} \newcommand{\given}{\MidSymbol[\vert]} \DeclareMathOperator{\opmus}{\mu^*} \newcommand{\IMof}[1]{\opmus[#1]} \DeclareMathOperator{\opInformationContent}{H} \newcommand{\ICof}[1]{\opInformationContent[#1]} \newcommand{\xICof}[1]{\opInformationContent(#1)} \DeclareMathOperator{\opEntropy}{H} \newcommand{\Hof}[1]{\opEntropy[#1]} \newcommand{\xHof}[1]{\opEntropy(#1)} \DeclareMathOperator{\opMI}{I} \newcommand{\MIof}[1]{\opMI[#1]} \DeclareMathOperator{\opTC}{TC} \newcommand{\TCof}[1]{\opTC[#1]} \newcommand{\CrossEntropy}[2]{\opEntropy(#1 \MidSymbol[\Vert] #2)} \DeclareMathOperator{\opKale}{D_\mathrm{KL}} \newcommand{\Kale}[2]{\opKale(#1 \MidSymbol[\Vert] #2)} \DeclareMathOperator{\opJSD}{D_\mathrm{JSD}} \newcommand{\JSD}[2]{\opJSD(#1 \MidSymbol[\Vert] #2)} \DeclareMathOperator{\opp}{p} \newcommand{\pof}[1]{\opp(#1)} \newcommand{\hpof}[1]{\hat{\opp}(#1)} \newcommand{\pcof}[2]{\opp_{#1}(#2)} \newcommand{\hpcof}[2]{\hat\opp_{#1}(#2)} \DeclareMathOperator{\opq}{q} \newcommand{\qof}[1]{\opq(#1)} \newcommand{\hqof}[1]{\hat{\opq}(#1)} \newcommand{\qcof}[2]{\opq_{#1}(#2)} \newcommand{\varHof}[2]{\opEntropy_{#1}[#2]} \newcommand{\xvarHof}[2]{\opEntropy_{#1}(#2)} \newcommand{\varMIof}[2]{\opMI_{#1}[#2]} \newcommand{\w}{\boldsymbol{\theta}} \newcommand{\W}{\boldsymbol{\Theta}} \DeclareMathOperator{\opf}{f} \newcommand{\fof}[1]{\opf(#1)} \newcommand{\Dany}{\mathcal{D}} \newcommand{\y}{y} \newcommand{\Y}{Y} \newcommand{\L}{\boldsymbol{L}} \newcommand{\x}{\boldsymbol{x}} \newcommand{\X}{\boldsymbol{X}} \newcommand{\pdata}[1]{\hpcof{\text{data}}{#1}} \newcommand{\normaldist}[1]{\mathcal{N}(#1)} $$ </div> <h2 id="introduction">Introduction</h2> <p>In information theory, the <strong>data processing inequality (DPI)</strong> expresses a fundamental idea: processing data (stochastically) cannot increase information. The DPI provides us with a powerful intuition about what information processing systems can do and what the limitations of data processing are.</p> <p>In this blog post, we first study the DPI, developing intuition through vivid examples and detailed proofs—especially the equality case, which is arguably the best way to understand inequalities. We will consider classic forms of the DPI as well as DPIs relating probability distributions more broadly. Then, we explore the intriguing connection between DPI and <strong>function-space variational inference (FSVI)</strong>, a modern Bayesian deep learning technique that focuses on the Bayesian predictive posterior rather than the parameter space. Exploring this connection is important because it can provide new insights into FSVI on a fundamental level. We apply the DPI to recover several interesting results from the literature in a simple form and build intuitions for the relationship between parameter and functional priors.</p> <p>Most importantly, we consider how FSVI can measure a <em>predictive</em> divergence between the approximate and true posterior which is independent of parameter symmetries. (With parameter symmetries, I refer to different parameters that yield the same predictions, which is very common in over-parameterized neural networks: think of parameter symmetries like different paths leading to the same destination; they might look different but end up at the same predictions<d-footnote>Thanks to ChatGPT for this analogy! 🤗</d-footnote>.) Explaining this connection is one of the main goals of this article and will help you understand the relationships between DPI, FSVI, and other deep learning methods. As a concrete example and application, we relate FSVI to training with knowledge distillation and label entropy regularization: potentially more meaningful priors than the ones usually used in Bayesian neural networks<d-footnote>In many papers, an isotropic Gaussian is used because of its simplicity. Indeed, there are better alternatives, see Fortuin et al (2022)<d-cite key="fortuin2022bayesian"></d-cite> and Fortuin (2022)<d-cite key="fortuin2022priors"></d-cite>.</d-footnote>. This connection highlights the practical relevance of the theoretical concepts discussed in this post and will hopefully inspire the reader to view Bayesian deep learning from a new point of view.</p> <h3 id="tldr">TL;DR</h3> <p>The following sections summarize the key takeaways of this blog post. If they don’t make sense, don’t worry: they will after reading this post.</p> <h4 id="data-processing-inequality">Data Processing Inequality</h4> <p>The data processing inequality examines how information cannot increase due to processing. In information theory, it is usually stated based on a Markov chain of random variables \(X \rightarrow Y \rightarrow Z\) and their mutual information. We will look at different data processing inequalities that relate different distributions instead of different random variables. However, the blog posts in particular looks at the DPI when formulated using Kullback-Leibler (KL) divergences between distributions. I will use “🥬 divergence” in headings to add a bit of color. 😊</p> <p>Concretely, this KL DPI states that processing data stochastically can only reduce information. More formally:</p> <aside class="l-body box-important"> <p>Consider two distributions \(\qof{\W}\) and \(\pof{\W}\) over a random variable \(\W\). Let \(Y = \fof{\W}\) be a stochastic mapping from \(\W\) to \(Y\)—that is, we could imagine having a noise random variable \(\boldsymbol{Z}\) as a hidden input to \(\fof{\W}\), i.e. \(\fof{\W;\boldsymbol{Z}}\). Then the DPI states:</p> \[\Kale{\qof{\W}}{\pof{\W}} \ge \Kale{\qof{Y}}{\pof{Y}}.\] </aside> <p>That is, the KL divergence between \(\qof{Y}\) and \(\pof{Y}\) cannot be larger than the one between the original \(\qof{\W}\) and \(\pof{\W}\). Intuitively, the stochastic mapping \(\opf\) induces a bottleneck that reduces how well we can distinguish between \(\opp\) and \(\opq\). Finally we have equality when \(\Kale{\qof{\W \given Y}}{\pof{\W \given Y}} = 0\).</p> <p>The paper “<em>Understanding Variational Inference in Function-Space</em>” by Burt et al. (2021)<d-cite key="burt2021understanding"></d-cite> succinctly summarizes the DPI as follows:</p> <blockquote> The data processing inequality states that if two random variables are transformed in this way, they cannot become easier to tell apart. </blockquote> <h4 id="function-space-variational-inference">Function-Space Variational Inference</h4> <p>Generally, <em>variational inference</em> is a powerful technique for approximating complex Bayesian posteriors with simpler distributions. In its usual form, it optimizes an approximate, <em>variational</em> distribution to match the <em>Bayesian <strong>parameter</strong> posterior</em> as closely as possible. This way, it transforms the problem of Bayesian inference into an optimization problem.</p> <p>However, especially for deep neural networks, obtaining a good approximation of the parameter space can be difficult. One reason is the sheer size of the parameter space. Additionally, the parameterization of a neural network often contains many symmetries—different parameter configurations can lead to the same predictions of the model—that are not taken into account either.</p> <p>Here, <strong>Function-space variational inference (FSVI)</strong> side-steps some of these restrictions by only requiring that the variational distribution matches the <em>Bayesian <strong>predictive</strong> posterior</em>:</p> <p>If we define equivalence classes of model parameters according to the model predictions, FSVI aims to approximate the posterior distribution over <em>equivalence classes</em> of parameters \(\pof{[\w] \given \Dany}\) rather than the posterior over raw parameters \(\pof{\w \given \Dany}\). This is because \(\pof{[\w] \given \Dany}\) captures the meaningful aspects of the posterior, as it is invariant to the symmetries and equivalences in the parameter space that leave the likelihood unchanged.</p> <p>To achieve this, FSVI employs an implicit variational distribution \(\qof{[\w]}\) and minimizes an objective that is equivalent to regular variational inference on the equivalence classes \(\Kale{\qof{[\W]}}{\pof{[\W] \given \Dany}}\).</p> <p>By using an implicit variational distribution and leveraging the DPI, FSVI sidesteps the need to explicitly define the equivalence classes or specify a model that operates on them directly. Instead, FSVI relies on the fact that matching the predictive prior distributions in the limit of infinite data is sufficient to match the posteriors over equivalence classes overall. This leads to approximations we can connect to label entropy regularization and knowledge distillation.</p> <aside class="l-body box-important"> <p>Function-space variational inference (FSVI) is a principled approach to Bayesian inference that respects the inherent symmetries and equivalences in overparameterized models. It focuses on approximating the meaningful posterior \(\pof{[\w] \given \Dany}\) while avoiding the complexities of explicitly constructing and working with equivalence classes. The FSVI-ELBO regularizes towards a data prior:</p> \[\begin{aligned} \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}, \end{aligned}\] <p>unlike in regular variational inference, where we regularize towards a parameter prior \(\Kale{\qof{\W}}{\pof{\W}}\).</p> </aside> <h2 id="background-information-theoretic-notation">Background: Information-Theoretic Notation</h2> <p>Information theory deals with the communication of information<d-footnote>See the excellent <a href="https://colah.github.io/posts/2015-09-Visual-Information/">"Visual Information Theory"</a> by Chris Olah for a visual introduction to information theory.</d-footnote>. In this blog post, we use a unified information-theoretic notation to express various quantities related to probability distributions and their relationships<d-footnote>It largely follows "<a href="https://arxiv.org/abs/2106.12062">A Practical &amp; Unified Notation for Information-Theoretic Quantities in ML</a>".</d-footnote>. Here are some key concepts we will use:</p> <p>The <strong>information content</strong> of an event \(x\) is denoted as \(\Hof{x}\) and is defined as \(-\log \pof{x}\). It represents the minimum amount of information needed to describe the occurrence of \(x\) given an underlying probability distribution. In machine learning, this information content is often used as a minimization objective, represented as the negative log-likelihood or cross-entropy when averaged over a dataset.</p> <p>The <strong>entropy</strong> \(\Hof{X}\) of a random variable \(X\) is the expectation of its information content:</p> \[\Hof{X} \triangleq \E{\pof{x}}{\Hof{x}} = \E{\pof{x}}{-\log \pof{x}}.\] <p>The entropy measures the average amount of information needed to describe the random variable \(X\). It provides a measure of uncertainty or randomness associated with \(X\). We can similarly define the entropy of a conditional distribution \(\Hof{X \given Y}\) and the joint entropy \(\Hof{X, Y}\).</p> <p>The <strong>mutual information</strong> \(\MIof{X;Y}\) between two random variables \(X\) and \(Y\) is a measure of the amount of information that one random variable contains about the other. It is defined as:</p> \[\begin{aligned} \MIof{X;Y} &amp; \triangleq \Hof{X} - \Hof{X \given Y} \\ &amp;= \Hof{Y} - \Hof{Y \given X} \\ &amp;= \Hof{X} + \Hof{Y} - \Hof{X, Y}. \end{aligned}\] <p>We will also use the <strong>Kullback-Leibler divergence</strong> \(\Kale{\pof{X}}{\qof{X}}\) and the <strong>cross-entropy</strong> \(\CrossEntropy{\pof{X}}{\qof{X}}\):</p> \[\begin{aligned} \CrossEntropy{\pof{X}}{\qof{X}} &amp; = \E{\pof{x}}{-\log \qof{x}}\\ \Kale{\pof{X}}{\qof{X}} &amp; = \CrossEntropy{\pof{X}}{\qof{X}} - \Hof{X} \end{aligned}\] <p>The cross-entropy quantifies the average number of bits needed to encode samples drawn from the true distribution \(\pof{X}\) using a different distribution \(\qof{X}\). The Kullback-Leibler divergence is a measure of the difference between two probability distributions and captures the additional bits needed to encode samples from \(\pof{X}\) compared to encoding them using the true distribution \(\qof{X}\).</p> <p>Now that we have covered the notation, let’s delve into the data processing inequality.</p> <h2 id="data-processing-inequality-1">Data Processing Inequality</h2> <p>The <strong>data processing inequality (DPI)</strong> is a fundamental inequality in information theory that states the mutual information between two random variables cannot increase through processing. The original DPI is typically stated for a Markov chain of random variables \(X \rightarrow Y \rightarrow Z\) and relates the mutual information terms as follows:</p> \[\MIof{X;Y} \ge \MIof{X;Z}.\] <p>We can view \(\rightarrow\) as a processing or transition step that maps \(X\) to \(Y\) and \(Y\) to \(Z\), whereas the mapping can be deterministic or stochastic. The inequality tells us that processing the random variable \(X\) to obtain \(Y\) and further processing \(Y\) to obtain \(Z\) cannot increase the mutual information between \(X\) and \(Z\) compared to the mutual information between \(X\) and \(Y\).</p> <p>The following three scenarios illustrate the data processing inequality using different mappings:</p> <h3 id="example-image-processing-pipeline">Example: Image Processing Pipeline</h3> <p>Consider an image processing pipeline with the following steps. Let:</p> <ul> <li>\(X\) be the original image data;</li> <li>\(Y\) be a compressed version of the image; and</li> <li>\(Z\) be \(Y\) after adding blur and pixelation.</li> </ul> <p>In this case, \(X\) has more mutual information with \(Y\) than with \(Z\). The compression reduces information, but the image is still recognizable. However, after the additional processing of blurring and pixelating, the mutual information between \(X\) and \(Z\) is further reduced. This gives an intuitive example of how additional processing on data reduces the mutual information with the original data. Each processing step results in some loss of information.</p> <h3 id="example-supervised-learning">Example: Supervised Learning</h3> <p>Consider a supervised learning pipeline with the following steps. Let</p> <ul> <li>\(X\) be the input features;</li> <li>\(Y\) be the intermediate representations learned by the model; and</li> <li>\(Z\) be the model predictions.</li> </ul> <p>Here, \(X \rightarrow Y \rightarrow Z\) forms a Markov chain. The data processing inequality tells us that the mutual information between the inputs \(X\) and predictions \(Z\) cannot exceed the mutual information between the inputs \(X\) and intermediate representations \(Y\):</p> \[\MIof{X; Y} \geq \MIof{X; Z}.\] <p>This makes intuitive sense—the intermediate representations \(Y\) are obtained by processing the raw inputs \(X\), so they cannot contain more information about \(X\) than \(X\) itself. The predictions \(Z\) are obtained by further processing \(Y\), so additional information may be lost, reducing the mutual information with the original inputs \(X\).</p> <p>As a more concrete example, consider an image classification model. Let:</p> <ul> <li>\(X\) be the input images;</li> <li>\(Y\) be the activations of the convolutional layers; and</li> <li>\(Z\) be predicted image labels.</li> </ul> <p>The convolutional layers will extract features from the input images, but cannot extract more information than present in the original images. The predicted labels are obtained by further processing these convolutional features, so may lose some fine-grained information about the original inputs.</p> <h3 id="example-autoencoders">Example: Autoencoders</h3> <p>An autoencoder compresses the input \(X\) into a latent code \(Y\) and then tries to reconstruct the original input from the code, producing \(\hat{X}\). Let:</p> <ul> <li>\(X\) be the input;</li> <li>\(Y\) be the latent code; and</li> <li>\(\hat{X}\) be the reconstruction;</li> </ul> <p>The data processing inequality tells us again:</p> \[\MIof{X; Y} \geq \MIof{X; \hat{X}}.\] <p>The latent code \(Y\) is obtained by compressing \(X\), so cannot contain more information. The reconstruction \(\hat{X}\) tries to recover \(X\) from \(Y\), but some information may be lost, reducing the mutual information with \(X\).</p> <p>Intuitively, autoencoders try to preserve as much mutual information between inputs \(X\) and reconstructions \(\hat{X}\) as possible by learning latent representations \(Y\) that compress inputs without losing too much information. The data processing inequality quantifies this information bottleneck.</p> <h3 id="proof-of-the-dpi">Proof of the DPI</h3> <p>The proof is simple and connects the DPI to another important inequality.</p> <p>First we note that the Markov Chain implies the following factorization of the joint distribution:</p> \[\pof{x, y, z} = \pof{x} \pof{y \given x} \pof{z \given y}.\] <p>Using this factorization, we can express the mutual information terms:</p> \[\begin{aligned} \MIof{X;Y} &amp;= \Hof{X} - \Hof{X \given Y} \\ &amp;\ge \Hof{X} - \Hof{X \given Z} \\ &amp;= \MIof{X;Z}. \end{aligned}\] <p>This relies on \(\Hof{X \given Y} \le \Hof{X \given Z}\). Why is this true?</p> <p>We have the following chain of inequalities:</p> \[\Hof{X \given Y} = \underbrace{\MIof{X ; Z \given Y}}_{\overset{(1)}{=}0} + \Hof{X \given Y, Z} \overset{(2)}{\le} \Hof{X \given Z}.\] <p><strong>(1)</strong> follows from the Markov chain property: when \(X \rightarrow Y \rightarrow Z\), \(X\) does not depend on \(Z\) at all when conditioned on \(Y\); and <strong>(2)</strong> follows from the fact that conditioning reduces entropy, i.e. \(\Hof{A \given B} \le \Hof{A}.\)</p> <p>The equality gap \(\Hof{X \given Y, Z} - \Hof{X \given Z}\) corresponds to the mutual information \(\MIof{X ; Y \given Z}\). This mutual information measures the extra information about \(X\) contained in \(Y\) that is not already conveyed by \(Z\). It is zero if and only if \(X \rightarrow Z \rightarrow Y\) forms a Markov chain, indicating that \(Z\) is a sufficient statistic for \(X\).</p> <details> <summary>Proof of <i>(2)</i> "Conditioning Reduces Entropy":</summary> <p>We can easily show that conditioning reduces entropy by using the non-negative property of the mutual information:</p> <p>\(\begin{aligned} 0 &amp;\le \Kale{\pof{X,Y}}{\pof{X}\pof{Y}} \\ &amp;= \MIof{X;Y} \\ &amp;= \Hof{X} - \Hof{X \given Y} \\ \implies \Hof{X \given Y} &amp;\le \Hof{X}. \end{aligned}\)</p> </details> <p>The fact that conditioning reduces entropy, \(\Hof{X} \ge \Hof{X \given Y}\), is an important property by itself and is reminiscent of the data processing inequality. The conditional entropy \(\Hof{X \given Y}\) quantifies the remaining uncertainty about \(X\) after observing \(Y\). If \(X\) and \(Y\) are independent, then \(\Hof{X} = \Hof{X \given Y}\), as knowing \(Y\) does not provide any information about \(X\). On the other hand, if \(Y\) completely determines \(X\), then \(\Hof{X \given Y} = 0\), as there is no remaining uncertainty about \(X\) once \(Y\) is known. In general, conditioning can only reduce the uncertainty about \(X\), but it does not necessarily reduce it to zero.</p> <p>Let’s move on and consider the KL data processing inequality.</p> <h2 id="-data-processing-inequality">🥬 Data Processing Inequality</h2> <p>A similar DPI can be expressed for different distributions \(\pof{x}\) and \(\qof{x}\) of the same random variable and the KL divergence between them. This DPI states that if we evolve two distributions using the same <em>transition function</em>, they cannot become less similar. The KL divergence is sometimes also referred to as “relative entropy”, so we could also call this the “<em>relative data processing inequality</em>”.</p> <p>This can be formalized for distributions \(\pof{x}\) and \(\qof{x}\) and a stochastic transition function \(X \overset{\fof{y \given x}}{\longrightarrow} Y\). Here, we use that such a stochastic mapping \(Y = \fof{X}\) is equivalent to having a probability (density) \(\fof{y \given x}\):</p> \[\Kale{\pof{X}}{\qof{X}} \ge \Kale{\pof{Y}}{\qof{Y}},\] <p>where \(\pof{y \given x} = \fof{y \given x} = \qof{y \given x}\). The marginals after the transition are \(\pof{y} = \E{\pof{x}}{\fof{y \given x}}\) and \(\qof{y} = \E{\qof{x}}{\fof{y \given x}}\), so more explicitly:</p> \[\Kale{\pof{X}}{\qof{X}} \ge \Kale{\E{\pof{x}}{\fof{Y \given x}}}{\E{\qof{x}}{\fof{Y \given x}}}.\] <p>In their book <a href="https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959">Elements of Information Theory</a>, Thomas and Cover describe this as “relative entropy never increases” and relate it to the second law of thermodynamics.</p> <h3 id="example-comparing-image-distributions">Example: Comparing Image Distributions</h3> <p>As an example, let:</p> <ul> <li>\(\pof{x}\) be the true distribution of images in a dataset;</li> <li>\(\qof{x}\) be a generative model that tries to mimic \(\pof{x}\); and</li> <li>\(\fof{y \given x}\) be a function that thresholds images \(x\) into bilevel black and white images \(y\).</li> </ul> <p>Then \(\pof{y}\) and \(\qof{y}\) will be more difficult to distinguish after the thresholding operation than \(\pof{x}\) and \(\qof{x}\). Converting to black and white images has lost information that could help distinguish the real and generated distributions.</p> <p>This provides some intuition for why the KL divergence between distributions decreases under a shared stochastic mapping, as formalized by the KL data processing inequality. Processing through \(\fof{y \given x}\) makes the distributions harder to tell apart.</p> <h3 id="counter-example-bayesian-inference">Counter-Example: Bayesian Inference</h3> <p>It might be inviting to think that this data processing inequality also applies to Bayesian inference, that is updating the model parameters based on new evidence. Then, we could argue that if two agents start with different prior beliefs but update based on the same evidence, their posterior beliefs will become more similar. However, this intuition is flawed: the data processing inequality does not apply to Bayesian inference.</p> <p>Let’s walk through why. Consider:</p> <ul> <li>\(\pof{\w}\) be an agent’s prior belief;</li> <li>\(\qof{\w}\) be another agent’s different prior;</li> <li>\(\pof{\w\given x}\) is the posterior after observing data \(x\); and</li> <li>\(\qof{\w\given x}\) is the other agent’s posterior.</li> </ul> <p>The priors \(\pof{\w}\) and \(\qof{\w}\) may have large divergence, representing very different initial beliefs. However, when conditioning on the same data \(x\), the KL divergence between \(\pof{\w \given x}\) and \(\qof{\w \given x}\) could increase or decrease—the data processing inequality does not give us any guarantee.</p> <p>This is because \(\pof{\w}\) and \(\qof{\w}\) are not evolving under the same stochastic mapping. Rather, each prior is mapped to its respective posterior via Bayes’ rule, which operates differently on \(\opp\) and \(\opq\):</p> \[\begin{aligned} \pof{\w \given x} &amp;= \frac{\pof{x \given \w}}{\pof{x}} \, \pof{\w}\\ \qof{\w \given x} &amp;= \frac{\qof{x \given \w}}{\qof{x}} \, \qof{\w}. \end{aligned}\] <p>Even assuming that both agents have the same internal model, that is they use the same likelihood \(\pof{x \given \w} = \qof{x \given \w}\), the priors \(\pof{\w}\) and \(\qof{\w}\) will still influence the posterior distributions differently because they lead to different evidence terms \(\pof{x}\) and \(\qof{x}\):</p> \[\begin{aligned} \pof{x} &amp;= \E{\pof{\w}}{\pof{x \given \w}}\\ \qof{x} &amp;= \E{\qof{\w}}{\qof{x \given \w}}. \end{aligned}\] <p>Thus, the correct intuition is that observing the same data \(x\) does not necessarily bring the posterior beliefs closer together—they depend on the interplay between their specific priors and likelihoods. The data processing inequality does not directly apply to this Bayesian updating scenario:</p> \[\Kale{\qof{\W}}{\pof{\W}} {\color{red}{\not\ge}} \Kale{\qof{\W \given \mathcal{D}}}{\pof{\W \given \mathcal{D}}},\] <p>This counterexample highlights the importance of precisely understanding the assumptions underlying conceptual principles like the DPI. While the DPI provides insight about information dynamics in many cases, it does not universally apply, as exemplified here by Bayesian updating under different priors. As always, bear in mind that:</p> <aside class="l-body box-note"> <p>Identifying counterexamples sharpens our comprehension of the true meaning and limitations of information-theoretic inequalities.</p> </aside> <p>As we currently also seem to experience a world of increasing polarization, this counterexample might also serve as a reminder that different priors can lead to different beliefs, even when observing the same evidence. This is a fundamental aspect of Bayesian inference and the scientific method.</p> <h3 id="proofs-of-the--dpi">Proofs of the 🥬 DPI</h3> <p>We will prove this inequality in two different ways. First, we will develop a “brute-force” proof, and then we will look at a more elegant proof that follows Thomas and Cover. Importantly, we will also consider the equality case in detail.</p> <h4 id="brute-force-proof">Brute-force Proof</h4> <p>If \(\opp\) does not have support in \(\opq\), the inequality is trivially true because then \(\Kale{\pof{Y}}{\qof{Y}}=\infty\).</p> <p>Thus, let’s now assume that \(\opp\) has support in \(\opq\). Then, we can brute-force using the definitions, starting from the cross-entropy:</p> \[\begin{aligned} \CrossEntropy{\pof{Y}}{\qof{Y}}&amp;=\CrossEntropy{\pof{Y}}{\E{\qof{x}}{\pof{Y \given x}}}\\ &amp;=\CrossEntropy{\pof{Y}}{\E{\qof{x}}{\frac{\pof{x \given Y}\pof{Y}}{\pof{x}}}}\\ &amp;=\CrossEntropy{\pof{Y}}{\E{\pof{x \given Y}}{\frac{\qof{x}}{\pof{x}}}}+\CrossEntropy{\pof{Y}}{\pof{Y}}\\ &amp;\overset{(1)}{=}\CrossEntropy{\pof{Y}}{\E{\pof{x \given Y}}{\frac{\qof{x}}{\pof{x}}}}+\xHof{\pof{Y}}\\ &amp;\overset{(2)}{\le}\CrossEntropy{\pof{X, Y}}{\frac{\qof{X}}{\pof{X}}}+\xHof{\pof{Y}}\\ &amp;\overset{(3)}{=}\CrossEntropy{\pof{X}}{\frac{\qof{X}}{\pof{X}}}+\xHof{\pof{Y}}\\ &amp;\overset{(4)}{=}\Kale{\pof{X}}{\qof{X}}+\xHof{\pof{Y}}\\ \iff \Kale{\pof{Y}}{\qof{Y}}&amp;\le\Kale{\pof{X}}{\qof{X}}, \end{aligned}\] <p>where we have used <strong>(1)</strong> that the cross-entropy of a distribution with itself is just the entropy, <strong>(2)</strong> that the cross-entropy is convex and we can apply Jensen’s inequality, <strong>(3)</strong> that the RHS side of the cross-entropy does not depend on \(Y\) and we can trivially marginalize it out, and <strong>(4)</strong> that the definition of the Kullback-Leibler divergence is equivalent an (unnormalized) cross-entropy over a fraction.</p> <p>This makes it difficult to extract the case for equality, however.</p> <h4 id="equality-case">Equality Case</h4> <p>We have only one inequality in above proof, and it stems from applying Jensen’s inequality. Remembering the equality case for Jensen’s inequality, we recall:</p> <aside class="l-body box-note"> <p>We have equality in Jensen’s inequality:</p> \[g(\E{\pof{x}}{f(x)}) \le \E{\pof{x}}{g(f(x))},\] <p>iff \(g\) is linear (commutes with addition)<d-footnote>Specifically, linear in the convex hull of the support of $\pof{x}$.</d-footnote> or \(f(x)\) is constant.</p> </aside> <p>For <strong>(2)</strong>, this is sadly slightly more complex than it might seem on first glance. Let’s unwrap the term:</p> \[\CrossEntropy{\pof{Y}}{\E{\pof{x \given Y}}{\frac{\qof{x}}{\pof{x}}}} = \E{\pof{y}}{-\log \E{\pof{x \given y}}{\frac{\qof{x}}{\pof{x}}}}.\] <p>We take an expectation over \(\pof{y}\), so we need to look at almost all \(\pof{x \given y} \not= 0\) for (almost all) \(\pof{y} \not= 0\) separately to consider equality. \(-\log x\) is strictly convex—and thus not linear—so we need \(f(x) = \frac{\qof{X}}{\pof{X}}\) to be constant for any fixed \(y\) with \(\pof{y} \not= 0\)—only then have we equality in Jensen’s inequality.</p> <p>In the following, I will limit myself to the discrete case to avoid having to deal with measure theory<d-footnote>I currently don't have a good 'toolbox' to express simple ideas cleanly in measure theory. I'm working on it.</d-footnote>. To obtain equality, for all \(y\) with \(\pof{y} \not= 0\) (i.e. we have support) and for all \(x_1, x_2\) with \(\pof{x_1 \given y}, \pof{x_2 \given y} \not= 0\), we need \(\frac{\qof{x_1}}{\pof{x_1}} = \frac{\qof{x_2}}{\pof{x_2}}\). Equivalently (for the reader, why is then \(\pof{x_1} \not= 0?\)):</p> \[\begin{aligned} \frac{\qof{x_1}}{\pof{x_1}} &amp;= \frac{\qof{x_2}}{\pof{x_2}} \\ \iff \qof{x_1} &amp;= \frac{\qof{x_2}}{\pof{x_2}} \, \pof{x_1} \\ \end{aligned}\] <p>This means that \(\qof{x} = C_y \pof{x}\) piecewise for all \(x\) for which \(\pof{x \given y} \not= 0\) for some fixed \(y\) with \(\pof{y} \not= 0\). That is if we keep \(y\) fixed, all the \(x\) for which \(\pof{x \given y} \not= 0\) have the same constant factor \(C_y\). Then for all \(y\) with \(\pof{y} \not= 0\), we have equality and overall equality in <strong>(2)</strong>.</p> <p>If for any \(x\) there are multiple \(y\), e.g. \(y_1, y_2\) for which \(\pof{x \given y} \not= 0\), then we have \(C_{y_1} = C_{y_2}\).</p> <p>As an example, at the simplest, if this is the case for all \(y\), then \(C_y = 1\) constant.</p> <p>As a side-note, this is a great reason why we often require full support for distributions as we then can avoid these piecewise constant factors (and the headaches they might cause).</p> <h4 id="simpler-elegant-proof">Simpler Elegant Proof</h4> <p>Thomas and Cover provide a beautifully simple proof:</p> <aside class="l-body box-note"> <p>Using the chain rule of the KL divergence:</p> \[\begin{aligned} \Kale{\pof{X, Y}}{\qof{X, Y}} &amp;= \Kale{\pof{X}}{\qof{X}} \\ &amp;+ \Kale{\pof{Y \given X}}{\qof{Y \given X}}, \end{aligned}\] <p>and its symmetry, we have:</p> \[\begin{aligned} &amp;\Kale{\pof{X}}{\qof{X}} + \underbrace{\Kale{\pof{Y\given X}}{\qof{Y \given X}}}_{=\Kale{\fof{Y\given X}}{\fof{Y \given X}}=0}\\ &amp;\quad =\Kale{\pof{X, Y}}{\qof{X, Y}}\\ &amp;\quad =\Kale{\pof{Y}}{\qof{Y}}+\underbrace{\Kale{\pof{X \given Y}}{\qof{X \given Y}}}_{\ge 0}\\ &amp;\quad \ge \Kale{\pof{Y}}{\qof{Y}}. \end{aligned}\] <p>We have equality exactly when \(\pof{x \given y} = \qof{x \given y}\) for (almost) all \(x, y.\)</p> </aside> <p>What does this mean? Whereas \(\fof{y \given x}\) is the ‘forward’ transition function, \(\pof{x \given y}\) and \(\qof{x \given y}\) are the ‘backward’ transition functions. We only have equality when the backward transition functions are equal (almost everywhere).</p> <p>The statement on equality is not very informative yet though, so we have to put in a bit more work. Again, this is written for the discrete case.</p> <p>This time we explicitly use Bayes’ rule to connect the forward and backward transition functions. First, we have to fix \(y\) such that \(\pof{y} \not= 0\) (i.e. \(y\) is in the support of \(\pof{y}\)) and then \(\qof{y} \not=0\). We have:</p> \[\begin{aligned} \pof{x \given y} &amp;= \qof{x \given y} \\ \overset{\text{ass. }\pof{y} \not= 0}{\iff} \frac{\fof{y \given x}\pof{x}}{\pof{y}} &amp;= \frac{\fof{y \given x}\qof{x}}{\qof{y}} \\ \overset{\text{ass. }\fof{y \given x}\not= 0}{\iff} \frac{\pof{x}}{\pof{y}} &amp;= \frac{\qof{x}}{\qof{y}} \\ \iff \pof{x} &amp;= \frac{\pof{y}}{\qof{y}} \, \qof{x}. \end{aligned}\] <p>For a given \(y\) with \(\pof{y} \not=0\), for the equality case, we see that for all \(x\) with \(\fof{y \given x} \not= 0\), \(\pof{x}\) and \(\qof{x}\) have to be coupled via piecewise constant factors.</p> <p>As another example, if \(\fof{y \given x} \not=0\) (has full support) for all possible \(x\), for the equality case we have \(\pof{x} = \qof{x}\).</p> <p>Compared to the previous equality case, we went a bit deeper and rewrote the conditions to consider the ratios between \(x\) and \(y\). Note we could have shown the same thing in the “brute-force” proof, too.</p> <p>Altogether, we have see that both \(x\) and \(y\) are modulated by the same constant factor between \(\pof{\cdot}\) and \(\qof{\cdot}\). Essentially, this tells us that we could split our support into unconnected sub-domains and examine each individually for the equality case.</p> <aside class="l-body box-note"> <p>One technicality is the question of what \(\pof{y \given x}\) is when \(\pof{x} = 0\). We could define it to be anything we want, but really it is undefined.</p> <p>Previously, we said that we want \(\pof{y \given x} = \fof{y \given x}\) and \(\qof{y \given x} = \fof{y \given x}\), but, given the above, we only need these equalities to hold where \(\pof{x} \not= 0\) and \(\qof{x} \not= 0\), respectively.</p> <p>On the other hand, \(\pof{x} = 0\) still implies \(\pof{x \given y} = 0\).</p> </aside> <h3 id="overall-statement">Overall Statement</h3> <p>We have the following overall statement:</p> <aside class="l-body box-warning"> <p>Given \(\pof{x}\) and \(\qof{x}\) and shared transition function \(\fof{y \given x}\) for the model \(X \rightarrow Y\), the relative KL data processing inequality is:</p> \[\Kale{\pof{X}}{\qof{X}} \ge \Kale{\pof{Y}}{\qof{Y}},\] <p>When \(\pof{x} \ll \qof{x}\), we have equality when \(\pof{X \given Y} = \qof{X \given Y}\).</p> </aside> <p>(\(\pof{x} \ll \qof{x}\) means that \(\qof{x} &gt; 0\) implies \(\pof{x} &gt; 0\), so the KL divergence is not \(\infty\).) But more precisely, for \(\pof{x} \ll \qof{x}\), we have equality when:</p> \[\forall y, \pof{y} \not= 0 \exists C_y \in \mathbb{R}_{&gt; 0} \forall x, \fof{y \given x}\not=0\colon \pof{x} = C_y \, \qof{x}.\] <h2 id="other-data-processing-inequalities">Other Data Processing Inequalities</h2> <p>Now, we can use these ideas to derive a few additional results and even close the circle to the original data processing inequality.</p> <h3 id="jensen-shannon-divergence">Jensen-Shannon Divergence</h3> <p>The KL divergence is not a metric: the triangle inequality does not hold, and it is not symmetric.</p> <p>However, we can symmetrize it to obtain the <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence">Jensen-Shannon divergence (JSD)</a>. The JSD is defined as the mean of the two KL divergences of the two distributions from their average. In essence, it makes the KL divergence symmetric:</p> \[\begin{aligned} \fof{x} &amp;= \frac{\pof{x} + \qof{x}}{2}\\ \JSD{\pof{x}}{\qof{x}} &amp;= \frac{1}{2} \Kale{\pof{x}}{\fof{x}} + \frac{1}{2} \Kale{\qof{x}}{\fof{x}}. \end{aligned}\] <p>Similar approaches can be used to “symmetrize” other concepts; for example matrices: \(\frac{1}{2} A + \frac{1}{2} A^T\) is also symmetric by construction for any matrix \(A\).</p> <p>The JSD is still not a metric, but the square root of the Jensen-Shannon divergence is symmetric and satisfies the triangle inequality and gives us the <em>Jensen-Shannon distance</em>, a metric.</p> <h3 id="jsd-dpi">JSD-DPI</h3> <p>We can also obtain a data processing inequality for the Jensen-Shannon divergence and the Jensen-Shannon distance:</p> <aside class="l-body box-warning"> <p>Given \(\pof{x}\) and \(\qof{x}\) and shared transition function \(\fof{y \given x}\) for the model \(X \rightarrow Y\), the Jensen-Shannon divergence data processing inequality is:</p> \[\JSD{\pof{X}}{\qof{X}} \ge \JSD{\pof{Y}}{\qof{Y}},\] <p>with equality exactly when \(\pof{x \given y} = \qof{x \given y}\) almost everywhere.</p> </aside> <p>The proof uses the KL data processing inequality:</p> \[\begin{aligned} \JSD{\pof{X}}{\qof{X}} &amp;= \frac{1}{2} \Kale{\pof{X}}{\fof{X}} + \frac{1}{2} \Kale{\qof{X}}{\fof{X}}\\ &amp;\ge \frac{1}{2} \Kale{\pof{Y}}{\fof{Y}} + \frac{1}{2} \Kale{\qof{Y}}{\fof{Y}}\\ &amp;= \JSD{\pof{Y}}{\qof{Y}}. \end{aligned}\] <p>We verify \(\fof{y} = \frac{\pof{y} + \qof{y}}{2}\) is the average of \(\pof{y}\) and \(\qof{y}\):</p> \[\begin{aligned} \fof{y} &amp;= \E{\fof{x}}{\fof{y \given x}}\\ &amp;= \E{\frac{\pof{x}+\qof{x}}{2}}{\fof{y \given x}}\\ &amp;= \frac{1}{2} \E{\pof{x}}{\fof{y \given x}} + \frac{1}{2} \E{\qof{x}}{\fof{y \given x}}\\ &amp;= \frac{1}{2} \pof{y} + \frac{1}{2} \qof{y}. \end{aligned}\] <p>Finally, \(\pof{x}, \qof{x} \ll \fof{x}\), and the equality condition of the KL data processing inequality gives us:</p> \[\begin{aligned} &amp;\Kale{\pof{X \given Y}}{\fof{X \given Y}} = 0 &amp;\\ \land \quad &amp;\Kale{\qof{X \given Y}}{\fof{X \given Y}} = 0 &amp;\\ \iff &amp;\pof{x \given y} = \fof{x \given y} \land \qof{x \given y} = \fof{x \given y}&amp; \forall x,y \\ \iff &amp;\pof{x \given y} = \qof{x \given y}&amp; \forall x,y. \end{aligned}\] <h3 id="mutual-information">Mutual Information</h3> <p>The JSD can also be expressed as a mutual information. For \(\begin{aligned} Z &amp;\sim \mathrm{Bernoulli}(\frac{1}{2}) = \fof{Z} \\ X \given Z = 0 &amp;\sim \pof{x}\\ X \given Z = 1 &amp;\sim \qof{x}, \end{aligned}\)</p> <p>we have:</p> \[\JSD{\pof{X}}{\qof{X}} = \MIof{X;Z}.\] <p>This follows from rewriting the mutual information as a KL divergence:</p> \[\begin{aligned} \MIof{X;Z} &amp;= \Kale{\fof{X \given Z}}{\fof{X}}\\ &amp;= \E{\fof{z}} {\Kale{\fof{X \given Z = z}}{\fof{X}}}\\ &amp;= \frac{1}{2} \Kale{\pof{x}}{\fof{x}} + \frac{1}{2} \Kale{\qof{x}}{\fof{x}}\\ &amp;= \JSD{\pof{X}}{\qof{X}}. \end{aligned}\] <p>We can generalize this to the Markov chain \(Z \rightarrow X \rightarrow Y\) with \(\fof{z, x, y} = \fof{z} \fof{x \given z} \fof{y \given x}\) for any distribution \(\fof{z}\):</p> \[\begin{aligned} \MIof{X;Z} &amp;= \Kale{\fof{X \given Z}}{\fof{X}}\\ &amp;= \E{\fof{z}} {\Kale{\fof{X \given z}}{\fof{X}}}\\ &amp;\overset{(1)}{\ge} \E{\fof{z}} {\Kale{\fof{Y \given z}}{\fof{Y}}}\\ &amp;= \Kale{\fof{Y \given Z}}{\fof{Y}}\\ &amp;= \MIof{Y;Z}, \end{aligned}\] <p>where \((1)\) follows from the KL data processing inequality.</p> <p>This is just the data processing inequality we presented initially. We have gone full circle!</p> <p>The equality gap (<em>Jensen gap</em>) is \(\Kale{\fof{X \given Y, Z}}{\fof{X \given Y}}\), and we have equality when:</p> \[\begin{aligned} \Kale{\fof{X \given Y, Z}}{\fof{X \given Y}} &amp;= 0\\ \iff \MIof{X;Z \given Y} &amp;= 0. \end{aligned}\] <p>This is exactly when \(X\) is independent of \(Z\) given \(Y\). (\(Y\) is a sufficient statistic in that case.)</p> <h2 id="function-space-variational-inference-1">Function-Space Variational Inference</h2> <p>So far we’ve explored the foundational aspects of the data processing inequality (DPI) and its extended forms, in particular the KL data processing inequality. Through detailed derivations and intuitive examples, we’ve demonstrated how these inequalities can be applied, emphasizing their significance and limitations. Specifically, we’ve shown how the KL data processing inequality relates to the reduction in information as data is processed. The examples and counterexample have hopefully demonstrated the nuances of applying these inequalities in different contexts.</p> <p>This exploration sets the stage for diving into function-space variational inference and building up a robust understanding of it, leveraging the insights gained about the DPI and its implications in Bayesian deep learning.</p> <h3 id="problem-setting--notation">Problem Setting &amp; Notation</h3> <p>In the following, we will consider a classification task with cross-entropy loss, and we will use the following the random variables and distributions:</p> <ul> <li>\(\y\) is the label,</li> <li>\(\x\) is the input,</li> <li>\(\qof{\y \given \x}\) is the predictive distribution we want to learn,</li> <li>\(\pdata{\y \given \x}\) is the data distribution,</li> <li>\(\Dany\) is the (training) dataset, and</li> <li>\(C\) is the number of classes.</li> </ul> <p>The probabilistic model is:</p> \[\pof{\y, \w \given \x} = \pof{\y \given \x, \w} \, \pof{\w}.\] <p>As before, I use upper-case letters for random variables, which we take an expectation over, e.g. in the KL divergence, and lower-case letters when I’m referring to specific observations or values that could be substituted (with the exception of \(\Dany\)).</p> <h3 id="chain-rule-of-the--divergence--dpi">Chain Rule of the 🥬 Divergence &amp; DPI</h3> <p>An important property of the KL divergence is the chain rule:</p> \[\begin{aligned} &amp;\Kale{\qof{\Y_n,...,\Y_1}}{\pof{\Y_n,...,\Y_1}} \\ &amp;\quad = \sum_{i=1}^n \Kale{\qof{\Y_i \given \Y_{i-1}, ..., \Y_1}}{\pof{\Y_i \given \Y_{i-1}, ..., \Y_1}}. \end{aligned}\] <p>The chain rule yields a <em>chain inequality</em> for the DPI as well:</p> \[\begin{aligned} \Kale{\qof{\W}}{\pof{\W}} &amp;\ge \Kale{\qof{\Y_n,...,\Y_1}}{\pof{\Y_n,...,\Y_1}}\\ &amp;\ge \Kale{\qof{\Y_{n-1},...,\Y_1}}{\pof{\Y_{n-1},...,\Y_1}}\\ &amp;\ge \Kale{\qof{\Y_1}}{\pof{\Y_1}}, \end{aligned}\] <p>where we start from the KL DPI and then apply the chain rule.</p> <h3 id="deriving-the-functional-elbo">Deriving the Functional ELBO</h3> <p>The DPI has an intriguing connection to FSVI. Let’s say we want to approximate a Bayesian posterior \(\pof{\w \given \Dany}\) with a variational distribution \(\qof{\w}\). In standard VI, we would minimize \(\Kale{\qof{\W}}{\pof{\W \given \Dany}}\) to match the variational distribution to the Bayesian posterior. Specifically:</p> \[\begin{aligned} &amp;\Kale{\qof{\W}}{\pof{\W \given \Dany}} =\\ &amp;\quad = \underbrace{\E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\W}}{\pof{\W}}}_{\text{Evidence}\ \text{Bound}} + \log \pof{\Dany} \ge 0 \\ &amp;\iff \underbrace{-\log \pof{\Dany}}_{=\xHof{\pof{\Dany}}} \le \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\W}}{\pof{\W}}. \end{aligned}\] <p>This is an information-theoretic evidence (upper) bound on the information content \(-\log \pof{\Dany}\) of the data \(\Dany\) under the variational distribution \(\qof{\w}\), which we can minimize as an objective to approximiate \(\pof{\w \given \Dany}\) via \(\qof{\w}\).</p> <p>In more probability-theory inspired literature, the negative of this bound is called the <em>evidence lower bound (ELBO)</em> and is maximized.</p> <p>Both the ELBO and the information-theoretic evidence upper-bound are equivalent, and we can use either objective, but the information-theoretic perspective is obviously superior 🙃 I’ll refer to this as evidence bound from now on.</p> <p>In FSVI (with a caveat I detail below), we apply the DPI to the prior KL divergence term and obtain a “functional” version of the evidence bound:</p> \[\begin{aligned} \Kale{\qof{\W}}{\pof{\W}} \ge \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}, \end{aligned}\] <p>where \(\Y... \given \x...\) are (finite or infinite) sets of samples. That is, we do not only optimize marginal distributions but also joint distributions.</p> <aside class="l-body box-note"> <p>For more on the specific differences between marginal and joint objectives, the workshop paper by Kirsch et al (2022)<d-cite key="kirsch2022marginal"></d-cite> provides conceptual thoughts on the differences between marginal and joint distributions. It builds on the following important papers:</p> <ul> <li>“The neural testbed: evaluating joint predictions” by Osband et al, 2022<d-cite key="osband2022neural"></d-cite></li> <li>“Evaluating high-order predictive distributions in deep learning” by Osband et al, 2022<d-cite key="osband2022evaluating"></d-cite></li> <li>“Beyond marginal uncertainty: how accurately can Bayesian regression models estimate posterior predictive correlations?” by Wang et al, 2021 <d-cite key="wang2021beyond"></d-cite></li> </ul> </aside> <p>The resulting objective:</p> \[\begin{aligned} \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}} \end{aligned}\] <p>is equal to the (negative) <em>functional ELBO (fELBO)</em> in “<em>Functional variational Bayesian neural networks</em>” by Sun et al. (2019)<d-cite key="sun2019functional"></d-cite>—with caveats that we discuss below.</p> <h3 id="choosing-the-coreset-x">Choosing the “Coreset” \(\x...\)</h3> <p>One important detail is the question of how to choose the \(\x...\):</p> <p>Ideally, we want to choose them such that the DPI inequality is as tight as possible.</p> <p>Given the chain inequality, it is obvious that the larger the set \(\x...\), the tighter the inequality will be. Hence, if we could choose an infinite set of points well, we might be able to get the tightest possible inequality. However, this might not be tractable, and in practice, it is often not.</p> <p>Some works take a supremum over finite subsets of a certain size, essentially building a core-set as an approximation (Rudner et al., 2022a<d-cite key="rudner2022tractable"></d-cite>/b<d-cite key="rudner2022continual"></d-cite>); others take an expectation over finite sets of input samples (Sun et al., 2019)<d-cite key="sun2019functional"></d-cite>, which is not necessarily yielding the tightest inequality but provides an unbiased estimate; while again other works focus on finite datasets for which the all points can be taken into account (Klarner et al., 2023)<d-cite key="klarner2023drug"></d-cite>.</p> <p>We will discuss the tightness of the inequality and the implications in the data limit below.</p> <p>Focusing on the most important aspect of FSVI, we observe:</p> <aside class="l-body box-important"> <p>Instead of a parameter prior (term) \(\Kale{\qof{\W}}{\pof{\W}}\), we have a functional or <em>predictive</em> prior (term):</p> \[\Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}.\] </aside> <h3 id="application-to-continual-learning">Application to Continual Learning</h3> <p>When we directly optimize the KL divergence on a finite input dataset, for example, we align \(\opq\) with the prior of \(\opp\) where it matters most: on the predictions of the observed data.</p> <p>This is of particular interest in continual learning, where the prior for the next task is chosen to be the posterior from the previous task. In this case, the functional ELBO can be used to approximate the posterior of the previous model while incorporating new data.</p> <p>For two great papers that are very readable and provide further insights, see “<em>Continual Learning via Sequential Function-Space Variational Inference</em>“<d-cite key="rudner2022continual"></d-cite> and “<em>Tractable Function-Space Variational Inference in Bayesian Neural Networks</em>“<d-cite key="rudner2022tractable"></d-cite>, both by Rudner et al. (2022).</p> <h2 id="comparison-to-fsvi-in-the-literature">Comparison to FSVI in the Literature</h2> <aside class="l-body box-error"> <p>An important detail is that in above FSVI papers, the DPI is applied to the <em>logits</em> not the <em>probabilities</em>. That is, we have a distribution over logits, which adds an extra layer of uncertainty. Unlike the class probabilities, which we can also obtain from a deterministic neural network, thanks to the semantic interpretation of the outputs as distribution \(\pof{\y \given \x, \w}\), a deterministic neural network only provides us with deterministic logits and not a distribution over them. Only a (more) Bayesian approach can provide this.</p> <p>In general, we have the following inequality chain that relates the KL divergence between the logits \(\L\), which is a vector unlike \(\Y\), and the KL divergence between the output probabilities for \(\Y\) to the KL divergence between the parameters \(\W\):</p> \[\begin{aligned} \Kale{\qof{\W}}{\pof{\W}} &amp;\ge \Kale{\qof{\L...\given \x...}}{\pof{\L...\given \x...}} \\ &amp;\ge \Kale{\qof{\Y...\given \x...}}{\pof{\Y...\given \x...}}. \end{aligned}\] <p>This follows that from applying the DPI twice, as the logits are functions of the weights and the weights are functions of the logits.</p> </aside> <p>In practice, Rudner et al. (2022) and its continual learning follow-up linearize the logits<d-footnote>The logits are the final activations of the neural network before applying the softmax function (in multi-class classification). They are not to be confused with the pre-logits, e.g. embeddings before the final linear layer.</d-footnote> (similar to a Laplace approximation) and use the DPI to show (in their notation):</p> \[\mathbb{D}_{\mathrm{KL}}\left(q_{f(\cdot ; \boldsymbol{\Theta})} \| p_{f(\cdot ; \boldsymbol{\Theta})}\right) \leq \mathbb{D}_{\mathrm{KL}}\left(q_{\Theta} \| p_{\Theta}\right)\] <p>which in my notation is equivalent to the first application of the DPI above:</p> \[\Kale{\qof{\L...\given \x...}}{\pof{\L...\given \x...}} \le \Kale{\qof{\W}}{\pof{\W}}.\] <p>They maximize the fELBO objective:</p> \[\begin{aligned} \mathcal{F}\left(q_{\boldsymbol{\Theta}}\right) &amp;=\mathbb{E}_{q_{f\left(\mathbf{x}_{\mathcal{D}} ; \boldsymbol{\Theta}\right)}}\left[\log p_{\mathbf{y} \mid f(\mathbf{X} ; \boldsymbol{\Theta})}\left(\mathbf{y}_{\mathcal{D}} \mid f\left(\mathbf{X}_{\mathcal{D}} ; \boldsymbol{\theta}\right)\right)\right]\\ &amp;\quad -\sup _{\mathbf{X} \in \mathcal{X}_{\mathbb{N}}} \mathbb{D}_{\mathrm{KL}}\left(q_{f(\mathbf{X} ; \boldsymbol{\Theta})} \| p_{f(\mathbf{X} ; \boldsymbol{\Theta})}\right), \end{aligned}\] <p>which is equivalent to minimizing the information-theoretic objective:</p> \[\E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\L... \given \x...}}{\pof{\L... \given \x...}},\] <p>if we choose the \(\x...\) to tighten the DPI inequality as much as possible (i.e. by “finding” the supremum).</p> <p>Using the inequality chain from above, we can sandwich their objective between a regular (negative) ELBO and the (negative) functional ELBO, we have derived above:</p> \[\begin{aligned} &amp;\E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\W}}{\pof{\W}} \\ &amp;\quad \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\L... \given \x...}}{\pof{\L... \given \x...}} \\ &amp;\quad \ge \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}. \end{aligned}\] <p><strong>Why are they using logits instead of probabilities?</strong> In practice, using the probabilities instead of logits when performing linearization is often cumbersome due to the non-linearity of the softmax functions, which requires Monte-Carlo sampling of the logits to obtain an approximation of the final probabilities. Furthermore, I speculate that sampling the logits can be more benign given that we often use ReLUs in the underlying neural networks. (Don’t quote me too strongly on this, though.)</p> <p>Conceptually, this explains the derivation of their ELBO objective and also relates them to the ‘purer’ and simpler functional evidence bound derived above, but this raises the question of how these inequalities are different and what the gap between them tells us. Let’s address this question next.</p> <h2 id="the-equality-case-and-equivalence-classes">The Equality Case and Equivalence Classes</h2> <p>When do we have equality? That is, when do we have:</p> \[\Kale{\qof{\W}}{\pof{\W}} = \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}?\] <p>And what does it tell us?</p> <p>As we have seen in the first part of this post, we have equality in the DPI if and only:</p> <p>\(\Kale{\qof{\W \given \Y..., \x...}}{\pof{\W \given \Y..., \x...}}=0\).</p> <p>Given that we are trying to approximate the Bayesian posterior \(\pof{\w \given \Y..., \x...}\) using \(\qof{\w}\), this equality condition tells us that we would have to find the exact posterior for equality. Hence, it is unlikely that we will have equality in practice. From this, the next question immediately follows: what does this predictive prior term</p> \[\Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}\] <p>provides us with?</p> <p>Another way to think about the gap between the two KL divergences is that one is parameter-based and the other one is not. This points to a deeper truth about overparameterized models used in deep learning:</p> <aside class="l-body box-important"> <p>Deep neural networks have many parameter symmetries, and it is often possible to permute the weights of a neural network without changing the outputs—for example, in a convolutional neural network, we could swap channels.</p> </aside> <p>The functional KL divergences won’t be affected by this as they are parameter-free and do not take into account the parameters of the model but only the predictions. The regular parameter-based KL divergence, however, would be affected by this—depending on the prior \(\pof{\w}\), they might express differences between the parameter distributions that have no effect on the outputs.</p> <p>In other words, if the prior assigns different probability to otherwise equivalent parameters, this obviously changes the parameter posterior, while the outputs are invariant to these changes if the overall assigned probability to a given output remains the same.</p> <aside class="l-body box-important"> <p>A better name for the functional KL divergence and function-space variational inference might be <em>predictive</em> KL divergence and <em>predictive variational inference</em>.</p> <p>To me, functions are an abstract thing, and determining how to assign a prior or even a distribution to them sounds difficult and complex, adding an air of mystique for me, whereas predictions are something I can easily understand and relate to.</p> </aside> <p>For example, the paper “Deep Ensembles: A Loss Landscape Perspective” by Fort et al. (2020)<d-cite key="fort2020deep"></d-cite> examines the similarity of the predictions of models trained from different initializations and shows that the prediction space has a multi-modal loss landspace. In the language of FSVI, this is similar to analyzing the function-space distances between different models.</p> <h3 id="equivalence-classes">Equivalence Classes</h3> <p>Unless there are other considerations, it makes sense to use priors that assign the same density to parameters that are equivalent. Hence, for a given function \(\fof{\x ; \w}\), which determines the likelihood \(\pof{\y \given \x, \w} \triangleq \pof{y \given \fof{\x ; \w}}\), we can define an equivalence relation such that \(\w \sim \w'\) if and only if \(\fof{\x; \w} = \fof{\x; \w'}\) <em>for all</em> \(\x\). This equivalence relation partitions the parameter space into equivalence classes:</p> \[[\w] \triangleq \{\w' : \fof{x ; \w} = \fof{x ; \w} \quad \forall x \}.\] <p>A prior \(\pof{\w}\) induces a prior \(\hpof{[\w]}\) over the equivalence classes:</p> \[\hpof{[\w]} \triangleq \sum_{\w' \in [\w]} \pof{\w'}.\] <p>—or \(\int_{[\w]} \pof{\w'} \, d \w'\) for continuous \(\w\)—with the corresponding model:</p> \[\begin{aligned} \hpof{\y, [\w] \given \x} &amp;\triangleq \hpof{\y \given \x, [\w]} \, \hpof{[\w]} \\ &amp;= \pof{\y \given \x, \w} \, \hpof{[\w]}. \end{aligned}\] <aside class="l-body box-error"> <p>Crucially, the definition of the equivalence classes depends on which \(\x\) we consider as part of the input domain: <em>different domains for \(\x\) will induce different equivalence classes.</em></p> <p>For example, for function families on \(\mathbb{R} \to \mathbb{R}\), if we restrict the domain to \(\x \in (0, 1)\), the equivalence classes will be different from the domain \(\x \in \mathbb{R}\). For \(\x \in (0, 1)\), we do not differentiate between functions that behave differently outside this domain. For example, for any \(k \ge 1,\)</p> \[\fof{x} = \begin{cases} x^2 &amp; \text{if } |x| \leq k \\ 2|x| - 2k + k^2 &amp; \text{otherwise} \end{cases}\] <p>has the same predictions within the domain \(\x \in (0, 1)\) while being different outside of it for different \(k\).</p> </aside> <h3 id="consistency">Consistency</h3> <p>Importantly, the definition of the equivalence classes above is consistent with Bayesian inference:</p> <aside class="l-body box-note"> <p>\([\w]\) commutes with Bayesian inference:</p> \[\hpof{[\w] \given \Dany} = \sum_{\w' \in [\w]} \pof{\w' \given \Dany}.\] </aside> <p>This is easy to show with using Bayes’ rule:</p> \[\begin{aligned} \hpof{[\w] \given \Dany} &amp;= \hpof{\Dany \given [\w]} \, \hpof{[\w]} / \hpof{\Dany} \\ &amp;= \pof{\Dany \given \w} \sum_{\w' \in [\w]} \pof{\w'} / \hpof{\Dany} \\ &amp;= \sum_{\w' \in [\w]} \pof{\Dany \given \w'} \, \pof{\w'} / \hpof{\Dany} \\ &amp;= \sum_{\w' \in [\w]} \pof{\w' \given \Dany} \, \pof{\Dany} / \hpof{\Dany} \\ &amp;= \sum_{\w' \in [\w]} \pof{\w' \given \Dany}. \end{aligned}\] <p>The last step follows from \(\hpof{\Dany}=\pof{\Dany}\):</p> \[\begin{aligned} \hpof{\Dany} &amp;= \sum_{[\w]} \hpof{\Dany, [\w]} \\ &amp;= \sum_{[\w]} \sum_{\w' \in [\w]} \pof{\Dany, \w'} \\ &amp;= \sum_{\w'} \pof{\Dany, \w} \\ &amp;= \pof{\Dany}. \end{aligned}\] <p>This also tells us that, for any \(\x\) and \(\y\):</p> <p>\(\pof{\y... \given \x...} = \hpof{\y... \given \x...}\).</p> <p>Given this consistency, we don’t have to differentiate between \(\hat\opp\) and \(\opp\) and can use \(\opp\) interchangeably. The same holds for \(\opq\).</p> <aside class="l-body box-important"> <p>This commutative property is not unique to the equivalence class mapping but rather a general characteristic of applying functions to random variables. The mapping \([\cdot]\) transforms the random variable \(\W\) into a new random variable \([\W]\), and remarkably, this transformation commutes with Bayesian inference.</p> <p>The proof above holds for any function \(\fof{\cdot}\) applied to a random variable. We are free to either draw a parameter sample from the posterior and then apply the function to it, or alternatively, perform Bayesian inference directly on the transformed random variable.</p> <p>Consequently, we have:</p> \[[\W \given \Dany] = [\W] \given \Dany.\] </aside> <p>These simple results deserve careful appreciation to fully internalize them.</p> <h3 id="equality--symmetries">Equality &amp; Symmetries</h3> <p>We can view \([\w]\) as a projection from \(\w\) to its equivalence class \([\w]\). The DPI then gives us:</p> \[\Kale{\qof{\W}}{\pof{\W}} \ge \Kale{\qof{[\W]}}{\pof{[\W]}}.\] <p>And again: what does the gap between the two terms tell us?</p> <aside class="l-body box-error"> <p>Intuitively speaking, \(\Kale{\qof{[\W]}}{\pof{[\W]}}\) captures the meaningful divergence between approximate and true distribution of the predictions, while \(\Kale{\qof{\W}}{\pof{\W}}\) also includes divergence due to parameter symmetries that have no effect on the predictions, now or later.</p> <p>The gap between the two terms tells us how much redundancy is captured by the parameter-based KL divergence.</p> </aside> <p>Let’s look at a few examples to get a better understanding of this.</p> <h4 id="1-trivial-constant-case">1. Trivial Constant Case</h4> <p>Let \(\fof{\x ; \w} = 0\) independent of any \(f\). Then \([\w] = [\w']\) for any \(\w\), \(\w'\).</p> <p>For any approximate distribution \(\qof{\w}\), the induced \(\Kale{\qof{[\W]}}{\pof{[\W]}}=0\), while \(\Kale{\qof{\W}}{\pof{\W}}\) also includes superfluous divergence.</p> <h4 id="2-unused-parameter">2. Unused Parameter</h4> <p>Let \(\y \given (\w_1, \w_2) = \w_1\) deterministic but independent of \(\w_2\). Then \([(\w_1, \w_2)] = [(\w_1, {\w'}_2)]\) for any \({\w'}_2\) and \([(\w_1,*)]\not=[({\w'}_1, *)]\) for any \(\w_1 \not= \w'_1\).</p> <p>\(\Kale{\qof{[\W]}}{\pof{[\W]}}=\Kale{\qof{\W_1}}{\pof{\W_1}}\) captures the meaningful divergence between approximate and true distribution, while \(\Kale{\qof{\W}}{\pof{\W}}\) also includes any divergence across \(\w_2\) that has no effect on the predictions.</p> <h4 id="3-periodic-parameter-space">3. Periodic Parameter Space</h4> <p>Finally, let’s assume that the predictions are periodic in some way. That is, for example \(\y = \sin \w\). We then have \([\w] = [\w + 2\pi]\).</p> <p>Further, let \(\pof{\w} = \operatorname{U}(\w; [0,2\pi \, N))\) for some \(N\) that determines the number of periods. Then, if we introduce another random variable \(K\), that captures which period we are in, we can (again) use the chain rule to write:</p> \[\begin{aligned} &amp;\Kale{\qof{\W}}{\pof{\W}} \\ &amp;\quad= \Kale{\qof{\W \given \W \in [K\,2\pi, (K+1)\,2\pi]}}{\pof{\W \given \W \in [K\,2\pi, (K+1)\,2\pi]}} \\ &amp;\quad + \Kale{\qof{\W \in [K\,2\pi, (K+1)\,2\pi]}}{\pof{\W \in [K\,2\pi, (K+1)\,2\pi]}} \\ &amp;\quad= \Kale{\qof{[\W]}}{\pof{[\W]}} \\ &amp;\quad\quad + \Kale{\qof{\W \in [K\,2\pi, (K+1)\,2\pi]}}{\pof{\W \in [K\,2\pi, (K+1)\,2\pi]}}. \end{aligned}\] <p>This follows from the setup of this specific example. Finally, we have:</p> \[\Kale{\qof{\W \in [K\,2\pi, (K+1)\,2\pi]}}{\pof{\W \in [K\,2\pi, (K+1)\,2\pi]}} \le \log N.\] <p>So, if \(\opq\) only had support in a single period for example, the difference between \(\Kale{\qof{\W}}{\pof{\W}}\) and \(\Kale{\qof{[\W]}}{\pof{[\W]}}\) would be \(\log N\): the redundancy.</p> <h3 id="predictive-prior">Predictive Prior</h3> <p>How does the predictive prior term fit into this? The DPI again yields the answer:</p> <aside class="l-body box-important"> <p>As the predictions are independent of the parameters, we can map from equivalence classes \([\w]\) to predictions, and the DPI yields:</p> \[\begin{aligned} \Kale{\qof{\W}}{\pof{\W}} &amp;\ge \Kale{\qof{[\W]}}{\pof{[\W]}} \\ &amp;\ge \Kale{\qof{\Y...\given\x...}}{\pof{\Y...\given\x...}}. \end{aligned}\] </aside> <p>This tells us that the predictive prior term can at best measure the KL divergence between the equivalence classes of the parameters—and not between the parameters itself—but luckily, this is the more meaningful divergence anyway!</p> <p>For the equality cases, we observe that:</p> <ol> <li>we need a 1:1 mapping between parameters and equivalence classes for the first bound to be tight, and</li> <li>we need \(\Kale{\qof{[\W] \given \Y_n,\x_n,...,\Y_1,\x_1}}{\pof{[\W] \given \Y_n,\x_n,...,\Y_1,\x_1}} \to 0\) for \(n \to \infty\) for the second bound to be tight, following the deductions in the first part of this post.</li> </ol> <p>For <strong>2.</strong>: as we know from the chain rule that</p> \[\Kale{\qof{\Y_n,...\Y_1\given\x_n,...,\x_1}}{\pof{\Y_n,...\Y_1\given\x_n,...,\x_1}}\] <p>is monotonically increasing in \(n\), and it is bounded by \(\Kale{\qof{[\W]}}{\pof{[\W]}}\) from above, it <em>must</em> converge<d-footnote>It is a <i>bounded monotonically increasing sequence.</i></d-footnote>. So, when does it close the gap?</p> <p>To give intuition that it might do that, and without attempting to prove this formally, we can appeal to <a href="https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem"><em>Bernstein von Mises</em> theorem</a>, which states that the posterior distribution of the parameters converges to a Gaussian distribution with mean and variance given by the maximum likelihood estimate (MLE) as the number of data points tends to infinity <em>as long as the model parameters are identifiable, that is the true parameters we want to learn are unique, and that they have support</em>.</p> <p>For the evidence bound to be meaningful, we already know that we need support of the approximate distribution \(\opq\) in the prior \(\opp\)—otherwise, the LHS is \(\infty\). Moreover, realizing that we take an expectation over \(\qof{\Y_n ,..., \Y_1 \given \x_n ,..., \x_1}\), we can decompose the KL term for the gap as:</p> \[\begin{aligned} &amp;\Kale{\qof{[\W] \given \Y_n,\x_n,...,\Y_1,\x_1}}{\pof{[\W] \given \Y_n,\x_n,...,\Y_1,\x_1}} \\ &amp;\quad = \E{\qof{\y_n,...,\y_1\given\x_n,...,\x_1}}{\Kale{\qof{[\W]\given \y_n, \x_n, ..., \y_1, \x_1}}{\pof{[\W]\given \y_n, \x_n, ..., \y_1, \x_1}}} \\ &amp;\quad = \simpleE{\qof{[\w']}}{\E{\qof{\y_n,..,.\y_1\given\x_n,...,\x_1, [\w']}}{\Kale{\qof{[\W]\given \y_n, \x_n, ..., \y_1, \x_1}}{\pof{[\W]\given \y_n, \x_n, ..., \y_1, \x_1}}}}. \end{aligned}\] <p>That is, we sample a \([\w'] \sim \qof{[\w']}\) and then sample \(\y_n,...\y_1\given\x_n,...,\x_1\) from the corresponding \(\qof{\y_n,...\y_1\given\x_n,...,\x_1, [\w']}\) and marginalize over these. Crucially, \([\w']\) are the true parameters of the data-generating process for the inner KL divergence term. We thus take an expectation over KL terms fulfilling the conditions of the Bernstein von Mises theorem:</p> \[\begin{aligned} \Kale{\qof{[\W] \given \y_n,\x_1...\y_1, \x_1}}{\pof{[\W] \given \y_n,\x_1...\y_1, \x_1}} \to 0. \end{aligned}\] <p>In other words:</p> <ol> <li> <p>For a given \([w']\), in the space of equivalence classes as defined previously, the equivalence class of all MLE solutions in the data limit, \([MLE]\), will be unique by definition—the model is identifiable—and match \([\w']\)<d-footnote>This follows from the consistency of MLE estimators but also from Berstein von Mises with a flat/uninformative prior.</d-footnote>.</p> </li> <li> <p>As the MLE is prior-independent once there is support for it, both \(\opq\) and \(\opp\) will converge to the MLE \([\w']\) with sufficient data. Taking the expectation, this yields \(\Kale{\qof{[\W]\given \Y,..., \x...}}{\pof{[\W] \given \Y,..., \x...}} \to 0\) for \(n \to \infty\), and thus, we have:</p> </li> </ol> \[\begin{aligned} &amp; \Kale{\qof{[\W]}}{\pof{[\W]}} = \\ &amp;\quad = \sup_{n\in \mathbb{N}} \Kale{\qof{\Y_n,...,\Y_1\given\x_n,...,\x_1}}{\pof{\Y_n,...,\Y_1\given\x_n,...,\x_1}}. \end{aligned}\] <p>(Again, this is not a formal proof but an intuition for why the gap might close in the data limit.)</p> <p>In my opinion, this is a great result. We have shown both that the predictive prior term converges given our assumptions and that it converges to the symmetry-free parameter-based divergence in the data limit. This is a strong argument for the predictive prior term being meaningful and not just a technical trick.</p> <p>Let’s appreciate one more thing: the predictive prior can consist of infinitely many data points and still converge to a finite value.</p> <h3 id="the-fsvi-objective">The FSVI Objective</h3> <p>The true objective of FSVI is to perform variational inference in the space of equivalence classes, approximating the posterior \(\pof{[\w] \given \Dany}\) that captures the meaningful aspects of the model while being invariant to parameter symmetries. This perspective reveals FSVI as a powerful and principled framework for Bayesian inference in overparameterized models, providing both conceptual and practical advantages over standard variational inference in parameter space.</p> <aside class="l-body box-important"> <p>FSVI aims to approximate the posterior distribution over <em>equivalence classes</em> of parameters \(\pof{[\w] \given \Dany}\) rather than the posterior over raw parameters \(\pof{\w \given \Dany}\).</p> </aside> <p>This is because \(\pof{[\w] \given \Dany}\) captures the meaningful aspects of the posterior, as it is invariant to the symmetries and equivalences in the parameter space that leave the likelihood unchanged.</p> <p>To achieve this, FSVI employs an implicit variational distribution \(\qof{[\w]}\) and minimizes the following objective, which is regular variational inference on the equivalence classes \(\Kale{\qof{[\W]}}{\pof{[\W] \given \Dany}}\), decomposed using the insights discussed:</p> \[\begin{aligned} &amp;\Hof{\Dany} \le \Hof{\Dany} + \underbrace{\Kale{\qof{[\W]}}{\pof{[\W] \given \Dany}}}_{\ge 0} \\ &amp;\quad = \Hof{\Dany} + \E{\qof{[\w]}}{-\log \pof{\Dany \given [\w]}} + \Kale{\qof{[\W]}}{\pof{[\W]}} - \Hof{\Dany}\\ &amp;\quad = \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{[\W]}}{\pof{[\W]}} \\ &amp;\quad = \sup_n \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y_n... \given \x_n...}}{\pof{\Y_n... \given \x_n...}} \\ &amp;\quad \ge \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y_n... \given \x_n...}}{\pof{\Y_n... \given \x_n...}} \quad \forall n. \end{aligned}\] <p>This objective differs from the standard variational inference objective in a subtle but crucial way: it operates in the space of equivalence classes \([\w]\) rather than the raw parameter space \(\w\). The likelihood term \(\pof{\Dany \given [\w]}\) is well-defined because all parameters \(\w\) within an equivalence class \([\w]\) induce the same likelihood by construction, allowing us to replace it with the likelihood term \(\pof{\Dany \given \w}\).</p> <p>The key insight is that FSVI leverages the data processing inequality (DPI) to avoid explicitly constructing the equivalence classes for the prior distribution or specifying a model that operates on them directly. By matching the predictive distributions via the predictive KL term, FSVI implicitly matches the priors over equivalence classes in the limit of infinite data, as shown in the previous section.</p> <p>This has advantages as overparameterized models are often simpler to optimize. The empirical evidence for this is the success of deep learning models, which are overparameterized and trained with stochastic gradient descent.</p> <p>This highlights the elegance and conceptual clarity of FSVI:</p> <aside class="l-body box-important"> <p>FSVI is a principled approach to Bayesian inference that respects the inherent symmetries and equivalences in overparameterized models. It focuses on approximating the meaningful posterior \(\pof{[\w] \given \Dany}\) while avoiding the complexities of explicitly constructing and working with equivalence classes.</p> </aside> <h2 id="parameter-priors-vs-predictive-priors">Parameter Priors vs. Predictive Priors</h2> <p>In Bayesian deep learning, we often use parameter priors that are not meaningful and which also do not take parameter symmetries into account. For example, a unit Gaussian prior over the parameters of a neural network does not induce different predictions for different parameters necessarily. While this prior can be sensible from a parameter compression perspective (e.g. see Hinton and van Camp (1993)<d-cite key="hinton1993keeping"></d-cite>), this does not have to be the only consideration guiding us.</p> <p>With function priors and predictive priors, we can specify more meaningful priors because we can focus on the predictions and ignore the parameters. More importantly, this connects Bayesian approaches to data augmentation and other regularization techniques as we will see next.</p> <p>Given that priors over equivalence classes are difficult to express explicitly though, using the DPI to obtain a functional ELBO can be an easier way to express and approximate them.</p> <h3 id="label-entropy-regularization">Label Entropy Regularization</h3> <p>All this also helps us gain a new perspective on label entropy regularization. The functional evidence bound can be lower-bounded using the chain rule by:</p> \[\begin{aligned} \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}} \\ \ge \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \E{\pdata{\x}}{\Kale{\qof{\Y \given \x}}{\pof{\Y \given \x}}}, \end{aligned}\] <p>where we can expand the term under the second expectation to:</p> \[\Kale{\qof{\Y \given \x}}{\pof{\Y \given \x}}=\CrossEntropy{\qof{\Y \given \x}}{\pof{\Y \given \x}} - \xHof{\qof{\Y \given \x}}.\] <p><em>Assuming that our prior yields a uniform distribution over the labels</em>, we can drop the cross entropy term because it is constant and obtain:</p> \[\E{\qof{\w}}{-\log \pof{\Dany \given \w}} - \E{\pdata{\x}}{\xHof{\qof{\Y \given \x}}}.\] <p>This is the same as an MLE minimization objective with an additional entropy regularization term \(-\xHof{\qof{\Y \given \x}}\) for different \(\x\) that prevents the model from overfitting to the labels and collapsing to the one-hot encoding of the labels.</p> <p>Thus, in the simplest approximation, the DPI and functional variational inference give us a new perspective on label entropy regularization.</p> <h3 id="knowledge-distillation">Knowledge Distillation</h3> <p>Obviously, assuming non-uniform prior predictions, \(\E{\pdata{\x}}{\Kale{\qof{\Y \given \x}}{\pof{\Y \given \x}}}\) can be related to knowledge distillation in deep neural networks as introduced by Hinton et al. (2015)<d-cite key="hinton2015distilling"></d-cite>.</p> <p>The main technical difference is that knowledge distillation is using the reverse KL divergence instead of the forward KL divergence, while the conceptual difference is that we are not distilling the knowledge from a teacher model but from the prior that we downweigh while also training our model on the data itself. However, the connection between knowledge distillation and continual learning using informative priors is manifest.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we took a deep dive into the data processing inequality (DPI) and its surprisingly far-reaching implications for modern Bayesian deep learning. By carefully examining the assumptions, equality conditions, and chain rule of the DPI, we arrived at an intuitive understanding of why function-space variational inference (FSVI) can be such a powerful tool. The DPI perspective shows how FSVI side-steps issues with high-dimensional parameter spaces by focusing on matching Bayesian predictive posteriors.</p> <p>Reasoning about parameter equivalence classes under the lens of the DPI, we saw how predictive KL divergences can capture meaningful differences between models while ignoring superficial discrepancies due to symmetries. This provides a fresh perspective on the advantages of predictive priors over standard parameter priors commonly used in Bayesian neural networks.</p> <p>A key insight that emerged is that FSVI’s true objective is to perform variational inference in the space of predictive equivalence classes, approximating the posterior \(\pof{[\w] \given \Dany}\) that captures the meaningful aspects of the model while being invariant to parameter symmetries. By employing an implicit variational distribution and leveraging the DPI, FSVI sidesteps the need to explicitly define equivalence classes, relying instead on the fact that matching predictive distributions is sufficient to match the posteriors over equivalence classes in the infinite data limit. This reveals FSVI as a principled and elegant approach to Bayesian inference in overparameterized models.</p> <p>While our treatment only scratched the surface of the full mathematical story, the intuitions we developed allowed us to re-derive key results from the literature and uncover deep connections between seemingly disparate methods like entropy regularization, continual learning, and knowledge distillation. The examples and proofs peppered throughout solidified the core concepts.</p> <p>More than a bag of technical tricks, the DPI reveals itself to be a powerful conceptual tool for reasoning about models, objectives, and algorithms. I hope this post inspires the reader to seek the fundamental principles underpinning machine learning innovations and to use those principles as a guide for future research. With a solid grasp of foundational tools like the DPI, we can all contribute to demystifying and unifying the rapidly evolving field of Bayesian deep learning.</p> <hr/> <p><strong>Acknowledgements.</strong> Many thanks to <a href="https://fbickfordsmith.com/">Freddie Bickford Smith</a> for very helpful comments and feedback on this post and to <a href="https://timrudner.com/">Tim Rudner</a> for additional pointers to relevant literature and feedback on the FSVI section in particular 🤗 GPT-4 and Claude 3 Opus were used for editing.</p>]]></content><author><name>Andreas Kirsch</name></author><category term="Data Processing Inequality"/><category term="Information Theory"/><category term="Function-Space Variational Inference"/><category term="Parameter Equivalence Classes"/><category term="Entropy Regularization"/><category term="Label Entropy Regularization"/><summary type="html"><![CDATA[This blog post explores the interplay between the Data Processing Inequality (DPI), a cornerstone concept in information theory, and Function-Space Variational Inference (FSVI) within the context of Bayesian deep learning. The DPI governs the transformation and flow of information through stochastic processes, and its unique connection to FSVI is employed to highlight FSVI's focus on Bayesian predictive posteriors over parameter space. The post examines various forms of the DPI, including the KL divergence based DPI, and provides intuitive examples and detailed proofs. It also explores the equality case of the DPI to gain a deeper understanding. The connection between DPI and FSVI is then established, showing how FSVI can measure a predictive divergence independent of parameter symmetries. The post relates FSVI to knowledge distillation and label entropy regularization, highlighting the practical relevance of the theoretical concepts. Throughout the post, theoretical concepts are intertwined with intuitive explanations and mathematical rigor, offering a comprehensive understanding of these complex topics. By examining these concepts in depth, the post provides valuable insights for both theory and practice in machine learning.]]></summary></entry><entry><title type="html">Elaborating on the Value of Flow Matching for Density Estimation</title><link href="https://iclr-blogposts.github.io/2024/blog/elaborating-on-the-value-of-flow-matching-for-density-estimation/" rel="alternate" type="text/html" title="Elaborating on the Value of Flow Matching for Density Estimation"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://iclr-blogposts.github.io/2024/blog/elaborating-on-the-value-of-flow-matching-for-density-estimation</id><content type="html" xml:base="https://iclr-blogposts.github.io/2024/blog/elaborating-on-the-value-of-flow-matching-for-density-estimation/"><![CDATA[<h1 id="motivation">Motivation</h1> <p>Normalizing Flows (NF) enable the construction of complex probability distributions by transforming a simple, known distribution into a more complex one. They do so by leveraging the change of variables formula, defining a bijection from the simple distribution to the complex one.</p> <p>For most of the time, flows were based on chaining several differentiable and invertible transformations. However, these diffeomorphic transformations limit the flows in their complexity as such have to be simple. Furthermore, this leads to trade-off sampling speed and evaluation performance <d-cite key="papamakarios_normalizing_2019"></d-cite>. Their continuous counterpart, Continuous Normalizing Flows (CNFs) have been held back by limitations in their Simulation-Based maximum likelihood training <d-cite key="tong_improving_2023"></d-cite>. By utilizing Flow Matching, this limitation has been overcome and CNFs have been shown to be a powerful tool for density estimation.</p> <p>In the following sections, CNFs and Flow Matching are explained. Following the explanation, the empirical results of Flow Matching are presented. Finally, the application of Flow Matching in Simulation-Based Inference is discussed, which shall highlight their wide applicability and consistent improvement.</p> <h1 id="continuous-normalizing-flows">Continuous Normalizing Flows</h1> <p>Continuous normalizing flows are among the first applications of neural ordinary differential equations (ODEs) <d-cite key="chen_neural_2018"></d-cite>. Instead of the traditional layers of neural networks, the flow is defined by a vector field that is integrated over time.</p> \[\frac{d}{dt} x(t) = f_{\theta}(x(t), t)\] <p>The vector field is typically parameterized by a neural network. While traditional layer based flow architectures need to impose special architectural restrictions to ensure invertibility, CNFs are invertible as long as the uniqueness of the solution of the ODE is guaranteed. This is for instance the case if the vector field is Lipschitz continuous in \(x\) and continuous in \(t\). Many common neural network architectures satisfy these conditions. Hence, the above equation defines a diffeomorphism \(\phi_t(x_0) = x_0 + \int_0^t f_{\theta}(x(t), t)\) under the discussed assumption. The change of variables formula can be applied to compute the density of a distribution that is transformed by \(\phi_t\).</p> <p>As usual, a CNF is trained to transform a simple base distribution \(p_B\), usually a standard normal distribution, into a complex data distribution \(p_D\). For each point in time \(t\in[0,1]\) the time-dependent vector field defines a distribution \(p_t\) (probability path) and the goal is to find a vector field \(f_\theta\) such that \(p_1=p_D\). This is usually achieved by maximum likelihood training, i.e. by minimizing the negative log-likelihood of the data under the flow.</p> <p>While CNFs are very flexible, they are also computationally expensive to train naively with maximum likelihood since the flow has to be integrated over time for each sample. This is especially problematic for large datasets which are needed for the precise estimation of complex high-dimensional distributions.</p> <h1 id="flow-matching">Flow Matching</h1> <p>The authors of <d-cite key="lipman_flow_2023"></d-cite> propose a new method for training CNFs, which avoids the need for simulation. The key idea is to regress the vector field directly from an implicit definition of a target vector field that defines a probability path \(p_t(x)\) with \(p_0=p_{B}\) and \(p_1=p_{D}\). Moreover, the authors propose a loss function that directly regresses the time dependent vector field against the conditional vector fields with respect to single samples.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/imagenet-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/imagenet-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/imagenet-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/imagenet.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Unconditional ImageNet-128 samples of a CNF trained using Flow Matching with Optimal Transport probability paths. Figure obtained from <d-cite key="lipman_flow_2023"></d-cite>. </div> <p>Assuming that the target vector field is known, the authors propose a loss function that directly regresses the time dependent vector field:</p> \[L_{\textrm{FM}}(\omega) = \mathbb{E}_{t, p_t(x)}(|f_{\omega}(x, t) - u_t(x)|^2),\] <p>where \(u_t\) is a vector field that generates \(p_t\) and the expectation with respect to \(t\) is over a uniform distribution. Unfortunately, the loss function is not directly applicable because we do not know how to define the target vector field. However, it turns out that one can define appropriate conditional target vector fields when conditioning on the outcome \(x_1\):</p> \[p_t(x) = \int p_t(x|x_1)p_{D}(x_1)d x_1.\] <p>Using this fact, the conditional flow matching loss can be defined, obtaining equivalent gradients as the flow matching loss.</p> \[L_{\textrm{CFM}}(\omega) = \mathbb{E}_{t, p_t(x|x_1), p_D(x_1)}(|f_{\omega}(x, t) - u_t(x|x_1)|^2).\] <p>Finally, one can easily obtain an unbiased estimate for this loss if samples from \(p_D\) are available, \(p_t(x|x_1)\) can be efficiently sampled, and \(u_t(x|x_1)\) can be computed efficiently. We discuss these points in the following.</p> <h2 id="gaussian-conditional-probability-paths">Gaussian Conditional Probability Paths</h2> <p>The vector field that defines a probability path is usually not unique. This is often due to invariance properties of the distribution, e.g. rotational invariance. The authors focus on the simplest possible vector fields to avoid unnecessary computations. They choose to define conditional probability paths that maintain the shape of a Gaussian throughout the entire process. Hence, the conditional probability paths can be described by a variable transformation \(\phi_t(x \mid x_1) = \sigma_t(x_1)x + \mu_t(x_1)\). The time-dependent functions \(\sigma_t\) and \(\mu_t\) are chosen such that \(\sigma_0(x_1) = 1\) and \(\sigma_1 = \sigma_\text{min}\) (chosen sufficiently small), as well as \(\mu_0(x_1) = 0\) and \(\mu_1(x_1)=x_1\). The corresponding probability path can be written as</p> \[p_t(x|x_1) = \mathcal{N}(x; \mu_t(x_1), \sigma_t(x_1)^2 I).\] <p>In order to train a CNF, it is necessary to derive the corresponding conditional vector field. An important contribution of the authors is therefore the derivation of a general formula for the conditional vector field \(u_t(x|x_1)\) for a given conditional probability path \(p_t(x|x_1)\) in terms of \(\sigma_t\) and \(\mu_t\):</p> \[u_t(x\mid x_1) = \frac{\sigma_t'(x_1)}{\sigma_t(x_1)}(x-\mu_t(x_1)) - \mu_t'(x_1),\] <p>where \(\psi_t'\) denotes the derivative with respect to time \(t\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/vectorfields.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/vectorfields.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/vectorfields.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/vectorfields.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Compared to the diffusion path’s conditional score function, the OT path’s conditional vector field has constant direction in time and is arguably simpler to fit with a parametric model. Note the blue color denotes larger magnitude while red color denotes smaller magnitude. Figure obtained from <d-cite key="lipman_flow_2023"></d-cite>. </div> <p>They show that it is possible to recover certain diffusion training objectives with this choice of conditional probability paths, e.g. the variance preserving diffusion path with noise scaling function \(\beta\) is given by:</p> \[\begin{align*} \phi_t(x \mid x_1) &amp;= (1-\alpha_{1-t}^2)x + \alpha_{1-t}x_1 \\\ \alpha_{t} &amp;= \exp\left(-\frac{1}{2}\int_0^t \beta(s) ds\right) \end{align*}\] <p>Additionally, they propose a novel conditional probability path based on optimal transport, which linearly interpolates between the base and the conditional target distribution.</p> \[\phi_t(x \mid x_1) = (1-(1-\sigma_{\text{min}})t)x + tx_1\] <p>The authors argue that this choice leads to more natural vector fields, faster convergence and better results.</p> <h1 id="empirical-results">Empirical Results</h1> <p>The authors investigate the utility of Flow Matching in the context of image datasets, employing CIFAR-10 and ImageNet at different resolutions. Ablation studies are conducted to evaluate the impact of choosing between standard variance-preserving diffusion paths and optimal transport (OT) paths in Flow Matching. The authors explore how directly parameterizing the generating vector field and incorporating the Flow Matching objective enhances sample generation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/imagegen.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/imagegen.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/imagegen.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/imagegen.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Likelihood (BPD), quality of generated samples (FID), and evaluation time (NFE) for the same model trained with different methods. Figure from <d-cite key="lipman_flow_2023"></d-cite>. </div> <p>The findings are presented through a comprehensive evaluation using various metrics such as negative log-likelihood (NLL), Frechet Inception Distance (FID), and the number of function evaluations (NFE). Flow Matching with OT paths consistently outperforms other methods across different resolutions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/sampling.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/sampling.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/sampling.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/sampling.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Flow Matching, especially when using OT paths, allows us to use fewer evaluations for sampling while retaining similar numerical error (left) and sample quality (right). Results are shown for models trained on ImageNet 32×32, and numerical errors are for the midpoint scheme. Figure from <d-cite key="lipman_flow_2023"></d-cite>. </div> <p>The study also delves into the efficiency aspects of Flow Matching, showcasing faster convergence during training and improved sampling efficiency, particularly with OT paths.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/sample_path-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/sample_path-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/sample_path-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/sample_path.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sample paths from the same initial noise with models trained on ImageNet 64×64. The OT path reduces noise roughly linearly, while diffusion paths visibly remove noise only towards the end of the path. Note also the differences between the generated images. Figure from <d-cite key="lipman_flow_2023"></d-cite>. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/superres.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/superres.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/superres.svg-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/superres.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image super-resolution on the ImageNet validation set. Figure from <d-cite key="lipman_flow_2023"></d-cite>. </div> <p>Additionally, conditional image generation and super-resolution experiments demonstrate the versatility of Flow Matching, achieving competitive performance in comparison to state-of-the-art models. The results suggest that Flow Matching presents a promising approach for generative modeling with notable advantages in terms of model efficiency and sample quality.</p> <h1 id="application-of-flow-matching-in-simulation-based-inference">Application of Flow Matching in Simulation-Based Inference</h1> <p>A very specifically interesting application of density estimation, i.e. Normalizing Flows, is in Simulation-Based Inference (SBI). In SBI, Normalizing Flows are used to estimate the posterior distribution of model parameters given some observations. An important factor here are the sample efficiency, scalability, and expressivity of the density model. Especially for the later two, Flow Matching has shown to the yield an improvement. This is due to the efficient transport between source and target density and the flexibility due the more complex transformations allowed by continuous normalizing flows. To start out, a brief introduction to SBI shall be given as not many might be familiar with this topic.</p> <h2 id="primer-on-simulation-based-inference">Primer on Simulation-Based Inference</h2> <p>In many practical scenarios, the likelihood function of a model is intractable and cannot be described analytically. This might be the case for where the forward model is a complex or proprietary simulation, or if it is a physical experiment <d-cite key="papamakarios_normalizing_2019"></d-cite>. In order to still be able to perform Bayesian inference, one can resort to a class of methods called Likelihood-free Inference. One possible but popular method in this class is SBI. The core idea is to use a prior in combination with the simulator to obtain samples from the joint distribution of the parameters and the data. Based on these samples, the posterior can either be learned directly or the likelihood can be approximated <d-cite key="cranmer_frontier_2020"></d-cite>. Depending on the exact method chosen, the approximated posterior is either amortized, i.e. does not require refitting when conditioned on different data, or non-amortized.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/kinds_of_sbi-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/kinds_of_sbi-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/kinds_of_sbi-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/kinds_of_sbi.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The figure depicts the schematic flow of information for different kinds of Likelihood-free methods. Modern methods in SBI are depicted in the bottom row where the likelihood is approximated in subfigure E, the posterior is approximated in subfigure F, and the likelihood-ratio in subfigure G. Figure from <d-cite key="cranmer_frontier_2020"></d-cite>. </div> <p>In order to formalize the method, let \(\theta \sim \pi(\theta)\) denote the parameters to a system and its respective prior distribution. The system under evaluation and the respective observations obtained are denoted by \(x = \mathcal{M}(\theta)\). To sample from the joint distribution \(p(\theta, x)\), the dedicated parameter \(\theta_i\) is sampled from the prior and the observation is obtained by evaluating the forward model on that parameter \(x_i = \mathcal{M}(\theta_i)\). According to this approach, a dataset of samples from the joint distribution can be generated \(\mathcal{X} = \{ (\theta, \mathbf{x})_i \}^N_{i=1}\). A density estimator is then fitted on the provided dataset in order to estimate the desired distribution, e.g. directly the posterior \(q_{\omega}(\theta \mid x) \approx p(\theta \mid x)\).</p> <p>The interested reader shall be directed to <d-cite key="papamakarios_fast_2016"></d-cite> and especially <d-cite key="papamakarios_normalizing_2019"></d-cite> for a more rigorous introduction to SBI. In order to compare the performances of the different approaches to SBI and their performance with respect to certain tasks, an excellent overview is provided in <d-cite key="lueckmann_benchmarking_2021"></d-cite>. For the sake of this post, a more abstract understanding is enough.</p> <h2 id="flow-matching-for-simulation-based-inference">Flow Matching for Simulation-Based Inference</h2> <p>The approach using the Flow Matching formulation to fit the density network is presented by Dax et al. <d-cite key="dax_flow_2023"></d-cite>. In the setting described by the authors and the before mentioned SBI context, the goal is to approximate a posterior distribution of over model parameters given observations \(p(\theta \vert x)\). To learn the posterior, the Flow Matching loss is adapted to the following:</p> \[\mathcal{L}_{FMPE} = \mathbb{E}_{t \sim p(t),\theta_1 \sim p(\theta), x \sim p(x \vert \theta_1),\theta_t \sim p_t(\theta_t \mid \theta_1)} \Vert f_{\omega,x}(\theta_t, t) - u_t(\theta_t \mid \theta_1) \Vert^2\] <p>The important details to note here are the adaptations to minimize the loss w.r.t. samples drawn from the joint distribution, as it is described in the general section to SBI. To do so, the expectation is adapted to be w.r.t. \(\theta_1 \sim p(\theta), x \sim p(x \vert \theta_1)\), which yield the desired samples.</p> <p>Another adaption by the authors is to exchange the uniform distribution over the time with a general distribution \(t \sim p(t)\). The effects of this substitution won’t be focus deeper. However, adapting the distribution makes intuitive sense as the training gets harder close to the target distribution. Therefore, focussing on time steps \(t\) closer to one is beneficial, as the authors have also found in their empirical studies.</p> <p>In order to provide a general comparison of the Flow Matching-based SBI approach, the CFM model is tested on the SBI benchmarking tasks <d-cite key="lueckmann_benchmarking_2021"></d-cite>. The results show either equal or better performance, underscoring the approaches ability and applicability to SBI.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/fmpe_sbi_benchmark-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/fmpe_sbi_benchmark-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/fmpe_sbi_benchmark-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/fmpe_sbi_benchmark.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The figure depicts the results of the CFM model on the SBI benchmarking tasks, as carried out by the authors of <d-cite key="dax_flow_2023"></d-cite>. Comparing the results to such obtained by neural posterior estimation with a normalizing flow shows comparable performance on most tasks while outperforming on some. </div> <p>Besides the general benchmarks, the authors use their proposed technique to estimate the posterior distribution of gravitational wave parameters \(p(\theta \mid x)\) where \(\theta \in \mathbb{R}^{15}, x \in \mathbb{R}^{15744}\). In order to reduce the problem’s dimensionality and increase the information density, the observations are compressed to \(128\) dimensions using an embedding network.</p> <p>Following the preprocessing of the data, three density estimators are fitted and compared to each other. The first method uses a neural spline flow, which has proven itself on these kinds of problems. It is compared to a neural posterior estimation using the Flow Matching approach described here. Finally, a neural posterior estimator leveraging physical symmetries is used to estimate the targeted posterior. All were trained on a simulation budget of \(5 \cdot 10^6\) samples for a total of 400 epochs.</p> <p>In order to evaluate the models’ performances, the obtained posteriors were compared w.r.t. their 50% credible regions as well as Jensen-Shannon divergence between the inferred posterior and reference results. The results shown below support the advantages found in the benchmarking tasks. The Flow Matching-based shows a good performance for all shown parameters and has a clear advantage over the classical NPE approach.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/fmpe_results_gw-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/fmpe_results_gw-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/fmpe_results_gw-1400.webp"/> <img src="/2024/assets/img/2024-05-07-elaborating-on-the-value-of-flow-matching-for-density-estimation/fmpe_results_gw.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The figure shows the single performances of a classic NPE approach using neural spline flows, the proposed Flow Matching approach, and a physics-focussed NPE approach. The results are shown for the 50% credible regions on the left, as well as the Jensen-Shannon divergence between the inferred posterior and reference results on the right. The Flow Matching-based approach shows a good performance for all investigated parameters and has a clear advantage over the classical NPE approach. In the pair plot on the left, the choice was made to only show the four parameters for which the classical NPE method performs the worst. While the Flow Matching approach could perform worse on other dimensions, this is not the case as shown on the right. Figure from <d-cite key="dax_flow_2023"> </d-cite>. </div> <p>Whilst the examples are interesting themselves, their evaluation has shown the applicability, scalability, and flexibility of Flow Matching for density estimation. These performance improvements in different areas have motivated the discussion of Flow Matching in the first place and hopefully become clear now.</p> <h1 id="a-personal-note">A Personal Note</h1> <p>Whilst this is a blog post, we’d like to use this last part to express our personal thoughts on this topic. SBI is a powerful method, enabling Bayesian Inference where it would not be possible<d-footnote>It might be more fitting to say that Bayesian Inference is not practically feasible in many scenarios as, in theory, it might still be possible by sampling. However, this is essentially not possible where single evaluations of the forward model are expensive or further evaluations are simply not available, as shown in the example.</d-footnote> otherwise. Due to the natural problem setting of SBI, where problems are high-dimensional, observations are scarce, and distribution complex, density estimators capable to counter these are required. In the past, Normalizing Flows have proven themselves to meet these challenges, whilst not resolving them completely. CNFs, due to their higher flexibility, have been a desired method to put to test whether they could even improve on these but were limited in the inability to train the efficiently.</p> <p>Formulating the Flow Matching variant of CNFs has allowed their application to complex density estimation tasks, as for example in SBI, and they’ve shown to yield the expected improvements – on standard SBI benchmarking tasks as well a very high dimensional task from the field of astrophysics. Furthermore, the generalization of CFM even broadens their applicability. It will be very interesting to see what possibilities are opened by this exact formulation and, in addition, what further improvements can be obtained by transferring techniques from the Diffusion Models to Normalizing Flows.</p>]]></content><author><name>Maternus Herold</name></author><summary type="html"><![CDATA[The transfer of matching-based training from Diffusion Models to Normalizing Flows allows to fit expressive continuous normalizing flows efficiently and therefore enables their usage for different kinds of density estimation tasks. One particularly interesting task is Simulation-Based Inference, where Flow Matching enabled several improvements. The post shall focus on the discussion of Flow Matching for Continuous Normalizing Flows. To highlight the relevance and the practicality of the method, their use and advantages for Simulation-Based Inference is elaborated.]]></summary></entry></feed>