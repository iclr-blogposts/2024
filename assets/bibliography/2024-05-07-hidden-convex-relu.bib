@inproceedings{allen-zhuConvergenceTheoryDeep2019a,
  title = {A Convergence Theory for Deep Learning via Over-Parameterization},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {{Allen-Zhu}, Zeyuan and Li, Yuanzhi and Song, Zhao},
  year = {2019},
  month = may,
  pages = {242--252},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-11-23},
  abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100\% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps ~ e\^\{-T\}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
  langid = {english},
  file = {/home/savar/Zotero/storage/J5AT5RQZ/Allen-Zhu et al. - 2019 - A Convergence Theory for Deep Learning via Over-Pa.pdf}
}

@article{bachBreakingCurseDimensionality,
  title = {Breaking the Curse of Dimensionality with Convex Neural Networks},
  author = {Bach, Francis},
  abstract = {We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non-convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomialtime algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.},
  langid = {english},
  file = {/home/savar/Zotero/storage/KJTXML6Y/Bach - Breaking the Curse of Dimensionality with Convex N.pdf}
}

@misc{baiEfficientGlobalOptimization2022,
  title = {Efficient Global Optimization of Two-Layer ReLU Networks: Quadratic-Time Algorithms and Adversarial Training},
  shorttitle = {Efficient Global Optimization of Two-Layer ReLU Networks},
  author = {Bai, Yatong and Gautam, Tanmay and Sojoudi, Somayeh},
  year = {2022},
  month = jan,
  number = {arXiv:2201.01965},
  eprint = {2201.01965},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-08},
  abstract = {The non-convexity of the artificial neural network (ANN) training landscape brings inherent optimization difficulties. While the traditional back-propagation stochastic gradient descent (SGD) algorithm and its variants are effective in certain cases, they can become stuck at spurious local minima and are sensitive to initializations and hyperparameters. Recent work has shown that the training of an ANN with ReLU activations can be reformulated as a convex program, bringing hope to globally optimizing interpretable ANNs. However, naively solving the convex training formulation has an exponential complexity, and even an approximation heuristic requires cubic time. In this work, we characterize the quality of this approximation and develop two efficient algorithms that train ANNs with global convergence guarantees. The first algorithm is based on the alternating direction method of multiplier (ADMM). It solves both the exact convex formulation and the approximate counterpart. Linear global convergence is achieved, and the initial several iterations often yield a solution with high prediction accuracy. When solving the approximate formulation, the per-iteration time complexity is quadratic. The second algorithm, based on the ``sampled convex programs'' theory, is simpler to implement. It solves unconstrained convex formulations and converges to an approximately globally optimal classifier. The non-convexity of the ANN training landscape exacerbates when adversarial training is considered. We apply the robust convex optimization theory to convex training and develop convex formulations that train ANNs robust to adversarial inputs. Our analysis explicitly focuses on one-hidden-layer fully connected ANNs, but can extend to more sophisticated architectures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{68Q25, 82C32, 49M29, 46N10, 62M45},Computer Science - Machine Learning},
  file = {/home/savar/Zotero/storage/R3P4Q44S/Bai et al. - 2022 - Efficient Global Optimization of Two-layer ReLU Ne.pdf}
}

@article{berthierIncrementalLearningDiagonal,
  title = {Incremental Learning in Diagonal Linear Networks},
  author = {Berthier, Raphael},
  abstract = {Diagonal linear networks (DLNs) are a toy simplification of artificial neural networks; they consist in a quadratic reparametrization of linear regression inducing a sparse implicit regularization. In this paper, we describe the trajectory of the gradient flow of DLNs in the limit of small initialization. We show that incremental learning is effectively performed in the limit: coordinates are successively activated, while the iterate is the minimizer of the loss constrained to have support on the active coordinates only. This shows that the sparse implicit regularization of DLNs decreases with time. This work is restricted to the underparametrized regime with anti-correlated features for technical reasons.},
  langid = {english},
  file = {/home/savar/Zotero/storage/J9SSZK2Y/Berthier - Incremental Learning in Diagonal Linear Networks.pdf}
}

@misc{boursierGradientFlowDynamics2022b,
  title = {Gradient Flow Dynamics of Shallow ReLU Networks for Square Loss and Orthogonal Inputs},
  author = {Boursier, Etienne and {Pillaud-Vivien}, Loucas and Flammarion, Nicolas},
  year = {2022},
  month = oct,
  number = {arXiv:2206.00939},
  eprint = {2206.00939},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.00939},
  urldate = {2023-05-15},
  abstract = {The training of neural networks by gradient descent methods is a cornerstone of the deep learning revolution. Yet, despite some recent progress, a complete theory explaining its success is still missing. This article presents, for orthogonal input vectors, a precise description of the gradient flow dynamics of training one-hidden layer ReLU neural networks for the mean squared error at small initialisation. In this setting, despite non-convexity, we show that the gradient flow converges to zero loss and characterise its implicit bias towards minimum variation norm. Furthermore, some interesting phenomena are highlighted: a quantitative description of the initial alignment phenomenon and a proof that the process follows a specific saddle to saddle dynamics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/savar/Zotero/storage/GFUYDNQG/Boursier et al. - 2022 - Gradient flow dynamics of shallow ReLU networks fo.pdf;/home/savar/Zotero/storage/BEF9KKY7/2206.html}
}

@inproceedings{chizatGlobalConvergenceGradient2018,
  title = {On the Global Convergence of Gradient Descent for Over-Parameterized Models Using Optimal Transport},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'e}al, Canada},
  author = {Chizat, L{\'e}na{\"i}c and Bach, Francis R.},
  editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and {Cesa-Bianchi}, Nicol{\`o} and Garnett, Roman},
  year = {2018},
  pages = {3040--3050},
  urldate = {2023-03-27},
  file = {/home/savar/Zotero/storage/KEDG8YKZ/Chizat and Bach - 2018 - On the Global Convergence of Gradient Descent for .pdf}
}

@misc{chizatLazyTrainingDifferentiable2020,
  title = {On Lazy Training in Differentiable Programming},
  author = {Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  year = {2020},
  month = jan,
  number = {arXiv:1812.07956},
  eprint = {1812.07956},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2023-05-22},
  abstract = {In a series of recent theoretical works, it was shown that strongly overparameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this ``lazy training'' phenomenon is not specific to overparameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that ``lazy training'' is behind the many successes of neural networks in difficult high dimensional tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/savar/Zotero/storage/YFG26URY/Chizat et al. - 2020 - On Lazy Training in Differentiable Programming.pdf}
}

@misc{duGradientDescentProvably2019,
  title = {Gradient Descent Provably Optimizes Over-Parameterized Neural Networks},
  author = {Du, Simon S. and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  year = {2019},
  month = feb,
  number = {arXiv:1810.02054},
  eprint = {1810.02054},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.02054},
  urldate = {2023-11-23},
  abstract = {One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an \$m\$ hidden node shallow neural network with ReLU activation and \$n\$ training data, we show as long as \$m\$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. Our analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/savar/Zotero/storage/ZJQ68Z8F/Du et al. - 2019 - Gradient Descent Provably Optimizes Over-parameter.pdf;/home/savar/Zotero/storage/Z49WVJF2/1810.html}
}

@misc{jacotNeuralTangentKernel2020a,
  title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  shorttitle = {Neural Tangent Kernel},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  year = {2020},
  month = feb,
  number = {arXiv:1806.07572},
  eprint = {1806.07572},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.07572},
  urldate = {2023-11-23},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/savar/Zotero/storage/ZCALKGZJ/Jacot et al. - 2020 - Neural Tangent Kernel Convergence and Generalizat.pdf;/home/savar/Zotero/storage/R2WWHMT5/1806.html}
}

@misc{mishkinFastConvexOptimization2022a,
  title = {Fast Convex Optimization for Two-Layer ReLU Networks: Equivalent Model Classes and Cone Decompositions},
  shorttitle = {Fast Convex Optimization for Two-Layer ReLU Networks},
  author = {Mishkin, Aaron and Sahiner, Arda and Pilanci, Mert},
  year = {2022},
  month = aug,
  number = {arXiv:2202.01331},
  eprint = {2202.01331},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-23},
  abstract = {We develop fast algorithms and robust software for convex optimization of two-layer neural networks with ReLU activation functions. Our work leverages a convex reformulation of the standard weight-decay penalized training problem as a set of group- 1-regularized data-local models, where locality is enforced by polyhedral cone constraints. In the special case of zero-regularization, we show that this problem is exactly equivalent to unconstrained optimization of a convex ``gated ReLU'' network. For problems with nonzero regularization, we show that convex gated ReLU models obtain data-dependent approximation bounds for the ReLU training problem. To optimize the convex reformulations, we develop an accelerated proximal gradient method and a practical augmented Lagrangian solver. We show that these approaches are faster than standard training heuristics for the non-convex problem, such as SGD, and outperform commercial interior-point solvers. Experimentally, we verify our theoretical results, explore the group- 1 regularization path, and scale convex optimization for neural networks to image classification on MNIST and CIFAR-10.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/savar/Zotero/storage/XZP2XBNM/Mishkin et al. - 2022 - Fast Convex Optimization for Two-Layer ReLU Networ.pdf}
}

@inproceedings{pilanciNeuralNetworksAre2020,
  title = {Neural Networks Are Convex Regularizers: Exact Polynomial-Time Convex Optimization Formulations for Two-Layer Networks},
  shorttitle = {Neural Networks Are Convex Regularizers},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Pilanci, Mert and Ergen, Tolga},
  year = {2020},
  month = nov,
  pages = {7695--7705},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-06-10},
  abstract = {We develop exact representations of training two-layer neural networks with rectified linear units (ReLUs) in terms of a single convex program with number of variables polynomial in the number of training samples and the number of hidden neurons. Our theory utilizes semi-infinite duality and minimum norm regularization. We show that ReLU networks trained with standard weight decay are equivalent to block \${\textbackslash}ell\_1\$ penalized convex models. Moreover, we show that certain standard convolutional linear networks are equivalent semi-definite programs which can be simplified to \${\textbackslash}ell\_1\$ regularized linear models in a polynomial sized discrete Fourier feature space},
  langid = {english},
  file = {/home/savar/Zotero/storage/6RH8PNHW/Pilanci et Ergen - 2020 - Neural Networks are Convex Regularizers Exact Pol.pdf;/home/savar/Zotero/storage/E6U3JCEY/Pilanci et Ergen - 2020 - Neural Networks are Convex Regularizers Exact Pol.pdf}
}

@inproceedings{sharifnassabBoundsOverParameterizationGuaranteed2019,
  title = {Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Sharifnassab, Arsalan and Salehkaleybar, Saber and Golestani, S. Jamaloddin},
  year = {2019},
  month = sep,
  urldate = {2023-12-08},
  abstract = {We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let \$m\$ and \$d\$ be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size \$n{\textbackslash}geq m+2d-2\$. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for \$n{\textbackslash}le m\$, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for "existence of descent paths" in the loss landscape.},
  langid = {english},
  file = {/home/savar/Zotero/storage/LY3ICPVJ/Sharifnassab et al. - 2019 - Bounds on Over-Parameterization for Guaranteed Exi.pdf}
}

@inproceedings{wangConvexGeometryBackpropagation2021,
  title = {The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program},
  shorttitle = {The Convex Geometry of Backpropagation},
  booktitle = {International Conference on Learning Representations},
  author = {Wang, Yifei and Pilanci, Mert},
  year = {2021},
  urldate = {2023-12-08}
}

@inproceedings{wangHiddenConvexOptimization2022,
  title = {The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: An Exact Characterization of Optimal Solutions},
  shorttitle = {The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks},
  booktitle = {The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022},
  author = {Wang, Yifei and Lacotte, Jonathan and Pilanci, Mert},
  year = {2022},
  publisher = {OpenReview.net},
  urldate = {2023-03-11},
  file = {/home/savar/Zotero/storage/FJ85TBX9/Wang et al. - 2022 - The Hidden Convex Optimization Landscape of Two-La.pdf}
}
