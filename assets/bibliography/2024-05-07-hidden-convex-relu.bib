@inproceedings{allen-zhuConvergenceTheoryDeep2019a,
  title = {A Convergence Theory for Deep Learning via Over-Parameterization},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  date = {2019-05-24},
  pages = {242--252},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/allen-zhu19a.html},
  urldate = {2023-11-23},
  abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100\% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps ~ e\^\{-T\}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
  eventtitle = {International Conference on Machine Learning},
  langid = {english},
  file = {/home/savar/Zotero/storage/J5AT5RQZ/Allen-Zhu et al. - 2019 - A Convergence Theory for Deep Learning via Over-Pa.pdf}
}

@online{chizatLazyTrainingDifferentiable2020,
  title = {On Lazy Training in Differentiable Programming},
  author = {Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  date = {2020-01-07},
  eprint = {1812.07956},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/1812.07956},
  urldate = {2023-05-22},
  abstract = {In a series of recent theoretical works, it was shown that strongly overparameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this “lazy training” phenomenon is not specific to overparameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that “lazy training” is behind the many successes of neural networks in difficult high dimensional tasks.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/savar/Zotero/storage/YFG26URY/Chizat et al. - 2020 - On Lazy Training in Differentiable Programming.pdf}
}

@online{duGradientDescentProvably2019,
  title = {Gradient Descent Provably Optimizes Over-Parameterized Neural Networks},
  author = {Du, Simon S. and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  date = {2019-02-04},
  eprint = {1810.02054},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.1810.02054},
  url = {http://arxiv.org/abs/1810.02054},
  urldate = {2023-11-23},
  abstract = {One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an \$m\$ hidden node shallow neural network with ReLU activation and \$n\$ training data, we show as long as \$m\$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. Our analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/savar/Zotero/storage/ZJQ68Z8F/Du et al. - 2019 - Gradient Descent Provably Optimizes Over-parameter.pdf;/home/savar/Zotero/storage/Z49WVJF2/1810.html}
}

@online{jacotNeuralTangentKernel2020a,
  title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  shorttitle = {Neural Tangent Kernel},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
  date = {2020-02-10},
  eprint = {1806.07572},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.1806.07572},
  url = {http://arxiv.org/abs/1806.07572},
  urldate = {2023-11-23},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_\textbackslash theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_\textbackslash theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/savar/Zotero/storage/ZCALKGZJ/Jacot et al. - 2020 - Neural Tangent Kernel Convergence and Generalizat.pdf;/home/savar/Zotero/storage/R2WWHMT5/1806.html}
}

@online{mishkinFastConvexOptimization2022a,
  title = {Fast Convex Optimization for Two-Layer ReLU Networks: Equivalent Model Classes and Cone Decompositions},
  shorttitle = {Fast Convex Optimization for Two-Layer ReLU Networks},
  author = {Mishkin, Aaron and Sahiner, Arda and Pilanci, Mert},
  date = {2022-08-31},
  eprint = {2202.01331},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.01331},
  urldate = {2023-11-23},
  abstract = {We develop fast algorithms and robust software for convex optimization of two-layer neural networks with ReLU activation functions. Our work leverages a convex reformulation of the standard weight-decay penalized training problem as a set of group- 1-regularized data-local models, where locality is enforced by polyhedral cone constraints. In the special case of zero-regularization, we show that this problem is exactly equivalent to unconstrained optimization of a convex “gated ReLU” network. For problems with nonzero regularization, we show that convex gated ReLU models obtain data-dependent approximation bounds for the ReLU training problem. To optimize the convex reformulations, we develop an accelerated proximal gradient method and a practical augmented Lagrangian solver. We show that these approaches are faster than standard training heuristics for the non-convex problem, such as SGD, and outperform commercial interior-point solvers. Experimentally, we verify our theoretical results, explore the group- 1 regularization path, and scale convex optimization for neural networks to image classification on MNIST and CIFAR-10.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/savar/Zotero/storage/XZP2XBNM/Mishkin et al. - 2022 - Fast Convex Optimization for Two-Layer ReLU Networ.pdf}
}

@inproceedings{pilanciNeuralNetworksAre2020,
  title = {Neural Networks Are Convex Regularizers: Exact Polynomial-Time Convex Optimization Formulations for Two-Layer Networks},
  shorttitle = {Neural Networks Are Convex Regularizers},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Pilanci, Mert and Ergen, Tolga},
  date = {2020-11-21},
  pages = {7695--7705},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/pilanci20a.html},
  urldate = {2022-06-10},
  abstract = {We develop exact representations of training two-layer neural networks with rectified linear units (ReLUs) in terms of a single convex program with number of variables polynomial in the number of training samples and the number of hidden neurons. Our theory utilizes semi-infinite duality and minimum norm regularization. We show that ReLU networks trained with standard weight decay are equivalent to block \$\textbackslash ell\_1\$ penalized convex models. Moreover, we show that certain standard convolutional linear networks are equivalent semi-definite programs which can be simplified to \$\textbackslash ell\_1\$ regularized linear models in a polynomial sized discrete Fourier feature space},
  eventtitle = {International Conference on Machine Learning},
  langid = {english},
  file = {/home/savar/Zotero/storage/6RH8PNHW/Pilanci et Ergen - 2020 - Neural Networks are Convex Regularizers Exact Pol.pdf;/home/savar/Zotero/storage/E6U3JCEY/Pilanci et Ergen - 2020 - Neural Networks are Convex Regularizers Exact Pol.pdf}
}
