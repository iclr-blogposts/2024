@article{sun2019functional,
  title={Functional variational Bayesian neural networks},
  author={Shengyang Sun and Guodong Zhang and Jiaxin Shi and Roger Grosse},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=rkxacs0qY7},
}

@inproceedings{klarner2023drug,
  title={Drug discovery under covariate shift with domain-informed prior distributions over functions},
  author={Klarner, Leo and Rudner, Tim GJ and Reutlinger, Michael and Schindler, Torsten and Morris, Garrett M and Deane, Charlotte and Teh, Yee Whye},
  booktitle={International Conference on Machine Learning},
  pages={17176--17197},
  year={2023},
  organization={PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/klarner23a/klarner23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/klarner23a.html},
}

@inproceedings{
osband2022neural,
title={The Neural Testbed: Evaluating Joint Predictions},
author={Ian Osband and Zheng Wen and Seyed Mohammad Asghari and Vikranth Dwaracherla and Xiuyuan Lu and Morteza Ibrahimi and Dieterich Lawson and Botao Hao and Brendan O'Donoghue and Benjamin Van Roy},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=JyTT03dqCFD}
}


@InProceedings{osband2022evaluating,
  title = 	 {Evaluating high-order predictive distributions in deep learning},
  author =       {Osband, Ian and Wen, Zheng and Asghari, Seyed Mohammad and Dwaracherla, Vikranth and Lu, Xiuyuan and Van Roy, Benjamin},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1552--1560},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/osband22a/osband22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/osband22a.html},
  abstract = 	 {Most work on supervised learning research has focused on marginal predictions. In decision problems, joint predictive distributions are essential for good performance. Previous work has developed methods for assessing low-order predictive distributions with inputs sampled i.i.d. from the testing distribution. With low-dimensional inputs, these methods distinguish agents that effectively estimate uncertainty from those that do not. We establish that the predictive distribution order required for such differentiation increases greatly with input dimension, rendering these methods impractical. To accommodate high-dimensional inputs, we introduce dyadic sampling, which focuses on predictive distributions associated with random pairs of inputs. We demonstrate that this approach efficiently distinguishes agents in high-dimensional examples involving simple logistic regression as well as complex synthetic and empirical data.}
}

@inproceedings{wang2021beyond,
  title = 	 { Beyond Marginal Uncertainty: How Accurately can Bayesian Regression Models Estimate Posterior Predictive Correlations? },
  author =       {Wang, Chaoqi and Sun, Shengyang and Grosse, Roger},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2476--2484},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/wang21g/wang21g.pdf},
  url = 	 {https://proceedings.mlr.press/v130/wang21g.html},
  abstract = 	 { While uncertainty estimation is a well-studied topic in deep learning, most such work focuses on marginal uncertainty estimates, i.e. the predictive mean and variance at individual input locations. But it is often more useful to estimate predictive correlations between the function values at different input locations. In this paper, we consider the problem of benchmarking how accurately Bayesian models can estimate predictive correlations. We first consider a downstream task which depends on posterior predictive correlations: transductive active learning (TAL). We find that TAL makes better use of modelsâ€™ uncertainty estimates than ordinary active learning, and recommend this as a benchmark for evaluating Bayesian models. Since TAL is too expensive and indirect to guide development of algorithms, we introduce two metrics which more directly evaluate the predictive correlations and which can be computed efficiently: meta-correlations (i.e. the correlations between the models correlation estimates and the true values), and cross-normalized likelihoods (XLL). We validate these metrics by demonstrating their consistency with TAL performance and obtain insights about the relative performance of current Bayesian neural net and Gaussian process models. }
}

@inproceedings{rudner2022tractable,
title={Tractable Function-Space Variational Inference in Bayesian Neural Networks},
author={Tim G. J. Rudner and Zonghao Chen and Yee Whye Teh and Yarin Gal},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=OQs0pLKGGpS}
}


@InProceedings{rudner2022continual,
  title = 	 {Continual Learning via Sequential Function-Space Variational Inference},
  author =       {Rudner, Tim G. J. and Bickford Smith, Freddie and Feng, Qixuan and Teh, Yee Whye and Gal, Yarin},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {18871--18887},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/rudner22a/rudner22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/rudner22a.html},
  abstract = 	 {Sequential Bayesian inference over predictive functions is a natural framework for continual learning from streams of data. However, applying it to neural networks has proved challenging in practice. Addressing the drawbacks of existing techniques, we propose an optimization objective derived by formulating continual learning as sequential function-space variational inference. In contrast to existing methods that regularize neural network parameters directly, this objective allows parameters to vary widely during training, enabling better adaptation to new tasks. Compared to objectives that directly regularize neural network predictions, the proposed objective allows for more flexible variational distributions and more effective regularization. We demonstrate that, across a range of task sequences, neural networks trained via sequential function-space variational inference achieve better predictive accuracy than networks trained with related methods while depending less on maintaining a set of representative points from previous tasks.}
}

@inproceedings{
burt2021understanding,
title={Understanding Variational Inference in Function-Space},
author={David R. Burt and Sebastian W. Ober and Adri{\`a} Garriga-Alonso and Mark van der Wilk},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},
url={https://openreview.net/forum?id=7P9y3sRa5Mk}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}