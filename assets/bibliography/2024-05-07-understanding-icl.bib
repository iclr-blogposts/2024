@article{schmidhuber_evolutionary_1987,
  type    = {Diploma thesis},
  title   = {Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
  journal = {PhD Thesis, Institut für Informatik, Technische Universität München},
  author  = {Schmidhuber, Jürgen},
  year    = {1987}
}

@inproceedings{finn_model-agnostic_2017,
  title     = {Model-agnostic meta-learning for fast adaptation of deep networks},
  abstract  = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning p...},
  language  = {en},
  booktitle = {International {Conference} on {Machine} {Learning}},
  author    = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year      = {2017}
}

@techreport{bengio_learning_1990,
  title       = {Learning a synaptic learning rule},
  institution = {Université de Montréal, Département d'Informatique et de Recherche opérationnelle},
  author      = {Bengio, Yoshua and Bengio, Samy and Cloutier, Jocelyn},
  year        = {1990}
}

@inproceedings{Brown2020,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@misc{transformers,
  journal = {arXiv preprint arXiv:1706.03762},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title   = {Attention Is All You Need},
  year    = {2017}
}

@inproceedings{linear_transformers_fast_weight,
  author    = {Imanol Schlag and Kazuki Irie and Jürgen Schmidhuber},
  title     = {Linear Transformers Are Secretly Fast Weight Programmers},
  year      = {2021},
  booktitle = {ICML}
}

@misc{hubinger2021risks,
  title         = {Risks from Learned Optimization in Advanced Machine Learning Systems},
  author        = {Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
  year          = {2021},
  eprint        = {1906.01820},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@inproceedings{garg2022what,
  title     = {What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  author    = {Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year      = {2022},
  url       = {https://openreview.net/forum?id=flNZJ2eOet}
}

@inproceedings{tsai-etal-2019-transformer,
  title     = {Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel},
  author    = {Tsai, Yao-Hung Hubert  and
               Bai, Shaojie  and
               Yamada, Makoto  and
               Morency, Louis-Philippe  and
               Salakhutdinov, Ruslan},
  editor    = {Inui, Kentaro  and
               Jiang, Jing  and
               Ng, Vincent  and
               Wan, Xiaojun},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year      = {2019},
  url       = {https://aclanthology.org/D19-1443}
}

@inproceedings{choromanski2021rethinking,
  title     = {Rethinking Attention with Performers},
  author    = {Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
  booktitle = {International Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=Ua6zuk0WRH}
}


@inproceedings{pmlr-v119-katharopoulos20a,
  title     = {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
  author    = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {5156--5165},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf},
  url       = {https://proceedings.mlr.press/v119/katharopoulos20a.html}
}

@inproceedings{Kingma2015,
  author    = {Kingma, Diederik P. and Ba, Jimmy},
  booktitle = {Proceedings of the 3rd International Conference on Learning Representations},
  year      = {2015},
  title     = {{Adam: A Method for Stochastic Optimization}},
  location  = {San Diego, USA},
  url       = {http://arxiv.org/abs/1412.6980}
}

@article{Hochreiter1997,
  author       = {Hochreiter, S. and Schmidhuber, J.},
  year         = {1997},
  journaltitle = {Neural Computation},
  number       = {8},
  pages        = {1735--1780},
  title        = {{Long Short-Term Memory}},
  volume       = {9}
}

@inproceedings{VinyalsBK15,
  author    = {Oriol Vinyals and
               Samy Bengio and
               Manjunath Kudlur},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Order Matters: Sequence to sequence for sets},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06391},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/VinyalsBK15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wei2022emergent,
  title   = {Emergent Abilities of Large Language Models},
  author  = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  journal = {Transactions on Machine Learning Research},
  issn    = {2835-8856},
  year    = {2022},
  url     = {https://openreview.net/forum?id=yzkSU5zdwD},
  note    = {Survey Certification}
}

@inproceedings{biderman2023emergent,
  title     = {Emergent and Predictable Memorization in Large Language Models},
  author    = {Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Gregory Anthony and Shivanshu Purohit and Edward Raff},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year      = {2023},
  url       = {https://openreview.net/forum?id=Iq0DvhB4Kf}
}

@misc{hubinger2021risks,
  title         = {Risks from Learned Optimization in Advanced Machine Learning Systems},
  author        = {Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
  year          = {2021},
  eprint        = {1906.01820},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{vonoswald2023uncovering,
  title         = {Uncovering mesa-optimization algorithms in Transformers},
  author        = {Johannes von Oswald and Eyvind Niklasson and Maximilian Schlegel and Seijin Kobayashi and Nicolas Zucchet and Nino Scherrer and Nolan Miller and Mark Sandler and Blaise Agüera y Arcas and Max Vladymyrov and Razvan Pascanu and João Sacramento},
  year          = {2023},
  eprint        = {2309.05858},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{fu2023transformers,
  title         = {Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models},
  author        = {Deqing Fu and Tian-Qi Chen and Robin Jia and Vatsal Sharan},
  year          = {2023},
  eprint        = {2310.17086},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{geshkovski2023the,
  title     = {The emergence of clusters in self-attention dynamics},
  author    = {Borjan Geshkovski and Cyril Letrouit and Yury Polyanskiy and Philippe Rigollet},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year      = {2023},
  url       = {https://openreview.net/forum?id=aMjaEkkXJx}
}


@InProceedings{oswald23a,
  title = 	 {Transformers Learn In-Context by Gradient Descent},
  author =       {Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Joao and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {35151--35174},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/von-oswald23a.html},
  abstract = 	 {At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers.}
}
